Вычисление среднего значения элементов вектора

Студент: Афанасьев Александр Евгеньевич 
Группа: 3823Б1ПР3
Технология: SEQ | MPI 
Вариант: 2

1. Введение
Задача вычисления среднего значения элементов вектора является базовой операцией при работе с данными. При обработке больших объемов данных последовательное выполнение может занимать очень большое время. Использование технологии MPI позволяет распределить данные и вычислительную нагрузку между несколькими процессами и должно поспособствовать ускорению выполнения задачи
    В данной работе реализованы две версии алгоритма:
Последовательная (SEQ) — обычная последовательная версия
Параллельная (MPI) — версия с распределением между несколькими процессами
    Цель работы: реализовать параллельный алгоритм и исследовать его производительность в сравнении с последовательной версией

2. Постановка задачи
    Входные данные:
using T = int;
using InType = std::vector<T>;
    Выходные данные:
using OutType = double;
    Формат данных:
Элементы вектора генерируются случайным образом с использованием равномерного распределения (std::uniform_int_distribution) в диапазоне [−10,10]
Размер вектора для тестов производительности: N=800,000,000 
    Ограничения:
Необходимо предотвратить переполнение целочисленного типа при суммировании элементов. Для суммы используется тип int64_t
Если N=0, результат должен быть равен 0.0

3. Базовый алгоритм (последовательный)
1) Проверка на пустоту вектора: если N=0, возвращается 0,0
2) Суммирование элементов: с использованием алгоритма accumulate 
3) Деление: полученная сумма преобразуется в double и делится на количество элементов
    Сложность:
Временная: O(N)

4. Схема параллелизации
    Распределение данных: Главный процесс распределяет части вектора между всеми процессами
1) Расчет размеров блоков:
chunk_size = N / P, remainder = N % P
2) Коммуникация (Scatter):
MPI_Scatterv - используется потому что размеры блоков могут отличаться больше чем на один элемент
Процесс 0 отправляет части глобального вектора в локальные буферы процессов
Локальные вычисления: Каждый процесс суммирует элементы своего локального буфера в переменную local_sum
Используется MPI_Reduce с операцией MPI_SUM
Результат собирается в переменную global_sum на процессе 0
Процесс 0 вычисляет среднее: global_sum
Результат рассылается всем процессам через MPI_Bcast

5. Детали реализации
    Структура кода:
common/include/common.hpp: Определение типов данных
seq/src/ops_seq.cpp, seq/src/ops_seq.hpp - последовательная версия 
mpi/src/ops_mpi.cpp, mpi/src/ops_mpi.рpp -параллельная версия
tests/performance/main.cpp, tests/functional/main.cpp - тесты производительности и функциональные
    Классы: 
AfanasyevAElemVecAvgMPI - параллельная реализация
AfanasyevAElemVecAvgSEQ - последовательная реализация 
    Граничные случаи: 
Обработка N=0 реализована с возвратом 0,0

6. Экспериментальная установка
    Аппаратное обеспечение:
Процессор: Apple Silicon M1 pro
Оперативная память: 16 гб
Операционная система: macOS Tahoe 26.0.1
    Программное обеспечение:
Фреймворк тестирования: Google Test
Библиотека MPI: OpenMPI
Компилятор: Clang
    Параметры тестирования:
Размер вектора: 800,000,000 элементов

7. Результаты и обсуждение
Корректность проверена с помощью функциональных тестов, они все пройдены успешно и результаты seq и mpi схожи при разных наборах тестовых данных
    Производительность:
К сожалению с таким большим наборам данных не проходится реквест, в следствии чего количество данных изменено на 100,000,000, что уже удовлетворяет условию и предотвращает ошибку (время прохождения >0,001) но ниже преведены результаты при тесте на 800,000,000 элементах

Режим               Тип замера      Время, с    Ускорение
SEQ (1 процесс)     Pipeline        5,437       1,00
MPI (2 процесса)    Pipeline        2.210       2,46
MPI (3 процесса)    Pipeline        3.481       1,56
MPI (4 процесса)    Pipeline        3,254       1,67

SEQ (1 процесс)     Task Run        0,217       1,00
MPI (2 процесса)    Task Run        2,124       0,10
MPI (3 процесса)    Task Run        2,657       0,08 
MPI (4 процесса)    Task Run        3,045       0,07 

Анализ результатов:

    Режим Pipeline:
В данном режиме наблюдается ускорение mpi версии в 1,67 раза, это связано с более успешным разделением ресурсов и перекрытии при выполнении частец задачи
    Режим Task Run:
mpi версия замедллась по сравнению с последовательной версией из за того, что сама по себе задача и вычисления очень простые, а в mpi версии очень много времени тратится на распределение дажных между процессами и последующую их синхронизацию в связи с чем, эти расходы нивелируют выигрыш времени от параллелизма 
    Итог: 
Задача вычмсления среднего значения элементов вектора ограничена пропускной способностью памяти в следствии чего параллелизация с применением mpi технологий на памяти ноутбука имеет накладные расходы на копирование данных между буферами процессов, что замедляет фазу Run, но в режиме Pipeline дает значительный выигрыш во времени.

8. Выводы
    Реализация: 
Успешно реализован параллельный алгоритм вычисления среднего значения с защитой от переполнения
    Корректность: 
Пройдено 100% тестов, включая граничные случаи,
    Производительность:
На 4 процессах достигнуто ускорение в режиме Pipeline в 1,67 раза, обнаружено, что для простых задач по типу этой накладные расходы на распределение данных и синхронизацию могут превышать время самого вычисления в последовательном режиме

9. Список источников информации
Лекции по параллельному программированию ННГУ
Документация - https://learning-process.github.io/parallel_programming_course/ru/index.html