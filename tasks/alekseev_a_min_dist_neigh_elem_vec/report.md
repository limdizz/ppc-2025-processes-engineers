# Нахождение наиболее близких соседних элементов вектора

- **Студент:** Алексеев Артемий Алексеевич, группа 3823Б1ПР2
- **Технология:** SEQ | MPI
- **Вариант:** №7

## 1. Введение

**Цель работы:** Реализовать алгоритм нахождения наиболее близких соседних элементов вектора, далее именуемая НБСЭВ, двумя способами и провести анализ их производительностей.

**Задачи:**
1. Реализовать последовательную версию алгоритма нахождения НБСЭВ.
2. Реализовать параллельную версию с использованием технологии MPI нахождения НБСЭВ.
3. Провести сравнительный анализ производительности и эффективности обеих реализаций.

## 2. Постановка задачи

**Задача**: Найти НБСЭВ.

**Описание метода решения:** Будем проходиться по вектору элементов, находя их модульные разности, и запоминать минимальное значение и индекс первого соседа.
При возникновении ситуации, когда имеем больше число пар ближайших соседей, берем соседей с минимальным индексом.

**Входные данные:**
- `vec` — входной вектор

**Выходные данные:**
- (index, index + 1) - пара соседей, ближайших друг к другу и имеющих минимальные индексы

**Ограничения:** 
- Не предполагаются


## 3. Описание алгоритма (последовательного)
**Алгоритм последовательного вычисления:**
1. **Пройдемся по циклу от 0 до size - 1**.
2. **Найдем модульную разность соседей** `|vec[i] - vec[i + 1]|`.
3. **При условии, что разность соседей меньше, чем в буфере, перезаписываем** ` index = i; index_value = value;`.

**Реализация на C++:**

```cpp
int index = -1;
int index_value = std::numeric_limits<int>::max();
for(int i = 0; i < static_cast<int>(vec.size()) - 1; i++){
  int value = std::abs(vec[i] - vec[i + 1]);
  if(value < index_value){
    index = i;
    index_value = value;
  }
}
```
## 4. Схема распараллеливания (MPI)
**Алгоритм параллельного вычисления:**
Аналогично, с некоторыми дополнениями 
1. **Каждый процесс идет по своему пуллу ресурсов**.
2. **Каждый процесс ищет свой минимум в его ресурсах**.
3. **Глобальная длина формируется из минимумов каждого локального минимума процесса:** `MPI_Allreduce(&local_dist, &global_dist, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);`.
4. **Финальный расчет: также при одинаковых минимальных дистанциях ищем по минимальному индексу** `MPI_Allreduce(&global_index, &finish_global_index, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);`
**Принцип разделения отрезков:**
```cpp
bool AlekseevAMinDistNeighElemVecMPI::RunImpl() {
  int rank = 0;
  int comm_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &comm_size);

  const auto &vec = GetInput();
  int total_size = static_cast<int>(vec.size());

  if (total_size < 2) {
    GetOutput() = std::make_tuple(-1, -1);
    return true;
  }

  std::vector<int> displacements;
  std::vector<int> send_counts = CalculateSendCountsAndDisplacements(total_size, comm_size, displacements);

  int my_chunk_size = send_counts[rank];
  std::vector<int> local_data(my_chunk_size);

  MPI_Scatterv(vec.data(), send_counts.data(), displacements.data(), MPI_INT, local_data.data(), my_chunk_size, MPI_INT,
               0, MPI_COMM_WORLD);

  std::vector<int> prev_elements = CalculatePrevElements(rank, comm_size, vec, displacements);

  int my_prev_element = 0;
  MPI_Scatter(prev_elements.data(), 1, MPI_INT, &my_prev_element, 1, MPI_INT, 0, MPI_COMM_WORLD);

  auto [local_min_dist, local_min_index] =
      FindLocalMinDistance(local_data, my_chunk_size, rank, my_prev_element, displacements);

  int global_min_dist = std::numeric_limits<int>::max();
  MPI_Allreduce(&local_min_dist, &global_min_dist, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);

  int candidate_index =
      (local_min_dist == global_min_dist && local_min_index != -1) ? local_min_index : std::numeric_limits<int>::max();

  int global_min_index = std::numeric_limits<int>::max();
  MPI_Allreduce(&candidate_index, &global_min_index, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);

  GetOutput() = std::make_tuple(global_min_index - 1, global_min_index);
  return true;
}
```

**Схема распределения вычислений:**
- Процесс 0: обрабатывает отрезки base_chunk * rank, base_chunk * rank + base_chunk, ...
- Процесс 1: обрабатывает отрезки base_chunk * rank, base_chunk * rank + base_chunk, ...
- Процесс k: обрабатывает отрезки base_chunk * rank, base_chunk * rank + base_chunk, ...

## 5. Детали реализации
### 5.1. Структура реализации
```text
alekseev_a_min_dist_neigh_elem_vec
    ├───common
    │   └───include
    │           common.hpp - определение типов входных/выходных/тестовых данных
    │
    ├───mpi
    │   ├───include
    │   │       ops_mpi.hpp - заголовочный файл MPI-реализации
    │   │
    │   └───src
    │           ops_mpi.cpp - код MPI-реализации
    │
    ├───seq
    │   ├───include
    │   │       ops_seq.hpp - заголовочный файл SEQ-реализации
    │   │
    │   └───src
    │           ops_seq.cpp - код SEQ-реализации
    │
    └───tests
        ├───functional
        │       main.cpp - функциональные тесты
        │
        └───performance
                main.cpp - тесты производительности
```
### 5.2. Основные классы / функции
- `AlekseevAMinDistNeighElemVecSEQ` - последовательная реализация
- `AlekseevAMinDistNeighElemVecMPI` - параллельная реализация
### 5.3. Пространственная и временная сложности алгоритмов
- Последовательная версия:
    - Временная сложность - `O(N)`, где `N` - размер входного вектора;
    - Пространственная сложность - `O(N)`, где `N` - размер входного вектора.
- Параллельная версия:
    - Временная сложность - `O(N/P + MP)`, где `N` - размер входного вектора, `P` - количество процессов. Слагаемое `N/P` - сложность локальных вычислений, `MP` - сложность операции `MPI`;
    - Пространственная сложность: `O(N/P)`, где `N` - размер входного вектора, `P` - количество процессов.
## 6. Экспериментальная среда
### 6.1. Аппаратное обеспечение:
| Параметр | Значение                                                                               |
| -------- | -------------------------------------------------------------------------------------- |
| CPU      | Ryzen 5 5600                                                                           |
| RAM      | 16 GB DDR4                                                                             |     
### 6.2. Программное обеспечение:
| Параметр   | Значение                                               |
| ---------- | ------------------------------------------------------ |
| ОС         | Windows 11 Home + WSL                                  |
| MPI        | OpenMPI 3.1                                            |
| Компилятор | g++ 14.2.0                                             |
| Сборка     | Release                                                |
### 6.3. Тестовые данные
**Функциональные тесты:**\
Используют заранее подготовленный массив пар `вектор - описание тестового случая`. Корректность ответа проверяется в функции `CheckTestOutputData` с помощью вычисления ответа последовательным алгоритмом.
**Тесты производительности:**
Вектор размером 100000000 генерируется в цикле по индексам вектора с помощью `(i % 1000 + 7) * 3`
## 7. Результаты и обсуждение

### 7.1 Корректность
Проверка корректности реализована средствами Google Test:
- 20 функциональных тестов покрывают различные ситуации.
- Perf-тест проверяет, что результат валиден.

Реализация успешно проходит как функциональные тесты, так и перф-тесты.

### 7.2 Производительность
**Метрики:**
1. Абсолютное время выполнения вычислительной части алгоритма в миллисекундах;
2. Ускорение относительно последовательной версии;
3. Эффективность распараллеливания = `(ускорение / число процессов) * 100%`.

**Полученные результаты:**

| **Режим** | **Количество процессов** | **Время, с** | **Speedup** | **Efficiency** |
|-----------|--------------------------|--------------|-------------|----------------|
| SEQ       | 1                        | 0.541        | 1.00        | N/A            |
| MPI       | 2                        | 0.278        | 1.94        | 97%            |
| MPI       | 4                        | 0.226        | 2.39        | 60%            |

## 8. Заключение
В рамках данной работы были успешно реализованы алгоритмы нахождения наиболее близких соседних элементов вектора. Проведенные эксперименты подтвердили значительное ускорение MPI-реализации и корректность работы обоих вариантов алгоритма.

## 9. Источники
1. [Презентация по курсу](https://learning-process.github.io/parallel_programming_slides/)
