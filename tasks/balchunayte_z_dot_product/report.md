# Скалярное произведение векторов

- Студент: Бальчунайте Злата Денисовна, группа 3823Б1ПР3  
- Технология: MPI + SEQ  
- Вариант: 9  

## 1. Введение
Скалярное произведение векторов — одна из базовых операций линейной алгебры, широко применяемая в численных методах, машинном обучении и моделировании физических процессов.  
Цель работы — реализовать последовательный и MPI-параллельный варианты вычисления скалярного произведения, интегрировать их в инфраструктуру PPC и исследовать производительность.

## 2. Постановка задачи
Даны два вектора одинаковой длины:

\[
v = (v_1, ..., v_n), \quad u = (u_1, ..., u_n)
\]

Необходимо вычислить:

\[
v \cdot u = \sum_{i=1}^{n} v_i u_i
\]

### Ограничения:
- Размеры векторов должны совпадать.  
- Используется тип `double`.  
- MPI-версия должна корректно распределять данные и собирать результаты.  
- Решение должно проходить функциональные тесты PPC.

## 3. Базовый алгоритм (последовательный)
Последовательный алгоритм представляет собой простой цикл:
1. Проходим по всем элементам векторов.
2. Перемножаем соответствующие элементы.
3. Суммируем результат в переменной `double`.

Этот вариант используется как эталон для проверки корректности и для сравнения производительности.

## 4. Схема распараллеливания (MPI)

### Распределение данных
- Процесс с рангом 0 передаёт всем процессам размер входных векторов.
- С помощью `MPI_Scatterv` каждый процесс получает свой поддиапазон элементов.
- Распределение учитывает остаток при делении размера массива на число процессов.

### Локальное вычисление
Каждый процесс вычисляет локальный результат:

\[
\text{local\_sum} = \sum_{i = start}^{end} v_i u_i
\]

### Сбор результата
- Глобальная сумма вычисляется с помощью `MPI_Allreduce` с операцией `MPI_SUM`.
- `MPI_Allreduce` делает итоговое значение доступным для всех процессов, но в инфраструктуре PPC реальный вывод производится только процессом 0.

### Коммуникационный шаблон
- Одно `MPI_Bcast` размера данных.
- Два `MPI_Scatterv` для блоков `a` и `b`.
- Один `MPI_Allreduce` для сбора итоговой суммы. 

Схема минимальна и эффективна для данной задачи.

## 5. Детали реализации

### Структура файлов
tasks/balchunayte_z_dot_product/
├── common/include/common.hpp
├── seq/include/ops_seq.hpp
├── seq/src/ops_seq.cpp
├── mpi/include/ops_mpi.hpp
├── mpi/src/ops_mpi.cpp
└── tests/

### Особенности
- Обработка граничных случаев (вектор нулевой длины и др.) выполняется через инфраструктуру PPC.
- MPI-версия хранит только локальный фрагмент, что позволяет масштабировать решение на большие размеры.
- Реализация соответствует требованиям PPC: наличие pipeline- и task_run-режимов, корректная интеграция.

## 6. Экспериментальная установка

### Аппаратное и программное окружение
- **CPU:** 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz  
- **Ядер / потоков:** 4 / 8  
- **ОЗУ:** 8 ГБ  
- **ОС:** Windows 10 22H2

### Инструменты
- **Компилятор:** MSVC + Clang-Format/Clang-Tidy  
- **MPI:** Microsoft MPI (MS-MPI)  
- **Сборка:** Release  

### Команды запуска
- Последовательная версия:  
  `ppc_perf_tests.exe --gtest_filter=*seq*`
- MPI-версия:  
  `mpiexec -n <k> ppc_perf_tests.exe --gtest_filter=RunModeTests/DotProductRunPerfTestProcesses.*`

### Данные
Входные данные автоматически генерируются тестовым фреймворком PPC.

## 7. Результаты и обсуждение

### 7.1 Проверка корректности
Корректность подтверждена:
- успешным прохождением всех функциональных тестов,
- сопоставлением MPI и SEQ результатов,
- прохождением PPC-режимов pipeline и task_run.

Ошибок корректности нет.

### 7.2 Производительность

#### Последовательная версия
| Режим | Время (с) |
|-------|-----------|
| seq pipeline | 0.0001064 |
| seq task_run | 0.0001086 |

#### MPI: 2 процесса
| Режим | Время (с) |
|-------|-----------|
| mpi pipeline | 0.000079 |
| mpi task_run | 0.000067 |

#### MPI: 4 процесса
| Режим | Время (с) |
|-------|-----------|
| mpi pipeline | 0.000074 |
| mpi task_run | 0.000089 |

#### Ускорение

Базовое время: **0.0001064 (seq pipeline)**

| Процессы | Время (с) | Ускорение | Эффективность |
|----------|-----------|-----------|---------------|
| 1 | 0.0001064 | 1.00 | 100% |
| 2 | 0.000079 | 1.35 | 67.5% |
| 4 | 0.000074 | 1.43 | 36% |

### Обсуждение
- MPI показывает ускорение даже на ноутбуке с ограниченным количеством ядер.
- Эффективность падает при 4 процессах, что ожидаемо из-за:
  - очень малого размера тестовых данных,
  - высоких коммуникационных накладных расходов,
  - единой памяти (MS-MPI работает поверх shared memory).
- При больших размерах векторов ускорение возрастёт.

## 8. Заключение
В работе были реализованы и протестированы последовательная и MPI-параллельная версии вычисления скалярного произведения.  
Производительность MPI-версии демонстрирует ускорение на 2–4 процессах.  
Корректность подтверждена тестами PPC.  
Реализация полностью соответствует требованиям задания.

## 9. Источники
1. Учебные материалы курса PPC (лекции и практики)
2. Microsoft MPI Documentation   

## Приложение (опционально)
```cpp
// Локальное вычисление суммы в MPI
double local = 0.0;
for (std::size_t i = 0; i < local_size; ++i)
    local += v_local[i] * u_local[i];
