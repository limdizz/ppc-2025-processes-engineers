# Нахождение максимальных значений по строкам матрицы

- Студент: Батушин Илья Александрович, группа 3823Б1ПР2
- Технологии: SEQ, MPI
- Вариант: 15

## 1. Введение

Одной из фундаментальных задач вычислительной математики является обработка матриц - структур данных, используемых в машинном обучении, научных вычислениях, компьютерной графике и многих других областях. Данная работа посвящена разработке и реализации параллельного алгоритма для поиска максимальных элементов в каждой строке матрицы. Алгоритм реализован с использованием технологии MPI (Message Passing Interface), которая обеспечивает эффективную коммуникацию между процессами и позволяет масштабировать вычисления на большое количество вычислительных узлов.

## 2. Постановка задачи

Дана матрица размера M×N. Требуется найти максимальные значения в каждой строке.

**Входные данные:**
- `M` - количество строк матрицы (целое положительное число)
- `N` - количество столбцов матрицы (целое положительное число)
- `matrix` - одномерный вектор вещественных чисел длиной M×N, содержащий элементы матрицы в построчном порядке

**Выходные данные:**
- `result` - одномерный вектор вещественных чисел длиной M, где каждый элемент `result[i]` содержит максимальное значение из i-й строки матрицы

**Ограничения:**
- Матрица должна быть плотной (все элементы явно заданы)
- Элементы матрицы - числа с плавающей точкой двойной точности (double)
- Минимальный размер матрицы: 1×1

**Пример:**

Входные данные: M = 3, N = 3, matrix = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
Выходные данные: result = [3.0, 6.0, 9.0]

## 3. Описание базового алгоритма (последовательная версия)

Последовательный алгоритм поиска максимальных элементов в строках матрицы: для каждой строки матрицы выполняется линейный проход по всем элементам с обновлением текущего максимального значения.

Алгоритм состоит из следующих шагов:

1. **Инициализация**: Создание результирующего вектора размером M
2. **Обработка строк**: Для каждой строки i от 0 до M-1:
   - Инициализация переменной `max_val` первым элементом строки
   - Последовательный просмотр всех элементов строки от 1 до N-1
   - Обновление `max_val` при обнаружении большего значения
   - Сохранение итогового `max_val` в результирующий вектор
3. **Завершение**: Возврат результирующего вектора

**Сложность алгоритма**

**Время:** O(M × N)
- Один проход по всем элементам матрицы
- M × N операций сравнения

**Память:** O(M × N)
- O(M × N) для хранения входной матрицы
- O(M) для результирующего вектора
- O(1) для временных переменных
- **Итого:** O(M × N) + O(M) = O(M × N)

## 4. Схема распараллеливания

Параллельная реализация основана на делении матрицы по строкам между доступными процессами.
Матрица разделяется на блоки строк с использованием стратегии блочного циклического распределения:

```cpp
base_rows = rows / proc_count      // Базовое количество строк на процесс
extra_rows = rows % proc_count     // Остаточные строки для распределения

// Процессы с rank < extra_rows получают дополнительную строку
rows_per_process[rank] = base_rows + (rank < extra_rows ? 1 : 0)
```

**Роли процессов**
- **rank 0:** Координирует выполнение, распределяет данные, собирает результаты
- **rank 1..P-1:** Выполняют вычисления на выделенных порциях данных

**Схема коммуникации**
- **Фаза распределения:** Ведущий процесс знает всю матрицу
- **Фаза вычислений:** Каждый процесс независимо вычисляет максимумы для своих строк
- **Фаза сбора:** Остальные процессы отправляют результаты ведущему
- **Фаза синхронизации:** Ведущий процесс рассылает финальный результат всем процессам

## 5. Детали реализации

**Файлы:**
- `common/include/common.hpp` - определение типов данных
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` - последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` - параллельная реализация
- `tests/functional/main.cpp` - функциоанльные тесты
- `tests/performance/main.cpp` - тесты производительности

**Ключевые классы:**
- `BatushinIMaxValRowsMatrixSEQ` - последовательная реализация
- `BatushinIMaxValRowsMatrixMPI` - параллельная MPI реализация

**Основные методы:**
- `ValidationImpl()` - проверка входных данных
- `PreProcessingImpl()` - подготовительные вычисления  
- `RunImpl()` - основной алгоритм
- `PostProcessingImpl()` - завершающая обработка

**Вспомогательные функции:**
- `GetRowRange()` - вычисление диапазона строк для процесса
- `CalcLocalMax()` - вычисление локальных максимумов
- `CollectResults()` - сбор результатов от всех процессов  
- `SynchronizationResult()` - синхронизация финального результата

**Допущения:**
- Матрица хранится построчно
- Все процессы имеют доступ ко всей матрице (в MPI версии)
- Размеры матрицы корректны (M > 0, N > 0)

**Обрабатываемые граничные случаи:**
- Матрица 1×1 (одна строка, один столбец)
- Матрица 1×N (одна строка, несколько столбцов)
- Матрица M×1 (несколько строк, один столбец)
- Все элементы в строке одинаковые
- Отрицательные числа в матрице
- Неравномерное распределение строк при MPI распараллеливании

**Проверки в ValidationImpl():**
```cpp
if (rows == 0 || columns == 0) return false;           // нулевые размеры
if (matrix.size() != rows * columns) return false;     // несоответствие размеров
if (!GetOutput().empty()) return false;               // выход уже заполнен
```

## 6. Экспериментальное окружение

### 6.1 Аппаратное обеспечение/ОС:

- **Процессор:** Intel Core i5-1135G7
- **Ядра:** 4 физических ядра (8 логических потоков)  
- **ОЗУ:** 8 ГБ DDR4
- **ОС:** WSL Ubuntu 24.04.3 LTS (Linux kernel 5.15)

### 6.2 Программный инструментарий

- **Компилятор:** g++ 13.3.0
- **Тип сборки:** Release
- **Стандарт C++:** C++20
- **MPI:** OpenMPI 4.1.6

### 6.3 Тестовое окружение

```bash
PPC_NUM_PROC=1,2,4,8
```

## 7. Результаты

### 7.1. Корректность работы

Все функциональные тесты пройдены успешно:
- Матрицы различных размеров (от 1×1 до больших размеров)
- Случаи с одним элементом в строке
- Матрицы с отрицательными числами
- Матрицы с одинаковыми значениями во всех строках
- Неравномерное распределение строк между процессами
- SEQ и MPI версии выдают идентичные результаты для всех тестовых случаев

### 7.2. Производительность

**Время выполнения (секунды) для матрицы 5000×5000:**

| Версия | Количество процессов | Task Run время |
|--------|---------------------|----------------|
| SEQ    | 1                   | 0.0272         |
| MPI    | 1                   | 0.0291         |
| MPI    | 2                   | 0.0142         |
| MPI    | 4                   | 0.0114         |

**Ускорение относительно SEQ версии:**

| Количество процессов | Ускорение | Эффективность |
|---------------------|-----------|---------------|
| 1                   | 0.93×     | 93%           |
| 2                   | 1.92×     | 96%           |
| 4                   | 2.39×     | 60%           |


**Формула ускорения:** Ускорение = Время SEQ / Время MPI

**Формула эффективности:** Эффективность = (Ускорение / Количество процессов) × 100%

### 7.3. Анализ эффективности

- **Лучшее ускорение:** 2.39× на 4 процессах (Task Run время)
- **Оптимальная конфигурация:** 2-4 процесса
- **Эффективность MPI:** Высокая при 2 процессах (96%), снижается при увеличении числа процессов

### 7.4. Наблюдения

1. **MPI с 1 процессом** показывает схожую производительность с SEQ версией
2. **MPI с 2 процессами** демонстрирует почти линейное ускорение
3. **MPI с 4 процессами** достигает максимального ускорения для данной задачи

## 8. Выводы

### 8.1. Достигнутые результаты

- **Корректность:** Разработанный алгоритм успешно прошел все функциональные тесты, включая граничные случаи
- **Эффективность параллелизации:** Достигнуто ускорение до 2.39× на 4 процессах для матрицы 5000×5000
- **Оптимальная конфигурация:** Наилучшая эффективность (96%) достигнута при использовании 2 процессов

### 8.2. Ограничения и проблемы

- **Накладные расходы MPI:** При использовании 4 процессов эффективность снижается до 60% из-за коммуникационных затрат
- **Ограничения аппаратуры:** Максимальное ускорение ограничено 4 физическими ядрами процессора
- **Размер данных:** Для матриц меньшего размера накладные расходы MPI могут превышать выгоду от параллелизации

## 9. Источники
1. Лекции по параллельному программированию Сысоева А. В
2. Материалы курса: https://github.com/learning-process/ppc-2025-processes-engineers

## 10. Приложение

```cpp
namespace {

std::pair<size_t, size_t> GetRowRange(int rank, int proc, size_t rows) {
  size_t base_rows = rows / proc;
  size_t extra_rows = rows % proc;

  size_t start_row = (rank * base_rows) + std::min<size_t>(rank, extra_rows);
  size_t end_row = start_row + base_rows + (std::cmp_less(rank, extra_rows) ? 1 : 0);

  return {start_row, end_row};
}

std::vector<double> CalcLocalMax(size_t start_row, size_t end_row, size_t columns, const std::vector<double> &matrix) {
  std::vector<double> loc_max;

  for (size_t i = start_row; i < end_row; i++) {
    double max_val = matrix[i * columns];
    for (size_t j = 1; j < columns; j++) {
      double val = matrix[(i * columns) + j];
      max_val = std::max(val, max_val);
    }
    loc_max.push_back(max_val);
  }

  return loc_max;
}

void CollectResults(int rank, int proc, size_t rows, const std::vector<double> &loc_max, size_t start_row,
                    std::vector<double> &res) {
  if (rank == 0) {
    for (size_t i = 0; i < loc_max.size(); i++) {
      res[start_row + i] = loc_max[i];
    }

    for (int src = 1; src < proc; src++) {
      auto [src_start, src_end] = GetRowRange(src, proc, rows);
      size_t src_size = src_end - src_start;

      std::vector<double> recv_buf(src_size);
      MPI_Recv(recv_buf.data(), static_cast<int>(src_size), MPI_DOUBLE, src, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

      for (size_t i = 0; i < src_size; i++) {
        res[src_start + i] = recv_buf[i];
      }
    }
  } else {
    MPI_Send(loc_max.data(), static_cast<int>(loc_max.size()), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
  }
}

void SynchronizationResult(int rank, std::vector<double> &res) {
  if (rank == 0) {
    int res_size = static_cast<int>(res.size());
    MPI_Bcast(&res_size, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(res.data(), res_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  } else {
    int res_size = 0;
    MPI_Bcast(&res_size, 1, MPI_INT, 0, MPI_COMM_WORLD);
    res.resize(res_size);
    MPI_Bcast(res.data(), res_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  }
}

}  // namespace

bool BatushinIMaxValRowsMatrixMPI::RunImpl() {
  int rank = 0;
  int proc = 0;

  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &proc);

  size_t rows = std::get<0>(GetInput());
  size_t columns = std::get<1>(GetInput());
  const auto &matrix = std::get<2>(GetInput());

  auto [start_row, end_row] = GetRowRange(rank, proc, rows);
  auto loc_max = CalcLocalMax(start_row, end_row, columns, matrix);

  std::vector<double> res;
  if (rank == 0) {
    res.resize(rows);
  }

  CollectResults(rank, proc, rows, loc_max, start_row, res);
  SynchronizationResult(rank, res);

  GetOutput() = res;
  return true;
}
```