# Поиск максимального элемента в векторе целых чисел

- Студент: Борцова Ангелина Сергеевна, группа 3823Б1ПР5
- Технология: MPI
- Вариант: 3

## 1. Введение

Задача поиска максимального элемента в массиве является одной из базовых операций обработки данных и часто встречается в различных областях: анализ данных, статистические вычисления, обработка изображений и многих других. 

Целью лабораторной работы является реализация параллельного алгоритма поиска максимального элемента в векторе целых чисел с использованием технологии MPI и сравнение его производительности с последовательной версией.

## 2. Постановка задачи

**Входные данные:** вектор целых чисел `data` размером `n`, где `n > 0`.

**Выходные данные:** максимальное значение среди всех элементов вектора.

**Формальное определение:**
```
Дано: data = [a0, a1, ..., an-1], где ai ∈ Z
Найти: max_value = max(data) = max{a0, a1, ..., an-1}
```

**Ограничения:**
- Размер входного вектора должен быть больше нуля
- Элементы вектора — целые числа типа `int` (диапазон от `-2^31` до `2^31-1`)
- При пустом векторе результат не определен

## 3. Базовый алгоритм (последовательный)

Последовательный алгоритм поиска максимального элемента реализован следующим образом:

1. **Инициализация:** устанавливаем текущий максимум равным первому элементу вектора
2. **Проход по вектору:** последовательно сравниваем каждый элемент с текущим максимумом
3. **Обновление максимума:** если текущий элемент больше сохраненного максимума, обновляем значение максимума
4. **Возврат результата:** после обхода всех элементов возвращаем найденное максимальное значение

**Псевдокод:**
```
function find_max(data):
    if data.empty():
        return error
    
    max_elem = data[0]
    for i from 1 to data.size() - 1:
        if data[i] > max_elem:
            max_elem = data[i]
    
    return max_elem
```

**Сложность:**
- Временная сложность: O(n), где n — размер вектора
- Пространственная сложность: O(1) 

## 4. Схема распараллеливания

### 4.1 Общая схема

1. **Распределение данных (Scatter)**
   - Нулевой процесс владеет полным вектором данных
   - Вектор разделяется на части относительно равного размера
   - Каждый процесс получает свою часть данных через `MPI_Scatterv`

2. **Локальные вычисления**
   - Каждый процесс независимо находит максимум в своей части
   - Используется тот же последовательный алгоритм для локального подмассива

3. **Сбор результатов (Reduce)**
   - Все локальные максимумы собираются на нулевой процесс
   - Используется операция `MPI_Reduce` с операцией `MPI_MAX`
   - Результат: глобальный максимум всего массива

### 4.2 Распределение нагрузки

При количестве процессов `p` и размере данных `n`:
- Базовый размер блока: `chunk_size = n / p`
- Остаток: `remainder = n % p`
- Первые `remainder` процессов получают по `chunk_size + 1` элементу
- Остальные процессы получают по `chunk_size` элементов
Это обеспечивает равномерное распределение даже при `n` не кратном `p`.

### 4.3 Коммуникационная топология и роли процессов

Взаимодействие процессов организовано по централизованной схеме через корневой процесс:

**Роли процессов:**
- **Процесс 0:** Владеет исходными данными, распределяет фрагменты через `MPI_Scatterv`, получает локальный максимум для своей части, собирает глобальный результат через `MPI_Reduce`
- **Остальные процессы (rank 1..p-1):** Получают свой фрагмент данных, вычисляют локальный максимум, отправляют результат в операции редукции

**Схема взаимодействия:**

```
Шаг 1: Процесс 0 имеет весь массив данных
    Процесс 0: [data[0]...data[n-1]]

Шаг 2: Распределение данных (MPI_Scatterv)
    Процесс 0 -> получает часть [0...n/p]
    Процесс 1 -> получает часть [n/p...2n/p]
    Процесс 2 -> получает часть [2n/p...3n/p]
    ...
    Процесс p-1 -> получает часть [...]

Шаг 3: Каждый процесс ищет максимум в своей части
    Процесс 0: local_max_0
    Процесс 1: local_max_1
    Процесс 2: local_max_2
    ...
    Процесс p-1: local_max_p-1

Шаг 4: Сбор результатов (MPI_Reduce с операцией MAX)
    Все локальные максимумы -> Процесс 0
    Процесс 0: global_max = max(local_max_0, local_max_1, ...)
```

### 4.4 Синхронизация

- `MPI_Bcast`: синхронизация размера вектора между всеми процессами
- `MPI_Scatterv`: асинхронное распределение данных с неравномерными блоками
- `MPI_Reduce`: сбор локальных максимумов с операцией MAX

## 5. Детали реализации

### 5.1 Структура кода

Проект организован следующим образом:

**Файлы:**
- `common/include/common.hpp` — общие определения типов данных
- `seq/include/ops_seq.hpp` и `seq/src/ops_seq.cpp` — последовательная версия
- `mpi/include/ops_mpi.hpp` и `mpi/src/ops_mpi.cpp` — параллельная версия 
- `tests/functional/main.cpp` — функциональные тесты

**Ключевые классы:**
- `BortsovaAMaxElemVectorSeq` — последовательная версия задачи
- `BortsovaAMaxElemVectorMpi` — параллельная версия задачи 

Оба класса наследуются от базового класса `ppc::task::Task` и реализуют интерфейс:
- `ValidationImpl()` — проверка корректности данных
- `PreProcessingImpl()` — подготовка к выполнению
- `RunImpl()` — основной алгоритм
- `PostProcessingImpl()` — завершение

### 5.2 Структуры данных

```cpp
struct InType {
  std::vector<int> data;  // входной вектор целых чисел
};

using OutType = int;  // максимальное значение
```

### 5.3 Важные особенности реализации

**Обработка граничных случаев:**
- Пустой вектор отклоняется на этапе валидации
- Используется `std::numeric_limits<int>::min()` как начальное значение для максимума
- Корректная работа с отрицательными числами и граничными значениями типа `int`

**MPI-специфичные особенности:**
- Использование `MPI_Scatterv` вместо `MPI_Scatter` для поддержки неравномерного распределения
- Вычисление смещений (`displs`) и размеров блоков (`sendcounts`) для каждого процесса
- Применение `MPI_Bcast` для синхронизации размера данных перед распределением

**Оптимизации:**
- Минимизация коммуникаций: только одна операция Scatter и одна Reduce
- Использование встроенной операции `MPI_MAX` для эффективного нахождения глобального максимума
- Локальные данные хранятся в `std::vector` для автоматического управления памятью

### 5.4 Использование памяти

- **Последовательная версия:** O(1) дополнительной памяти, не учитывая входных данных
- **Параллельная версия:**
  - Процесс 0: O(n) для хранения полного массива + O(p) для массивов смещений
  - Остальные процессы: O(n/p) для локального фрагмента данных
  - Общая память на всех процессах: O(n + p^2)

## 6. Экспериментальное окружение

### 6.1 Аппаратное и программное обеспечение

**Аппаратная конфигурация:**
- Процессор: Intel(R) Core(TM) Ultra 7 155H (3.80 GHz)
- Количество ядер/потоков: 16 ядер/22 потока
- Оперативная память: 32 ГБ
- Операционная система: Windows 11 pro

**Программное окружение:**
- Компилятор: MSVC 
- MPI версия: 10.0
- Тип сборки: Release
- CMake версия: 4.2.0-rc1

**Параметры окружения:**
- Количество процессов MPI: 1, 2, 3, 4
- Размер тестового вектора: 250,000,000 чисел
- Тип данных: `int` 
- Объем данных: ~953 МБ 

### 6.2 Генерация тестовых данных

Для экспериментов используются векторы, сгенерированные следующим образом:
```cpp
std::vector<int> CreateVector(size_t size, int max_value, size_t max_position) {
  std::vector<int> vec(size);
  for (size_t i = 0; i < size; ++i) {
    const int base = -static_cast<int>(size) + static_cast<int>(i);
    vec[i] = (base * 2) + 3;
  }
  if (max_position < size) {
    vec[max_position] = max_value;
  }
  return vec;
}
```


## 7. Результаты и обсуждение

### 7.1 Корректность

1. **Сравнение с эталонной функцией:** результат параллельной версии сравнивается с результатом функции `ComputeMaxValue`, которая использует простой перебор

2. **Граничные случаи:**
   - Вектор из одного элемента
   - Максимум в начале, середине и конце вектора
   - Все элементы равны
   - Отрицательные числа
   - Граничные значения типа `int`: `INT_MIN`, `INT_MAX`

3. **Различные размеры данных:**
   - Малые размеры (1-100 элементов)
   - Средние размеры (100-10,000 элементов)
   - Большие размеры (10,000-1,500,000 элементов)

4. **Различное количество процессов:**
   - Проверка корректности при 1, 2, 3, 4 и более процессах
   - Случаи, когда размер данных не делится нацело на количество процессов


### 7.2 Производительность

#### 7.2.1 Замеры времени выполнения в task_run

**Таблица 1. Время выполнения основного алгоритма (task_run)**

| Размер вектора | Режим | Число процессов | Время, с | Ускорение | Эффективность |
|----------------|-------|-----------------|----------|-----------|---------------|
| 250,000,000    | seq   | 1               | 0.2389   | 1.00      | N/A           |
| 250,000,000    | mpi   | 1               | 1.1445   | 0.21      | 20.9%         |
| 250,000,000    | mpi   | 2               | 0.8485   | 0.28      | 14.1%         |
| 250,000,000    | mpi   | 3               | 0.6933   | 0.34      | 11.5%         |
| 250,000,000    | mpi   | 4               | 0.7454   | 0.32      | 8.0%          |

Формулы для расчета:
- **Ускорение (Speedup):** `S = T_seq / T_parallel`
- **Эффективность (Efficiency):** `E = S / p × 100%`, где `p` — количество процессов

#### 7.2.2 Замеры времени выполнения в task_pipeline

**Таблица 2. Время выполнения полного пайплайна (task_pipeline)**

| Размер вектора | Режим | Число процессов | Время, с | Ускорение | Эффективность |
|----------------|-------|-----------------|----------|-----------|---------------|
| 250,000,000    | seq   | 1               | 0.2326   | 1.00      | N/A           |
| 250,000,000    | mpi   | 1               | 1.1489   | 0.20      | 20.2%         |
| 250,000,000    | mpi   | 2               | 0.8377   | 0.28      | 13.9%         |
| 250,000,000    | mpi   | 3               | 0.7191   | 0.32      | 10.8%         |
| 250,000,000    | mpi   | 4               | 0.7336   | 0.32      | 7.9%          |

#### 7.2.3 Анализ результатов

**Наблюдаемое поведение:**

1. **Отсутствие ускорения:**
   - MPI-версия на 1 процессе работает в 4.8 раза медленнее последовательной версии (1.145с vs 0.239с)
   - Даже на 4 процессах MPI-версия медленнее SEQ в 3.1 раза (0.745с vs 0.239с)
   - Ускорение отсутствует из-за превышения накладных расходов над выигрышем от параллелизма

2. **Причины низкой производительности:**
   
   **a) Высокие коммуникационные издержки:**
   - `MPI_Scatterv`: передача ~953 МБ данных от процесса 0 к остальным процессам
   - `MPI_Reduce`: сбор результатов (минимальные затраты - только одно число)
   - Время коммуникации превышает время вычислений локального максимума
   
   **b) Простота вычислений:**
   - Поиск максимума — операция O(n) с минимальными вычислениями на элемент
   - Один проход по массиву с простым сравнением `if (x > max)`
   - Вычислительная интенсивность слишком мала для эффективного параллелизма
   
   **c) Memory bandwidth bottleneck:**
   - Задача ограничена скоростью доступа к памяти (memory-bound)
   - CPU ожидает данные из RAM больше времени, чем выполняет сравнения
   - Параллелизм не помогает при узком месте в пропускной способности памяти

3. **Положительная динамика при увеличении процессов:**
   - При переходе от 1 к 3 процессам наблюдается улучшение (task_run: 1.145с → 0.693с)
   - Лучший результат на 3 процессах для task_run: 0.693 секунды
   - При 4 процессах время немного увеличилось до 0.745с (возможно, из-за оверхеда синхронизации)

4. **Разница между task_run и task_pipeline:**
   - Результаты практически идентичны (различие < 10%)
   - Pipeline включает pre/post processing, но они минимальны для данной задачи
   - Основное время тратится в RunImpl() — непосредственно на поиск максимума

**Выводы по результатам:**

- Корректность: все тесты проходят успешно
- Масштабируемость наблюдается при увеличении числа процессов 

## 8. Выводы

В ходе выполнения лабораторной работы была реализована параллельная версия алгоритма поиска максимального элемента в векторе с использованием технологии MPI и проведен детальный анализ её производительности.

**Достигнутые результаты:**
1. Реализована корректная параллельная версия алгоритма с использованием коллективных операций MPI
2. Обеспечена равномерная балансировка нагрузки между процессами с использованием `MPI_Scatterv`
3. Проведено тестирование на 42 различных наборах данных — все тесты пройдены успешно
4. Выполнены замеры производительности на векторе из 250 миллионов элементов
5. Проведен анализ производительности и выявлены узкие места реализации

**Что работает:**
- **Корректность:** реализация полностью корректна для всех тестовых случаев
- **Архитектура кода:** чистая структура с разделением на seq/mpi версии
- **Балансировка:** равномерное распределение нагрузки даже при n не кратном p
- **Коллективные операции:** правильное использование `MPI_Scatterv` и `MPI_Reduce`

**Ограничения и проблемы:**
- **Отсутствие ускорения:** MPI-версия медленнее последовательной в 3-5 раз
- **Коммуникационные издержки:** передача ~953 МБ данных занимает больше времени, чем вычисления
- **Характер задачи:** поиск максимума — слишком простая операция для параллелизма через MPI
- **Memory-bound:** задача ограничена пропускной способностью памяти, а не вычислительной мощностью

**Практические выводы:**
1. **Не каждая задача выигрывает от параллелизма** — важно оценивать соотношение вычислений к коммуникациям
2. Для задачи поиска максимума лучше подходят:
   - Последовательная реализация на одном процессе
   - SIMD-инструкции (AVX/SSE) для векторизации
   - OpenMP для многопоточности с разделяемой памятью (без накладных расходов на передачу данных)
3. MPI эффективен для задач с высокой вычислительной интенсивностью и минимальными коммуникациями

**Образовательная ценность:**
Несмотря на отсутствие ускорения, работа продемонстрировала:
- Правильное использование коллективных операций MPI
- Понимание накладных расходов параллелизма
- Умение анализировать и объяснять результаты производительности
- Критическое мышление при выборе технологии распараллеливания

## 9. Источники

1. MPI: A Message-Passing Interface Standard, Version 4.1. Message Passing Interface Forum, 2023.  
   URL: https://www.mpi-forum.org/docs/

2. Documentation for MS-MPI (Microsoft MPI).  
   URL: https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi

3. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.

4. Intel MPI Library Developer Reference.  
   URL: https://www.intel.com/content/www/us/en/docs/mpi-library/

5. Open MPI Documentation.  
   URL: https://www.open-mpi.org/doc/

## Приложение

### A. Фрагмент кода последовательной версии

```cpp
bool BortsovaAMaxElemVectorSeq::RunImpl() {
  const auto &vec = GetInput().data;

  if (vec.empty()) {
    return false;
  }

  int max_elem = vec[0];
  for (size_t i = 1; i < vec.size(); i++) {
    max_elem = std::max(vec[i], max_elem);
  }

  GetOutput() = max_elem;
  return true;
}
```

### B. Фрагмент параллельной версии 

```cpp
// вычисление размеров блоков для каждого процесса
int chunk_size = vec_size / size;
int remainder = vec_size % size;

std::vector<int> sendcounts(size);
std::vector<int> displs(size);

int offset = 0;
for (int i = 0; i < size; i++) {
  sendcounts[i] = chunk_size + (i < remainder ? 1 : 0);
  displs[i] = offset;
  offset += sendcounts[i];
}

// распределение данных
std::vector<int> local_data(sendcounts[rank]);
MPI_Scatterv(vec.data(), sendcounts.data(), displs.data(), MPI_INT,
             local_data.data(), sendcounts[rank], MPI_INT, 0, MPI_COMM_WORLD);

// локальный поиск максимума
int local_max = std::numeric_limits<int>::min();
if (!local_data.empty()) {
  local_max = local_data[0];
  for (size_t i = 1; i < local_data.size(); i++) {
    local_max = std::max(local_data[i], local_max);
  }
}

// cбор глобального максимума
int global_max = std::numeric_limits<int>::min();
MPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);
```

### C. Пример теста корректности

```cpp
TEST_P(BortsovaAMaxElemVectorFuncTests, MatmulFromPic) {
  ExecuteTest(GetParam());
}

```

