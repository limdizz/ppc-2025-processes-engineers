# Поиск максимального элемента в векторе

- Студент: Дергачёв Арсений Сергеевич, группа 3823Б1ПР3
- Технология: SEQ | MPI
- Вариант: 3
- Преподаватель: Сысоев Александр Владимирович, лектор, доцент кафедры высокопроизводительных вычислений и системногo программирования

## 1. Введение

Задача поиска максимального элемента в массиве является одной из базовых операций при работе с данными. При обработке больших объемов данных последовательное выполнение может занимать значительное время. Использование параллельных вычислений с помощью технологии MPI позволяет распределить работу между несколькими процессами и ускорить выполнение.

В данной работе реализованы две версии алгоритма:
- Последовательная (SEQ) - для сравнения производительности
- Параллельная (MPI) - с распределением данных между процессами

Цель работы: реализовать эффективную параллельную версию алгоритма и измерить достигнутое ускорение на разном количестве процессов.

## 2. Постановка задачи

**Входные данные:**
- Целое число N > 0 - размер вектора

**Выходные данные:**
- Целое число - максимальный элемент вектора

**Формат данных:**
- Элементы вектора генерируются по формуле: `value[idx] = (idx * 7) % 2000 - 1000`
- Диапазон значений: -1000 ≤ value[idx] ≤ 999

**Ограничения:**
- N > 0
- Используется детерминированная генерация (для воспроизводимости результатов)

Такой подход с генерацией вместо хранения массива позволяет экономить память и работать с очень большими размерами данных.

## 3. Базовый алгоритм (последовательный)

Последовательный алгоритм выполняет простой проход по всем элементам:

```
1. Инициализировать max_elem = INT_MIN
2. Для каждого индекса idx от 0 до N-1:
   a. Вычислить value = (idx * 7) % 2000 - 1000 (ф-ла генерации элементов вектора)
    Пояснение: 7 - множитель индекса для смены последовательности значений, 2000 - модуль для ограничения 
    диапазона, 1000 - сдвиг. Эта формула нужна для задания последовательности целых чисел в диапазоне [-1000, 999] без хранения массива.
   b. Если value > max_elem, то max_elem = value
3. Вернуть max_elem
```

**Сложность:**
- Временная: O(N) - один проход по всем элементам
- Пространственная: O(1) - массив не хранится в памяти

## 4. Схема параллелизации

### Распределение данных

Используется блочная декомпозиция с использованием `MPI_Scatterv` для распределения данных между процессами.

**Алгоритм распределения:**

1. **Генерация данных:** Процесс 0 создаёт полный вектор размером N, генерируя элементы по формуле: `(idx * 7) % 2000 - 1000`

2. **Вычисление размеров блоков:** Для каждого процесса вычисляются размеры данных и смещения:
   ```
   chunk_size[i] = N / P + (i < N % P ? 1 : 0)
   displacement[i] = (i * (N / P)) + min(i, N % P)
   ```
   Первые `remainder` процессов получают на 1 элемент больше для равномерного распределения.

3. **Распределение через MPI_Scatterv:** Процесс 0 рассылает блоки данных всем процессам согласно вычисленным размерам и смещениям.

4. **Локальное хранение:** Каждый процесс получает свой блок данных в локальный вектор `local_data_`.

Использование `MPI_Scatterv` необходимо, так как при N % P != 0 разные процессы получают блоки разного размера.

### Коммуникация

1. **MPI_Bcast (PreProcessing)** - процесс 0 рассылает размер вектора N всем процессам
2. **MPI_Scatterv (PreProcessing)** - процесс 0 распределяет блоки данных между всеми процессами с учётом неравномерных размеров
3. **Локальные вычисления (Run)** - каждый процесс находит максимум в своём блоке полученных данных
4. **MPI_Allreduce (Run)** - сбор локальных максимумов и поиск глобального максимума с операцией MPI_MAX

Схема:
```
Процесс 0: [0 ... chunk_0] -> local_max_0  ─┐
Процесс 1: [... chunk_1 ...] -> local_max_1 ├─ -> MPI_Allreduce(MAX) -> global_max
...                                         │
Процесс P-1: [... N-1] -> local_max_P-1     ┘
```

### Роли процессов

- **Процесс 0 (Master):**
  - PreProcessing: Получает размер вектора из входных данных, генерирует полный вектор, рассылает размер через MPI_Bcast, распределяет данные через MPI_Scatterv 
  - Run: Обрабатывает свой блок полученных данных, участвует в MPI_Allreduce
  - PostProcessing: Сохраняет и проверяет результат

- **Процессы 1..P-1:**
  - PreProcessing: Получают размер вектора через MPI_Bcast, получают свой блок данных через MPI_Scatterv
  - Run: Обрабатывают свой блок полученных данных, участвуют в MPI_Allreduce
  - PostProcessing: Проверяют корректность результата

Преимущества: явное распределение данных через MPI_Scatterv, автоматическая балансировка нагрузки, правильное разделение этапов pipeline.

## 5. Детали реализации

### Структура кода

**Файлы:**
- `common/include/common.hpp` - общие типы данных
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` - последовательная версия
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` - параллельная версия
- `tests/functional/main.cpp` - функциональные тесты
- `tests/performance/main.cpp` - тесты производительности

**Классы:**
- `DergachevAMaxElemVecSEQ` - последовательная реализация
- `DergachevAMaxElemVecMPI` - параллельная реализация

**Методы:**
- `ValidationImpl()` - проверка входных данных (N > 0)
- `PreProcessingImpl()` - распределение данных: процесс 0 генерирует полный вектор и распределяет его между процессами через `MPI_Scatterv`, размер вектора рассылается через `MPI_Bcast`
- `RunImpl()` - основной алгоритм: каждый процесс вычисляет локальный максимум в своём блоке полученных данных, затем через `MPI_Allreduce` находится глобальный максимум
- `PostProcessingImpl()` - проверка результата

### Граничные случаи

- **N = 1:** Работает корректно, один процесс обрабатывает элемент
- **N < P:** Некоторые процессы получают пустые блоки данных. Они используют локальный максимум = INT_MIN и корректно участвуют в `MPI_Allreduce`, не влияя на результат
- **N % P != 0:** Остаток распределяется между первыми процессами (процессы с рангом < remainder получают на 1 элемент больше)

### Использование памяти

- Последовательная версия: O(1)
- Параллельная версия: 
  - Процесс 0: O(N) для генерации полного вектора + O(N/P) для локального блока
  - Процессы 1...P-1: O(N/P) для локального блока
  - Временная память для send_counts и displacements: O(P)

## 6. Экспериментальная установка

### Аппаратное и программное обеспечение

- **CPU:** 11th Gen Intel(R) Core(TM) i3-1115G4 @ 3.00GHz   3.00 GHz
- **Ядра/Потоки:** 2 ядра / 4 потока
- **ОС:** Windows 11
- **Компилятор:** MSVC 14.44
- **Тип сборки:** Release 
- **MPI реализация:** MS-MPI 10.0
- **CMake:** 4.2.0-rc1
- **Фреймворк тестирования:** Google Test

### Параметры тестирования

**Функциональные тесты:**
- Размеры: 1, 2, 3, 5, 7, 17, 31, 64, 99, 128, 256, 50000 элементов
- Проверка корректности: сравнение SEQ и MPI результатов

**Тесты производительности:**
- Размер вектора: 800,000,000 элементов 
- Количество процессов: 1, 2, 3, 4
- Режимы: task_run, task_pipeline

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность реализации проверена следующими способами:

1. **Функциональные тесты:** Все 24 теста пройдены 
2. **Сравнение SEQ и MPI:** Результаты идентичны на всех тестовых данных
3. **Проверка инвариантов:** Результат всегда находится в диапазоне [-1000, 999]
4. **Граничные случаи:** Протестированы векторы размером от 1 до 50000 элементов

### 7.2 Производительность

Результаты измерений на векторе из 800,000,000 элементов.

*Замечание: все процессы были действительно протестированы на 800,000,000 элементах. Но из-за того, 
что CI падал при таком наборе данных, пришлось "откатить" размер вектора до 100,000,000 элементов,
иначе реквест просто не проходил. Было принято решение оставить замеры в таблице для 800,000,000
элементов, а в коде оставить предыдущий размер в 100,000,000 элементов.

**task_pipeline:**

| Режим | Процессов | Время, сек | Ускорение | Эффективность |
|-------|-----------|------------|-----------|---------------|
| seq   | 1         | 1.8481     | 1.00      | N/A           |
| mpi   | 1         | 1.8159     | 1.02      | 101.8%        |
| mpi   | 2         | 1.1442     | 1.62      | 80.8%         |
| mpi   | 3         | 1.0232     | 1.81      | 60.2%         |
| mpi   | 4         | 1.1145     | 1.66      | 41.5%         |

**task_run:**

| Режим | Процессов | Время, сек | Ускорение | Эффективность |
|-------|-----------|------------|-----------|---------------|
| seq   | 1         | 1.8387     | 1.00      | N/A           |
| mpi   | 1         | 1.8150     | 1.01      | 101.3%        |
| mpi   | 2         | 1.1236     | 1.64      | 81.8%         |
| mpi   | 3         | 1.0217     | 1.80      | 60.0%         |
| mpi   | 4         | 1.0995     | 1.67      | 41.8%         |

**Расчет показателей:**
- Ускорение (Speedup) = T_seq / T_mpi
- Эффективность (Efficiency) = Speedup / P × 100%

**Анализ результатов:**

1. **1 процесс MPI:** В обоих режимах время практически равно seq версии (ускорение ~1.01×), что подтверждает минимальные накладные расходы MPI при одном процессе.

2. **2 процесса:** 
   - task_pipeline: ускорение 1.62× при эффективности 80.8%
   - task_run: ускорение 1.64× при эффективности 81.8%
   - Оба режима показывают хорошее масштабирование с эффективностью выше 80%

3. **3 процесса:** 
   - task_pipeline: ускорение 1.81× при эффективности 60.2% - максимальное ускорение в этом режиме
   - task_run: ускорение 1.80× при эффективности 60.0% - максимальное ускорение в этом режиме
   - Оба режима достигают пикового ускорения на 3 процессах

4. **4 процесса:** 
   - task_pipeline: ускорение 1.66× при эффективности 41.5% - снижение производительности
   - task_run: ускорение 1.67× при эффективности 41.8%
   - Снижение эффективности по сравнению с 3 процессами связано с ростом коммуникационных затрат

**Особенности:**

- Максимальное ускорение достигается на 3 процессах в обоих режимах (~1.8×)
- Эффективность остается высокойна 2 процессах
- На 3-4 процессах эффективность снижается до 40-60% из-за увеличения коммуникационных затрат

**Масштабируемость:**

Алгоритм показывает умеренную масштабируемость на больших объемах данных. Хорошее ускорение достигается на 2 процессах (1.6×), максимальное на 3 процессах (1.8×). При увеличении до 4 процессов производительность снижается из-за роста коммуникационных затрат относительно вычислительной работы. 

## 8. Выводы

В ходе работы были получены следующие результаты:

1. **Реализация:** Разработаны и протестированы последовательная и параллельная версии алгоритма поиска максимального элемента в векторе.

2. **Корректность:** Все функциональные тесты пройдены. SEQ и MPI версии дают одинаковые результаты на всех тестовых данных.

3. **Производительность:** На векторе из 800 млн элементов достигнуто максимальное ускорение:
   - task_pipeline режим: 1.81× на 3 процессах 
   - task_run режим: 1.80× на 3 процессах 

4. **Оптимизация памяти:** Использование детерминированной генерации данных вместо хранения массива позволило достичь O(1) использования памяти.

5. **Ограничения:** 
   - Эффективность снижается при увеличении количества процессов из-за коммуникационных затрат
   - При более сложных вычислениях на элемент ожидается лучшая масштабируемость

6. **Практическая применимость:** Разработанный алгоритм может использоваться для обработки больших массивов данных в научных вычислениях и системах анализа данных.

## 9. Список источников информации

1. MPI Forum. MPI: A Message-Passing Interface Standard, Version 4.0. 2021. https://www.mpi-forum.org/docs/
2. Gropp W., Lusk E., Skjellum A. Using MPI: Portable Parallel Programming with the Message-Passing Interface. MIT Press, 2014.
3. Microsoft. Microsoft MPI Documentation. https://learn.microsoft.com/en-us/message-passing-interface/microsoft-mpi
4. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.

## Приложение

### Фрагменты кода

**Последовательная версия (SEQ):**

```cpp
bool DergachevAMaxElemVecSEQ::RunImpl() {
  if (GetInput() <= 0) {
    return false;
  }

  const int vector_size = GetInput();
  InType current_max = std::numeric_limits<InType>::min();

  for (int idx = 0; idx < vector_size; ++idx) {
    const auto current_value = static_cast<InType>(((idx * 7) % 2000) - 1000);
    current_max = std::max(current_value, current_max);
  }

  GetOutput() = current_max;
  return true;
}
```

**Параллельная версия (MPI):**

*PreProcessing - генерация и распределение данных через MPI_Scatterv:*

```cpp
bool DergachevAMaxElemVecMPI::PreProcessingImpl() {
  int process_rank = 0;
  int total_processes = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &process_rank);
  MPI_Comm_size(MPI_COMM_WORLD, &total_processes);

  if (process_rank == 0) {
    vector_size_ = GetInput();
    if (vector_size_ <= 0) return false;
  }
  MPI_Bcast(&vector_size_, 1, MPI_INT, 0, MPI_COMM_WORLD);

  const int base_chunk_size = vector_size_ / total_processes;
  const int remainder = vector_size_ % total_processes;
  
  std::vector<int> send_counts(total_processes);
  std::vector<int> displacements(total_processes);
  
  for (int i = 0; i < total_processes; ++i) {
    send_counts[i] = base_chunk_size + (i < remainder ? 1 : 0);
    displacements[i] = (i * base_chunk_size) + std::min(i, remainder);
  }

  std::vector<InType> full_data;
  if (process_rank == 0) {
    full_data.resize(vector_size_);
    for (int idx = 0; idx < vector_size_; ++idx) {
      full_data[idx] = ((idx * 7) % 2000) - 1000;
    }
  }

  local_data_.resize(send_counts[process_rank]);
  MPI_Scatterv(full_data.data(), send_counts.data(), 
               displacements.data(), MPI_INT, local_data_.data(),
               send_counts[process_rank], MPI_INT, 0, MPI_COMM_WORLD);

  return true;
}
```

*Run - вычисление локальных и глобального максимума:*

```cpp
bool DergachevAMaxElemVecMPI::RunImpl() {
  InType local_maximum = std::numeric_limits<InType>::min();

  for (const auto& value : local_data_) {
    local_maximum = std::max(value, local_maximum);
  }

  InType global_maximum = std::numeric_limits<InType>::min();
  MPI_Allreduce(&local_maximum, &global_maximum, 1, MPI_INT, 
                MPI_MAX, MPI_COMM_WORLD);

  GetOutput() = global_maximum;
  return true;
}
```

**Формула распределения данных:**

```cpp
int base_chunk_size = N / P;

int remainder = N % P;

int start = (rank * base_chunk_size) + min(rank, remainder);

int end = start + base_chunk_size + (rank < remainder ? 1 : 0);
```
