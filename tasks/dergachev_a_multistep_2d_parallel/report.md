# Многошаговая схема решения двумерных задач глобальной оптимизации. Распараллеливание по характеристикам

- Студент: Дергачёв Арсений Сергеевич, группа 3823Б1ПР3
- Технология: MPI, SEQ
- Вариант: 13
- Преподаватель: Сысоев Александр Владимирович, лектор, доцент кафедры высокопроизводительных вычислений и системногo программирования

## 1. Введение

В лабораторной работе реализована многошаговая схема решения двумерных задач глобальной оптимизации методом характеристик. Алгоритм основан на информационно-статистическом подходе Стронгина и использует кривые Пеано, заполняющие пространство, для сведения двумерной задачи к одномерному случаю. Распараллеливание достигается с использованием интерфейса передачи сообщений (MPI) для распределения вычисления характеристик интервалов между несколькими процессами.

## 2. Постановка задачи

### Формальное определение

Дана непрерывная функция f(x, y), определенная на прямоугольной области D = [x_min, x_max] x [y_min, y_max]. Требуется найти глобальный минимум:

минимизировать f(x, y) при условии: (x, y) принадлежит D


### Входные параметры

| Параметр | Описание |
|----------|----------|
| func | Целевая функция f(x, y) |
| x_min, x_max | Границы по координате x |
| y_min, y_max | Границы по координате y |
| epsilon | Точность сходимости |
| r_param | Параметр надежности (должен быть > 1.0) |
| max_iterations | Максимальное число итераций |

### Выходные данные

| Поле | Описание |
|------|----------|
| x_opt | Оптимальная координата x |
| y_opt | Оптимальная координата y |
| func_min | Минимальное значение функции |
| iterations | Число выполненных итераций |
| converged | Флаг сходимости алгоритма |

### Ограничения

- Функция должна быть липшицевой на области определения
- x_min < x_max и y_min < y_max
- epsilon > 0
- r_param > 1.0
- max_iterations > 0

## 3. Описание последовательного алгоритма 

### 3.1 Отображение кривой Пеано

Алгоритм использует кривую Пеано, заполняющую пространство, для отображения двумерного пространства поиска на одномерный интервал [0, 1]. Это преобразование сохраняет свойства локальности: близкие точки на кривой соответствуют близким точкам в двумерном пространстве.

Отображение реализовано рекурсивно путем деления единичного квадрата на квадранты:

```
PeanoMap(t, level):
    x = 0, y = 0
    scale = 0.5
    
    для i от 0 до level-1:
        quadrant = floor(t * 4)
        t = t * 4 - quadrant
        
        (tx, ty) = позиция в зависимости от квадранта (0-3)
        x += tx * scale
        y += ty * scale
        scale *= 0.5
    
    вернуть (x, y)
```

### 3.2 Метод глобальной оптимизации Стронгина

Алгоритм выполняет следующие шаги:

1. **Инициализация**: Создать две пробные точки при t=0 и t=1 на кривой Пеано, вычислить значения функции.

2. **Основной цикл**:

   2.1. **Сортировка испытаний** по значениям параметра t
   
   2.2. **Оценка константы Липшица** M:
   ```
   M = max(|z[i] - z[i-1]| / |t[i] - t[i-1]|) для всех смежных пар
   ```
   
   2.3. **Вычисление характеристик** для каждого интервала [t[i], t[i+1]]:
   ```
   R[i] = m*delta + (diff^2)/(m*delta) - 2*(z[i+1] + z[i])
   
   где:
     delta = t[i+1] - t[i]
     diff = z[i+1] - z[i]
     m = r_param * M
   ```
   
   2.4. **Выбор интервала** с максимальной характеристикой
   
   2.5. **Вычисление новой пробной точки**:
   ```
   t_new = 0.5*(t_left + t_right) - (z_right - z_left)/(2*m)
   ```
   
   2.6. **Проверка сходимости**: если delta < epsilon, остановка
   
   2.7. **Выполнение испытания**: вычислить функцию в новой точке

3. **Постобработка**: Найти точку с минимальным значением функции среди всех испытаний.

## 4. Схема распараллеливания

### 4.1 Стратегия распределения данных

MPI-распараллеливание сосредоточено на распределении вычисления характеристик интервалов между несколькими процессами. Это наиболее вычислительно затратная часть при большом количестве испытаний.


Процесс 0:
  - Управляет всеми пробными точками
  - Координирует итерации
  - Выполняет вычисление новых испытаний
  - Рассылает обновленные данные

Процессы 1..N-1:
  - Получают данные интервалов
  - Вычисляют локальные характеристики
  - Отправляют результаты главному процессу

### 4.2 Схема коммуникаций

**Все MPI-коммуникации выполняются в RunImpl:**

1. **Инициализация и рассылка начальных данных (MPI_Send/MPI_Recv)**:
   - Главный процесс создает начальные пробные точки
   - MPI_Send рассылает начальные данные всем остальным процессам
   - MPI_Recv на рабочих процессах принимает данные

2. **Рассылка данных испытаний (MPI_Bcast)**:
   - Размер массивов испытаний
   - Массив t-значений
   - Данные пробных точек 

3. **Распределение данных интервалов (MPI_Scatterv)**:
   - Интервалы равномерно распределяются между процессами
   - Каждый интервал содержит: t_left, t_right, z_left, z_right

4. **Сбор характеристик (MPI_Send/MPI_Recv)**:
   - Каждый процесс отправляет вычисленные характеристики главному
   - Главный собирает полный массив характеристик

5. **Рассылка результатов (MPI_Bcast)**:
   - Все характеристики рассылаются для согласованного выбора
   - Флаг сходимости рассылается всем процессам

### 4.3 Балансировка нагрузки

Распределение использует сбалансированную схему:

```
base_count = num_intervals / world_size
remainder = num_intervals % world_size

counts[i] = base_count + (i < remainder ? 1 : 0)
```

Это гарантирует, что разница в работе между любыми двумя процессами составляет не более одного интервала.

## 5. Детали реализации

### 5.1 Структура кода

| Файл | Описание |
|------|----------|
| common/include/common.hpp | Структуры данных, функции отображения Пеано |
| seq/include/ops_seq.hpp | Объявление класса последовательной задачи |
| seq/src/ops_seq.cpp | Реализация последовательного алгоритма |
| mpi/include/ops_mpi.hpp | Объявление класса MPI-задачи |
| mpi/src/ops_mpi.cpp | Реализация параллельного алгоритма |
| tests/functional/main.cpp | Функциональные тесты |
| tests/performance/main.cpp | Тесты производительности |

### 5.2 Основные классы и структуры

- **OptimizationInput**: Контейнер входных параметров
- **OptimizationResult**: Контейнер выходных результатов
- **TrialPoint**: Хранит (x, y, z) для каждого испытания
- **DergachevAMultistep2dParallelSEQ**: Реализация последовательного алгоритма
- **DergachevAMultistep2dParallelMPI**: Реализация параллельного алгоритма

### 5.3 Важные допущения

- Целевая функция передается как std::function и должна быть вызываемой на всех MPI-процессах
- Уровень кривой Пеано фиксирован на значении 8, что обеспечивает достаточную точность для большинства приложений
- Вычисление функции предполагается детерминированным

### 5.4 Использование памяти

- Пробные точки хранятся на всех процессах после рассылки
- Память растет линейно с числом итераций
- Максимальное использование памяти: O(max_iterations * sizeof(TrialPoint))

## 6. Экспериментальная установка

### 6.1 Аппаратное обеспечение и ОС

- **CPU:** 11th Gen Intel(R) Core(TM) i3-1115G4 @ 3.00GHz   3.00 GHz
- **Ядра/Потоки:** 2 ядра / 4 потока
- **ОС:** Windows 11
- **Компилятор:** MSVC 14.44
- **Тип сборки:** Release 
- **MPI реализация:** MS-MPI 10.0
- **CMake:** 4.2.0-rc1
- **Фреймворк тестирования:** Google Test
### 6.2 Тестовые функции

Использовались следующие тестовые функции:

1. **Сферическая функция**: f(x,y) = x^2 + y^2
   - Глобальный минимум: (0, 0), f_min = 0

2. **Простая квадратичная**: f(x,y) = (x-2)^2 + (y-3)^2
   - Глобальный минимум: (2, 3), f_min = 0

3. **Функция Матьяса**: f(x,y) = 0.26*(x^2 + y^2) - 0.48*x*y
   - Глобальный минимум: (0, 0), f_min = 0

### 6.3 Конфигурация тестов

- Тест производительности: max_iterations = 2000, epsilon = 0.0001
- Область: [-5, 5] x [-5, 5]
- r_param = 2.5

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность проверялась при помощи:

1. **Модульные тесты** для структур данных (равенство OptimizationResult, сравнение TrialPoint)
2. **Граничные тесты** для функций отображения Пеано
3. **Тесты валидации** для проверки входных параметров
4. **Функциональные тесты** сравнения SEQ и MPI реализаций на известных тестовых функциях
5. **Тесты конвейера** проверки полного рабочего процесса

Все тесты проверяют, что:
- Результаты попадают в заданную область поиска
- Число итераций неотрицательно
- MPI и SEQ реализации дают согласованные результаты

### 7.2 Производительность

**pipeline:**

| Режим | Процессов | Время, сек | Ускорение | Эффективность |
|-------|-----------|------------|-----------|---------------|
| seq   | 1         | 0.4499     | 1.00      | N/A           |
| mpi   | 1         | 0.4781     | 0.94      | 94.1%         |
| mpi   | 2         | 0.7211     | 0.62      | 31.2%         |
| mpi   | 3         | 0.9951     | 0.45      | 15.1%         |
| mpi   | 4         | 1.1491     | 0.39      | 9.8%          |

**task_run:**

| Режим | Процессов | Время, сек | Ускорение | Эффективность |
|-------|-----------|------------|-----------|---------------|
| seq   | 1         | 0.3614     | 1.00      | N/A           |
| mpi   | 1         | 0.4653     | 0.78      | 77.7%         |
| mpi   | 2         | 0.6188     | 0.58      | 29.2%         |
| mpi   | 3         | 0.9959     | 0.36      | 12.1%         |
| mpi   | 4         | 1.1045     | 0.33      | 8.2%          |

**Расчет показателей:**

- Ускорение (Speedup) = T_seq / T_mpi
- Эффективность (Efficiency) = Speedup / P x 100%
- Базовое время seq измерено без использования mpiexec для объективного сравнения

### 7.3 Анализ результатов

Результаты показывают **отрицательное масштабирование** - MPI версия работает медленнее последовательной. Это объясняется следующими факторами:

1. **Накладные расходы на коммуникации** (все в RunImpl): 
   - MPI_Send/MPI_Recv для рассылки начальных данных
   - На каждой итерации:
     - MPI_Bcast для рассылки данных испытаний
     - MPI_Scatterv для распределения интервалов
     - MPI_Send/MPI_Recv для сбора характеристик
     - MPI_Bcast для рассылки результатов и флага сходимости

2. **Малый объем вычислений на итерацию**: Вычисление характеристики интервала - простая арифметическая операция, которая выполняется за наносекунды. Накладные расходы на передачу данных многократно превышают время вычислений.

3. **Последовательные узкие места**: 
   - Только главный процесс выполняет новые испытания
   - Сортировка выполняется на всех процессах после каждой рассылки

4. **Синхронизация**: Все процессы должны ждать завершения коммуникаций на каждой итерации.

### 7.4 Масштабируемость

Для достижения положительного ускорения необходимо:
- Увеличить вычислительную сложность целевой функции
- Использовать многошаговую схему с несколькими испытаниями за итерацию
- Применить асинхронные коммуникации для перекрытия вычислений и передачи данных

При текущей реализации и размере задачи последовательная версия является оптимальным выбором

## 8. Выводы

### Что работает хорошо

1. Кривая Пеано успешно сводит двумерную оптимизацию к одномерной
2. Метод Стронгина эффективно балансирует исследование и эксплуатацию
3. MPI-распараллеливание вычисления характеристик масштабируется с количеством интервалов
4. Реализация корректно обрабатывает граничные случаи и валидирует входные данные

### Ограничения

1. Уровень кривой Пеано фиксирован, что может ограничивать точность 
2. Вычисление испытаний последовательное 
3. Алгоритм может требовать много итераций для сильно мультимодальных функций

### Возможные улучшения

1. Распараллелить вычисление испытаний между процессами
2. Реализовать адаптивный выбор уровня кривой Пеано
3. Добавить поддержку многошагового распараллеливания с несколькими испытаниями за итерацию
4. Оптимизировать схемы коммуникаций с использованием асинхронных MPI-операций

## 9. Источники

1. Сергеев Я.Д., Квасов Д.Е. Диагональные методы глобальной оптимизации. ФизМатЛит, Москва, 2008.

2. MPI Forum. MPI: Стандарт интерфейса передачи сообщений, версия 3.1. 2015.

3. Гергель В.П., Стронгин Р.Г. Параллельные вычисления для задач глобальной оптимизации. МГУ, 2003.

4. Microsoft. Microsoft MPI Documentation. https://learn.microsoft.com/en-us/message-passing-interface/microsoft-mpi

5. Баркалов К.А., Использование параллельных характеристических алгоритмов для решения многомерных задач глобальной оптимизации, 2014

## Приложение. Фрагменты кода

### Инициализация и рассылка начальных данных (RunImpl)

```cpp
bool DergachevAMultistep2dParallelMPI::RunImpl() {
  const auto &input = GetInput();
  auto &output = GetOutput();

  trials_.clear();
  t_values_.clear();

  std::vector<double> initial_data(8);

  if (world_rank_ == 0) {
    double t0 = 0.0;
    double t1 = 1.0;

    double x0 = PeanoToX(t0, input.x_min, input.x_max, input.y_min, input.y_max, peano_level_);
    double y0 = PeanoToY(t0, input.x_min, input.x_max, input.y_min, input.y_max, peano_level_);
    double z0 = input.func(x0, y0);

    double x1 = PeanoToX(t1, input.x_min, input.x_max, input.y_min, input.y_max, peano_level_);
    double y1 = PeanoToY(t1, input.x_min, input.x_max, input.y_min, input.y_max, peano_level_);
    double z1 = input.func(x1, y1);

    initial_data[0] = t0; initial_data[1] = x0; initial_data[2] = y0; initial_data[3] = z0;
    initial_data[4] = t1; initial_data[5] = x1; initial_data[6] = y1; initial_data[7] = z1;

    for (int proc = 1; proc < world_size_; ++proc) {
      MPI_Send(initial_data.data(), 8, MPI_DOUBLE, proc, 0, MPI_COMM_WORLD);
    }
  } else {
    MPI_Recv(initial_data.data(), 8, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
  }

  t_values_.push_back(initial_data[0]);
  t_values_.push_back(initial_data[4]);
  trials_.emplace_back(initial_data[1], initial_data[2], initial_data[3]);
  trials_.emplace_back(initial_data[5], initial_data[6], initial_data[7]);
}
```

### Отображение кривой Пеано

```cpp
inline void PeanoMap(double t, int level, double &x, double &y) {
  x = 0.0;
  y = 0.0;
  double scale = 0.5;

  for (int i = 0; i < level; ++i) {
    int quadrant = static_cast<int>(t * 4.0);
    t = (t * 4.0) - quadrant;

    double tx = 0.0;
    double ty = 0.0;
    switch (quadrant) {
      case 0: tx = 0.0; ty = 0.0; break;
      case 1: tx = 0.0; ty = 1.0; break;
      case 2: tx = 1.0; ty = 1.0; break;
      case 3: tx = 1.0; ty = 0.0; break;
    }
    x += tx * scale;
    y += ty * scale;
    scale *= 0.5;
  }
}
```

### Вычисление характеристики интервала

```cpp
double ComputeCharacteristic(int idx, double m_val) {
  double t_i = t_values_[idx];
  double t_i1 = t_values_[idx + 1];
  double z_i = trials_[idx].z;
  double z_i1 = trials_[idx + 1].z;

  double delta = t_i1 - t_i;
  double diff = z_i1 - z_i;

  return (m_val * delta) + ((diff * diff) / (m_val * delta)) - (2.0 * (z_i1 + z_i));
}
```

### Параллельное вычисление характеристик (MPI)

```cpp
void ComputeCharacteristicsParallel(double m_val, std::vector<double> &characteristics) {
  int num_intervals = static_cast<int>(t_values_.size()) - 1;

  std::vector<int> counts, displs;
  ComputeDistribution(num_intervals, world_size_, counts, displs);

  std::vector<double> interval_data;
  if (world_rank_ == 0) {
    PrepareIntervalData(t_values_, trials_, num_intervals, interval_data);
  }

  int local_count = counts[world_rank_];
  std::vector<double> local_interval_data(local_count * 4);
  MPI_Scatterv(interval_data.data(), send_counts.data(), send_displs.data(), 
               MPI_DOUBLE, local_interval_data.data(), local_count * 4, 
               MPI_DOUBLE, 0, MPI_COMM_WORLD);

  std::vector<double> local_chars;
  ComputeLocalCharacteristics(local_interval_data, local_count, m_val, local_chars);

  characteristics.resize(num_intervals);
  GatherResultsToRoot(local_chars, counts, displs, world_rank_, world_size_, characteristics);
  MPI_Bcast(characteristics.data(), num_intervals, MPI_DOUBLE, 0, MPI_COMM_WORLD);
}
```

### Основной цикл оптимизации

```cpp
for (int iter = 0; iter < input.max_iterations; ++iter) {
  SortTrialsByT();
  m_estimate_ = ComputeLipschitzEstimate();

  int best_idx = SelectBestInterval();
  double t_left = t_values_[best_idx];
  double t_right = t_values_[best_idx + 1];

  double m_val = input.r_param * m_estimate_;
  double t_new = 0.5 * (t_left + t_right) - (z_right - z_left) / (2.0 * m_val);

  if (t_right - t_left < input.epsilon) {
    output.converged = true;
    break;
  }

  double z_new = PerformTrial(t_new);
  t_values_.push_back(t_new);
  trials_.emplace_back(x_new, y_new, z_new);
}
```
