# Метод простой итерации

- Студент: Дергачёв Арсений Сергеевич, группа 3823Б1ПР3
- Технология: MPI, SEQ
- Вариант: 20
- Преподаватель: Сысоев Александр Владимирович, лектор, доцент кафедры высокопроизводительных вычислений и системногo программирования

## 1. Введение

Целью данной работы является реализация параллельного варианта метода простой итерации с использованием технологии MPI (Message Passing Interface) и сравнение его производительности с последовательной версией.

## 2. Постановка задачи

**Задача:** Решить систему линейных уравнений **Ax = b** методом простой итерации, где:
- **A** — квадратная матрица размером n*n (в данной реализации используется единичная матрица I)
- **b** — вектор правых частей размером n (заполнен единицами: b = [1, 1, ..., 1]^T)
- **x** — искомый вектор решения размером n

**Входные данные:**
- Целое число n > 0 — размерность системы

**Выходные данные:**
- Целое число — округленная сумма компонент решения sum(x[i])

**Ограничения:**
- n > 0
- Максимальное число итераций: 1000
- Точность сходимости: epsilon = 10^(-6)
- Параметр релаксации: tau = 0.5

## 3. Описание последовательного алгоритма

Метод простых итераций основан на итерационной схеме:

**x^(k+1) = x^(k) - tau*(A*x^(k) - b)**

где:
- **x^(k)** — приближение решения на k-й итерации
- **tau** — параметр релаксации (шаг итерации)
- **A** — матрица системы
- **b** — вектор правых частей

**Алгоритм:**

1. **Инициализация:**
   - Начальное приближение: x^(0) = [0, 0, ..., 0]^T
   - Инициализация матрицы A как единичной: A[i][i] = 1
   - Вектор правых частей: b = [1, 1, ..., 1]^T

2. **Итерационный процесс** (для k = 0, 1, 2, ...):
   - Для каждой компоненты i:
     - Вычислить (Ax)[i] = sum_j(A[i][j] * x^(k)[j])
     - Обновить: x^(k+1)[i] = x^(k)[i] - tau * ((Ax)[i] - b[i])
   - Вычислить норму изменения: ||x^(k+1) - x^(k)||_2
   - Если ||x^(k+1) - x^(k)||_2 < epsilon, завершить
   - Если k >= max_iterations, завершить

3. **Постобработка:**
   - Вычислить сумму компонент решения: result = sum_i(x[i])
   - Округлить до целого числа

**Сложность:** O(n^2 * iter), где iter — количество итераций до сходимости.

## 4. Схема распараллеливания (MPI)

Распараллеливание основано на декомпозиции по строкам матрицы A между MPI-процессами.

### 4.1 Распределение данных

**Топология:** Master-Worker с коллективными операциями

**Распределение строк:** Каждому процессу с рангом `rank` назначается блок строк:
- Количество строк на процесс: `rows_per_proc = n / size`
- Остаток: `remainder = n % size`
- Процессы с `rank < remainder` получают на одну строку больше
- Начальная строка процесса: `start_row = rank * rows_per_proc + min(rank, remainder)`
- Количество строк процесса: `local_rows = rows_per_proc + (rank < remainder ? 1 : 0)`

**Пример распределения** для n=10, size=3:
- Процесс 0: строки 0-3 
- Процесс 1: строки 4-7 
- Процесс 2: строки 8-9 

### 4.2 Схема коммуникации

На каждой итерации:

1. **Локальные вычисления:** Каждый процесс вычисляет произведение своих строк матрицы A на текущий вектор x:
   ```
   для i от 0 до local_rows:
       global_i = start_row + i
       local_ax[i] = sum_j(A[global_i][j] * x[j])
   ```

2. **Коллективная операция MPI_Allgatherv:**
   - Собирает локальные результаты `local_ax` от всех процессов в глобальный вектор `ax_global`
   - Все процессы получают полный вектор `ax_global`

3. **Обновление вектора x:** Все процессы одинаково обновляют весь вектор x:
   ```
   для i от 0 до n:
       x_new[i] = x[i] - tau * (ax_global[i] - b[i])
   ```

4. **Проверка сходимости:** Вычисление нормы ||x_new - x|| выполняется на всех процессах

5. **Финализация:**
   - Процесс 0 вычисляет итоговый результат
   - `MPI_Bcast` распространяет результат на все процессы

## 5. Детали реализации

### 5.1 Структура кода

**Файлы:**
- `common/include/common.hpp` — общие типы и базовый класс Task
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` — последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` — параллельная реализация
- `tests/functional/main.cpp` — функциональные тесты 
- `tests/performance/main.cpp` — тесты производительности

**Основные классы:**
- `DergachevASimpleIterationMethodSEQ` — последовательная версия
- `DergachevASimpleIterationMethodMPI` — параллельная версия

**Основные методы:**
- `ValidationImpl()` — проверка входных данных (n > 0)
- `PreProcessingImpl()` — инициализация выходных данных
- `RunImpl()` — основной вычислительный процесс
- `PostProcessingImpl()` — проверка корректности результата

### 5.2 Вспомогательные функции (MPI)

```cpp

void ComputeRowDistribution(int n, int size, 
    std::vector<int> &recvcounts, std::vector<int> &displs);

int ComputeFinalResult(const std::vector<double> &x, int n);

void InitializeIdentityMatrix(std::vector<std::vector<double>> &a, int n);

double ComputeVectorNorm(const std::vector<double> &x_new, 
    const std::vector<double> &x, int n);
```

### 5.3 Особенности реализации

**Память:**
- Матрица A хранится на всех процессах 
- Вектор x реплицируется на всех процессах и обновляется синхронно
- Промежуточные результаты `local_ax` хранятся локально

**Обработка граничных случаев:**
- Если n <= 0, функция возвращает `false`
- Если число итераций достигло максимума, возвращается текущее приближение

**Синхронизация:**
- `MPI_Allgatherv` обеспечивает синхронизацию результатов между итерациями
- `MPI_Bcast` используется для распространения финального результата

## 6. Экспериментальное окружение

### 6.1 Аппаратное и программное обеспечение

- **CPU:** 11th Gen Intel(R) Core(TM) i3-1115G4 @ 3.00GHz   3.00 GHz
- **Ядра/Потоки:** 2 ядра / 4 потока
- **ОС:** Windows 11
- **Компилятор:** MSVC 14.44
- **Тип сборки:** Release 
- **MPI реализация:** MS-MPI 10.0
- **CMake:** 4.2.0-rc1
- **Фреймворк тестирования:** Google Test

### 6.2 Параметры тестирования

- **Переменная окружения:** `PPC_NUM_PROC` — количество MPI процессов
- **Размеры задач:** n = {100, 500, 1000, 5000}
- **Количество процессов:** {1, 2, 3, 4}

### 6.3 Тестовые данные

Тестовые данные генерируются программно:
- Матрица A — единичная матрица I (диагональные элементы = 1)
- Вектор b — все элементы равны 1
- Точное решение для данной системы: x = [1, 1, ..., 1]

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

Корректность реализации проверялась следующим образом:

1. **Сравнение с аналитическим решением:**
   - Для системы Ix = b, где b = [1, 1, ..., 1]^T, точное решение x = [1, 1, ..., 1]^T                    
   - Сумма компонент решения должна быть равна n
   - Проверка: `output ~= n` (с учетом округления и погрешности)

2. **Сравнение SEQ и MPI версий:**
   - Результаты последовательной и параллельной версий должны совпадать
   - Функциональные тесты проверяют эквивалентность результатов для различных n

3. **Тесты:**
   - Тесты для малых значений n 
   - Тесты для граничных случаев 
   - Проверка валидации входных данных 

4. **Проверка сходимости:**
   - Метод сходится за количество итераций < 1000
   - Норма изменения вектора на последней итерации < epsilon = 10^(-6)

**Результат:** Все тесты пройдены. Расхождение между SEQ и MPI версиями не превышает погрешности округления.

### 7.2 Производительность

#### 7.2.1 Измерения времени выполнения

Представлены результаты тестирования для размера задачи n = 2500.

**task_run**

| Режим       | Процессов | Время, сек | Ускорение | Эффективность |
|-------------|-----------|------------|-----------|---------------|
| SEQ         | 1         | 1.160      | 1.00      | N/A           |
| MPI         | 1         | 0.852      | 1.36      | 136%          |
| MPI         | 2         | 0.541      | 2.14      | 107%          |
| MPI         | 3         | 0.536      | 2.16      | 72%           |
| MPI         | 4         | 0.474      | 2.45      | 61%           |

Ускорение вычисляется как S = T_seq / T_parallel, 
Эффективность вычисляется как E = S / N_proc * 100%*

**task_pipeline**

| Режим       | Процессов | Время, сек | Ускорение | Эффективность |
|-------------|-----------|------------|-----------|---------------|
| SEQ         | 1         | 1.155      | 1.00      | N/A           |
| MPI         | 1         | 0.873      | 1.32      | 132%          |
| MPI         | 2         | 0.535      | 2.16      | 108%          |
| MPI         | 3         | 0.533      | 2.17      | 72%           |
| MPI         | 4         | 0.491      | 2.35      | 59%           |

#### 7.2.2 Анализ результатов

1. **Суперлинейное ускорение на 1-2 процессах:**
   - MPI версия на 1 процессе быстрее SEQ на ~36%
   - Причина: MPI версия использует плоский массив для матрицы, а SEQ версия — `vector<vector<double>>`

2. **Масштабируемость task_run:**
   - Ускорение: 1 процесс - 1.36x, 2 процесса - 2.14x, 3 процесса - 2.16x, 4 процесса - 2.45x
   - Насыщение на 2-3 процессах: прирост минимален
   - На 4 процессах заметно улучшение благодаря распределению вычислительной нагрузки

3. **Масштабируемость pipeline:**
   - Аналогичная ситуация: насыщение на 2-3 процессах
   - Незначительно медленнее task_run из-за накладных расходов конвейера

4. **Узкие места:**
   - Коммуникации `MPI_Send`/`MPI_Recv` и `MPI_Bcast` на каждой итерации
   - При увеличении числа процессов накладные расходы на коммуникации растут
   - Для n=2500 объем данных относительно невелик, коммуникации занимают значительную долю времени

**Структура коммуникаций MPI версии:**

- `MPI_Scatterv` — распределение строк матрицы и вектора b 
- `MPI_Send`/`MPI_Recv` — сбор локальных результатов на мастер-процессе 
- `MPI_Bcast` — рассылка обновленного вектора x 
- `MPI_Reduce` — суммирование локальных норм для проверки сходимости 

**Возможные улучшения:**

1. Увеличение размера задачи для лучшего соотношения вычислений к коммуникациям
2. Оптимизация SEQ версии 

## 8. Выводы

В рамках данной лабораторной работы была реализована параллельная версия метода простых итераций для решения СЛАУ с использованием технологии MPI.

**Ограничения:**

1. Репликация полной матрицы A на всех процессах ограничивает масштабируемость по памяти
2. Коллективная операция `MPI_Allgatherv` становится узким местом при большом числе процессов
3. Для малых задач (при n < 500) накладные расходы MPI сопоставимы с временем вычислений

## 9. Источники

1. Копченова Н.В., Марон И.А. *Вычислительная математика в примерах и задачах*. Лань, 2009.
2. MPI Forum. *MPI: A Message-Passing Interface Standard, Version 4.0*. URL: https://www.mpi-forum.org/
3. Бахвалов Н.С., Жидков Н.П., Кобельков Г.М. *Численные методы*. Лаборатория знаний, 2015.
4. Documentation: MS-MPI. URL: https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi

## Приложение

### Основной вычислительный цикл (MPI)

```cpp
for (int iteration = 0; iteration < max_iterations; iteration++) {
  for (int i = 0; i < local_rows; i++) {
    int global_i = start_row + i;
    double ax_i = 0.0;
    for (int j = 0; j < n; j++) {
      ax_i += a[global_i][j] * x[j];
    }
    local_ax[i] = ax_i;
  }

  std::vector<double> ax_global(n, 0.0);
  MPI_Allgatherv(local_ax.data(), local_rows, MPI_DOUBLE, 
                 ax_global.data(), recvcounts.data(), displs.data(),
                 MPI_DOUBLE, MPI_COMM_WORLD);

  for (int i = 0; i < n; i++) {
    x_new[i] = x[i] - (tau * (ax_global[i] - b[i]));
  }

  double diff = ComputeVectorNorm(x_new, x, n);
  x = x_new;

  if (diff < epsilon) {
    break;  
  }
}
```

### Схема распределения строк

```
Матрица A (n=10) при size=3:

Process 0:  ┌──────────┐
            │ Строки 0 │  local_rows = 4
            │ Строки 1 │
            │ Строки 2 │
            │ Строки 3 │
            ├──────────┤
Process 1:  │ Строки 4 │  local_rows = 4
            │ Строки 5 │
            │ Строки 6 │
            │ Строки 7 │
            ├──────────┤
Process 2:  │ Строки 8 │  local_rows = 2
            │ Строки 9 │
            └──────────┘
```

