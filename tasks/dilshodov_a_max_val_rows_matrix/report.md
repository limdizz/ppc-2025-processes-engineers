# Поиск максимальных значений в строках матрицы

- Студент: Дилшодов Адхам Умидович, группа 3823Б1ПР4
- Технология: SEQ + MPI
- Вариант: Поиск максимума в каждой строке матрицы

## 1. Введение

Задача поиска максимального элемента в каждой строке матрицы является типичной задачей обработки данных. Несмотря на простоту вычислений, она позволяет продемонстрировать основные принципы параллельного программирования с использованием MPI: распределение данных между процессами, локальные вычисления и сбор результатов.

Цель работы — реализовать последовательную и параллельную версии алгоритма, провести тестирование производительности и проанализировать эффективность распараллеливания.

## 2. Постановка задачи

**Входные данные:** Матрица целых чисел размером N × M, представленная как `std::vector<std::vector<int>>`.

**Выходные данные:** Вектор из N целых чисел, где i-й элемент — максимальное значение в i-й строке матрицы.

**Ограничения:**
- N, M > 0
- Все строки матрицы имеют одинаковую длину
- Элементы матрицы — целые числа типа `int`

## 3. Базовый алгоритм (последовательный)

Последовательный алгоритм выполняет линейный проход по каждой строке матрицы с использованием стандартной функции `std::max_element`:

```cpp
for (int i = 0; i < rows; ++i) {
  output[i] = *std::max_element(input[i].begin(), input[i].end());
}
```

**Сложность:** O(N × M) — каждый элемент матрицы просматривается ровно один раз.

## 4. Схема распараллеливания

Используется схема распределения по строкам:

1. Rank 0 рассылает размеры матрицы всем процессам
2. Rank 0 отправляет части матрицы другим процессам
3. Каждый процесс вычисляет максимумы своих строк
4. Результаты собираются на rank 0

### Балансировка нагрузки

При неравном делении строки распределяются по формуле:
```cpp
int base_rows = rows / size;
int extra_rows = rows % size;
int local_rows = base_rows + (rank < extra_rows ? 1 : 0);
```

## 5. Используемые функции MPI

- **MPI_Comm_rank** — получение номера текущего процесса
- **MPI_Comm_size** — получение общего количества процессов
- **MPI_Bcast** — широковещательная рассылка данных от одного процесса всем остальным
- **MPI_Isend** — неблокирующая отправка данных (возвращает управление сразу, не дожидаясь завершения)
- **MPI_Waitall** — ожидание завершения всех неблокирующих операций
- **MPI_Recv** — блокирующий приём данных
- **MPI_Send** — блокирующая отправка данных

## 6. Экспериментальная установка

| Параметр | Значение |
|----------|----------|
| CPU | AMD Ryzen 5 5600 (6 ядер / 12 потоков) @ 3.693 GHz |
| RAM | 16 ГБ |
| ОС | Windows 11 + WSL2 (Ubuntu 24.04.2 LTS) |
| Ядро | 6.6.87.2-microsoft-standard-WSL2 |
| Компилятор | GCC (g++) |
| MPI | Open MPI |
| Тип сборки | Release |

### Тестовые данные

- Размер матрицы: **5000 × 5000** (25 млн элементов, ~100 МБ)
- Генерация: случайные числа в диапазоне [-10000, 10000]
- Seed: 42

## 7. Результаты

### 7.1 Корректность

Все функциональные тесты проходят успешно на матрицах разных размеров (1x1, 3x3, 5x10, 10x5, 100x100, 50x200).

### 7.2 Производительность

| Режим | Процессы | Время, мс | Speedup (vs MPI-1) | Efficiency |
|-------|----------|-----------|---------------------|------------|
| SEQ | 1 | 6 | — | — |
| MPI | 1 | 82 | 1.00 | 100% |
| MPI | 2 | 70 | 1.17 | 58.5% |
| MPI | 4 | 57 | 1.44 | 36.0% |
| MPI | 6 | 65 | 1.26 | 21.0% |


### 7.3 Профилирование MPI (4 процесса)

| Операция | Время, мс | Доля |
|----------|-----------|------|
| MPI_Bcast (размеры) | 0.001 | 0% |
| Копирование своих данных | 2.4 | 4% |
| Копирование + MPI_Isend | 28.5 | 50% |
| Вычисление max | 14.0 | 25% |
| MPI_Waitall | 0.01 | 0% |
| MPI_Recv (сбор результатов) | 5.6 | 10% |
| **Итого** | **57** | 100% |

**Вывод из профилирования:** 60% времени занимает передача данных (копирование + отправка + приём), только 25% — полезные вычисления.

## 8. Выводы

В ходе работы реализованы последовательная и параллельная версии алгоритма поиска максимума в строках матрицы. Обе версии успешно проходят функциональные тесты.

По абсолютному времени последовательная версия оказывается быстрее параллельной (6 мс против 57 мс при 4 процессах). Причина в том, что операция поиска максимума слишком простая — один проход по массиву. Время на передачу данных между процессами (~35 мс) многократно превышает время самих вычислений (~14 мс).

Параллельная MPI версия демонстрирует ускорение относительно себя же с одним процессом: при 4 процессах достигается speedup 1.44x. Однако эффективность низкая (36%) из-за накладных расходов на коммуникацию.


## 9. Источники

1. Сысоев А. В. Лекции курса «Параллельное программирование для кластерных систем»
2. Документация лабораторных работ — https://learning-process.github.io/parallel_programming_course/ru/

## Приложение

### Последовательная версия

```cpp
bool MaxValRowsMatrixTaskSequential::RunImpl() {
  const auto &input = GetInput();
  auto &output = GetOutput();
  int rows = static_cast<int>(input.size());

  for (int i = 0; i < rows; ++i) {
    output[i] = *std::max_element(input[i].begin(), input[i].end());
  }
  return true;
}
```

### MPI версия (ключевой фрагмент)

```cpp
// Неблокирующая отправка данных другим процессам
for (int proc = 1; proc < size; ++proc) {
  MPI_Isend(send_buffers[proc - 1].data(), proc_rows * cols, 
            MPI_INT, proc, 0, MPI_COMM_WORLD, &send_requests[proc - 1]);
}

// Вычисление максимумов параллельно с отправкой
for (int i = 0; i < local_rows; ++i) {
  local_max[i] = *std::max_element(row_ptr, row_ptr + cols);
  row_ptr += cols;
}

// Ожидание завершения отправки
MPI_Waitall(size - 1, send_requests.data(), MPI_STATUSES_IGNORE);

// Сбор результатов от других процессов
for (int proc = 1; proc < size; ++proc) {
  MPI_Recv(GetOutput().data() + result_offset, proc_rows, MPI_INT, 
           proc, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
}
```
