# Отчёт по лабораторной работе  
## Подсчёт числа нарушений монотонности в векторе (SEQ | MPI)
## Вариант: 6

**Студент:** Ермаков Алексей Викторович, группа 3823Б1ПР3  
**Преподаватели:** Сысоев А. В., Оболенский А. А., Нестеров А.

---

## 1. Введение

Задача подсчёта числа пар соседних элементов в векторе, для которых выполняется условие  
**vec[i] > vec[i+1]**, является базовой операцией анализа последовательностей.

Для больших объёмов данных последовательная обработка может быть медленной.  
Использование **MPI** позволяет распределить вычисления между процессами и потенциально ускорить алгоритм.

В работе реализованы:

- **Последовательная версия (SEQ)**
- **Параллельная версия (MPI)**

**Цель:** корректно реализовать алгоритм, проверить работоспособность и оценить ускорение при разных количествах процессов.

---

## 2. Постановка задачи

Входные данные:  
- Вектор целых чисел `vec` длины `N`.

Выходные данные:  
- Число нарушений монотонности `viol`.

Учитываемые граничные случаи:

- `N = 0` → результат `0`
- `N = 1` → результат `0`
- `N < P` → часть процессов получает пустые блоки
- Межпроцессные границы должны учитываться

---

## 3. Базовый алгоритм (последовательный)

Последовательный метод:

1. `viol = 0`
2. Для `i = 0..N-2`  
   Если `vec[i] > vec[i+1]`, увеличить `viol`.
3. Вернуть `viol`.

### Оценка сложности

- **Временная сложность:** O(N)
- **Пространственная сложность:** O(N)

---

## 4. Схема параллелизации (MPI)

- Разделение данных

- Вектор делится на P частей, каждая часть передается своему ранку:

- base = N / P
- remainder = N % P
- cnt[i] = base + (i < remainder ? 1 : 0)
- disp[i] = смещение начала блока в векторе

### Локальные вычисления

- Каждый ранк подсчитывает количество нарушений в своём блоке.

### Обработка границ

- Для границы между соседними блоками:

- Если последний элемент левой части больше первого элемента правой части, добавляется 1 к счётчику нарушений.

- Агрегация результатов

- MPI_Reduce(..., MPI_SUM, root=0):

- Выполнение глобальной редукционной операции (например, сумма, максимум) по всем процессам.
- Вычисление общей суммы значений, распределенных по процессам.
- Может рассматриваться как операция, обратная широковещательной рассылке.

- MPI_Bcast - Отправка данных из одного процесса всем остальным процессам.

### Схема MPI:

- Процесс 0: `[0..chunk_0]` -> `local_viol_0`  ─┐
- Процесс 1: `[..chunk_1..]` -> `local_viol_1` ├─ MPI_Reduce(SUM) -> `total_viol`
- Процесс 2: `[...]` -> `local_viol_2`          ┘

---

## 5. Детали реализации

**Основные файлы:**

- `common.hpp` - типы данных
- `ops_seq.cpp` - реализация SEQ
- `ops_mpi.cpp` - реализация MPI
- Тесты: functional + performance (GoogleTest)

**Классы:**

- `ErmakovANumbViolElemVecSEQ`
- `ErmakovANumbViolElemVecMPI`

**Методы:**

- `ValidationImpl()`
- `PreProcessingImpl()`
- `RunImpl()`
- `PostProcessingImpl()`

**Граничные случаи:**

- `N = 0` - возвращает 0

- `N = 1` - возвращает 0

- `N < P` - некоторые ранки получают пустой блок

- `N % P != 0` - остаток распределяется между первыми ранками

---

## 6. Экспериментальная установка

**Оборудование:**

- CPU: Ryzen 5 1600 (3.8 GHz)  
- RAM: 8 GB  
- OS: Windows 11

**Программное обеспечение:**

- MS-MPI 10.0  
- CMake 4.2.0-rc1  
- Google Test  
- Режим сборки: Release

---

## 7. Результаты и обсуждение

### 7.1 Корректность

- Корректность реализации проверялась на следующих данных:

- Векторы маленького и среднего размера:

- {} — пустой вектор, результат = 0

- {42} — один элемент, результат = 0

- {2,1,3} — результат = 1

- {1,2,3,4,5} — упорядоченный, результат = 0

- {5,4,3,2,1} — полностью неупорядоченный, результат = 4

- {1,3,2,5,4} — смешанный, результат = 2

- {1,3,2,4,3,5,4} — смешанный, результат = 3

- {7,7,7,7} — одинаковые элементы, результат = 0

- {1,2,1,3,2,4,3,5,4,6,5,7,6,8,7,9,8} - первый больше, второй меньше и т.д., результат = 8

#### Методы проверки:

- Функциональные тесты: все тесты успешно пройдены

- Сравнение SEQ и MPI: результаты идентичны для всех векторов

- Граничные случаи: пустой вектор, вектор с одним элементом, векторы с одинаковыми элементами

### 7.2 Производительность

Тестирование проводилось для вектора размера **250 000 000** элементов.

| Режим | Процессы | Время (сек) | Speedup | Efficiency |
|-------|----------|--------------|----------|--------------|
| seq pipeline | 1 | 0.1812       | 1.00     | N/A          |
| seq task_run | 1 | 0.1908       | 1.00     | N/A          |
| mpi pipeline | 1 | 0.6825       | 0.27     | 26.6%        |
| mpi task_run | 1 | 0.5879       | 0.32     | 32.5%        |
| mpi pipeline | 2 | 0.4325       | 0.42     | 20.9%        |
| mpi task_run | 2 | 0.5329       | 0.36     | 17.9%        |
| mpi pipeline | 3 | 0.3602       | 0.50     | 16.8%        |
| mpi task_run | 3 | 0.5220       | 0.37     | 12.2%        |
| mpi pipeline | 4 | 0.8899       | 0.20     | 5.1%         |
| mpi task_run | 4 | 0.4042       | 0.47     | 11.8%        |

- Расчет показателей:

- Ускорение (Speedup) = T_seq / T_mpi

- Эффективность (Efficiency) = Speedup / P × 100%

- MPI эффективно масштабируется только при небольшом числе процессов.  
- Для больших P коммуникации становятся критическим узким местом.

---

## 8. Выводы

- Реализованы корректные SEQ и MPI версии алгоритма.
- Все функциональные тесты успешно пройдены.
- MPI даёт прирост производительности при малом P, но в целом на такой простой задаче SEQ быстрее.
- Граничные случаи и пустые блоки корректно обработаны.

---

## 9. Список источников

1. MPI Forum. MPI: A Message-Passing Interface Standard, Version 4.0, 2021.  
2. Microsoft MPI Documentation.  
3. Документация преподавателей: https://learning-process.github.io/parallel_programming_slides/  
4. Лекции Сысоев А. В., Оболенский А. А., Нестеров А., ННГУ, 2025.

---

# Приложение — фрагменты кода

## SEQ версия

```cpp
int viol = 0;
for (int i = 0; i + 1 < vec.size(); ++i) {
    if (vec[i] > vec[i + 1]) ++viol;
}
GetOutput() = viol;
```

## MPI версия
```cpp
// Разделение данных на блоки
int base = N / P;
int rem = N % P;
std::vector<int> counts(P), displs(P);

for (int i = 0, shift = 0; i < P; ++i) {
    counts[i] = base + (i < rem ? 1 : 0);
    displs[i] = shift;
    shift += counts[i];
}

// Рассылка данных
std::vector<int> local(counts[rank]);
if (rank == 0) {
    std::copy(vec.begin(), vec.begin() + counts[0], local.begin());
    for (int dest = 1; dest < P; ++dest) {
        if (counts[dest] > 0) {
            MPI_Send(vec.data() + displs[dest], counts[dest], MPI_INT, dest, 0, MPI_COMM_WORLD);
        }
    }
} else {
    if (!local.empty()) {
        MPI_Recv(local.data(), counts[rank], MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    }
}

// Локальные нарушения
int local_viol = 0;
for (size_t i = 0; i + 1 < local.size(); ++i) {
    if (local[i] > local[i + 1]) ++local_viol;
}

// Граница блоков
int border_viol = 0;
if (rank > 0) {
    int left_last;
    MPI_Recv(&left_last, 1, MPI_INT, rank - 1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    if (left_last > local.front()) border_viol = 1;
}
if (rank < P - 1) {
    MPI_Send(&local.back(), 1, MPI_INT, rank + 1, 1, MPI_COMM_WORLD);
}

// Суммирование
int local_sum = local_viol + border_viol;
int global_sum = 0;

MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
MPI_Bcast(&global_sum, 1, MPI_INT, 0, MPI_COMM_WORLD);

GetOutput() = global_sum;
```
