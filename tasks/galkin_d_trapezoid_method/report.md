# Численное интегрирование методом трапеций

**Студент:** Галкин Данила Алексеевич, группа 3823Б1ПР1  
**Технология:** SEQ + MPI  
**Вариант:** 20  

## 1. Введение

Численное интегрирование используется тогда, когда аналитически вычислить интеграл затруднительно или невозможно.  
Метод трапеций — один из самых простых способов приблизить значение интеграла. Он хорошо ложится на параллельную обработку: каждый участок разбиения независим от остальных.

Цель работы — реализовать метод трапеций в последовательном варианте (SEQ) и в параллельном (MPI), проверить корректность и сравнить их производительность.

---

## 2. Постановка задачи

Требуется численно вычислить интеграл:

```
I = ∫[a..b] f(x) dx
```

при числе разбиений `n > 0`.

Шаг сетки:

```
h = (b - a) / n
```

Метод трапеций вычисляет интеграл по формуле:

```
I ≈ h * ( (f(a) + f(b)) / 2 + Σ f(a + k*h) )
```

В MPI-версии сумма разбивается между процессами:

```
I ≈ Σ ((f(x_i) + f(x_{i+1})) / 2) * h
```

### Формат входных данных

```cpp
struct InputParams {
  double a;
  double b;
  int n;
  int func_id;
};
```

Поддерживаемые функции:

* `f(x) = x`
* `f(x) = x²`
* `f(x) = sin(x)`

### Результат

Одно число типа `double`.

---

## 3. Последовательная реализация (SEQ)

Алгоритм работы:

1. Посчитать шаг `h`.
2. Начать сумму со значения `(f(a) + f(b)) / 2`.
3. Просуммировать `f(a + k*h)` для всех `k = 1..n-1`.
4. Умножить итог на `h`.

Сложность: **O(n)** по времени и **O(1)** по памяти.  

---

## 4. Параллельная реализация (MPI)

### Распределение нагрузки

Интервалы делятся равномерно между процессами:

```cpp
int base = n / size;
int rem  = n % size;

int local_n = base + (rank < rem ? 1 : 0);

int start_i = rank * base + std::min(rank, rem);
int end_i   = start_i + local_n;
```

### Локальные вычисления

Каждый процесс считает сумму площадей тех трапеций, которые ему достались.

### Сбор результата

* частичные суммы процессов объединяются через `MPI_Reduce` на процессе 0
* итоговое значение распространяется всем процессам через `MPI_Bcast`
* после рассылки каждый процесс записывает полученный результат в свой `GetOutput()`

То есть у всех процессов после выполнения задачи результат **одинаковый** и **валидный**.

---

## 5. Описание MPI-версии (программная реализация)

### 5.1. Архитектура решения

MPI-версия реализована в виде класса `GalkinDTrapezoidMethodMPI`, который наследуется от базового `ppc::task::Task`.  
Архитектура следует общей структуре фреймворка PPC и состоит из четырёх последовательных этапов:

1. **Validation** — проверка корректности входных данных  
2. **PreProcessing** — подготовка состояния перед вычислением  
3. **Run** — выполнение основных вычислений (MPI-схема)  
4. **PostProcessing** — финальная проверка результата

Такая архитектура позволяет логично разделить этапы обработки данных, упростить тестирование и сохранить единый стиль для SEQ и MPI-реализаций.

---

### 5.2. Структура классов

```cpp
namespace galkin_d_trapezoid_method {

using InType = InputParams;
using OutType = double;
using BaseTask = ppc::task::Task<InType, OutType>;

class GalkinDTrapezoidMethodMPI : public BaseTask {
 public:
  explicit GalkinDTrapezoidMethodMPI(const InType& in);

 private:
  bool ValidationImpl() override;
  bool PreProcessingImpl() override;
  bool RunImpl() override;
  bool PostProcessingImpl() override;
};

class GalkinDTrapezoidMethodSEQ : public BaseTask {
 public:
  explicit GalkinDTrapezoidMethodSEQ(const InType& in);

 private:
  bool ValidationImpl() override;
  bool PreProcessingImpl() override;
  bool RunImpl() override;
  bool PostProcessingImpl() override;
};

}  // namespace galkin_d_trapezoid_method
```

В проекте реализованы обе версии (SEQ и MPI). Они имеют одинаковый интерфейс и отличаются только реализацией `RunImpl()`.

---

### 5.3. Реализация методов

#### 5.3.1. Конструктор

```cpp
GalkinDTrapezoidMethodMPI::GalkinDTrapezoidMethodMPI(const InType &in) {
  SetTypeOfTask(GetStaticTypeOfTask());
  GetInput() = in;
  GetOutput() = 0.0;
}
```

Конструктор устанавливает тип задачи, записывает входные параметры и инициализирует выход нулём.

---

### 5.3.2. Валидация

```cpp
bool GalkinDTrapezoidMethodMPI::ValidationImpl() {
  const auto& in = GetInput();
  return (in.n > 0) && (in.b > in.a);
}
```

Валидация проверяет базовые условия:

* число разбиений `n > 0`;
* границы интегрирования корректны: `a < b`.

---

### 5.3.3. Предварительная обработка

```cpp
bool GalkinDTrapezoidMethodMPI::PreProcessingImpl() {
  GetOutput() = 0.0;
  return true;
}
```

На этом этапе сбрасывается выходной буфер.

---

### 5.3.4. Основные вычисления (MPI-версия)

```cpp
bool GalkinDTrapezoidMethodMPI::RunImpl() {
  const auto& in = GetInput();
  double a = in.a;
  double b = in.b;
  int n = in.n;

  int rank, size;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int base = n / size;
  int rem  = n % size;

  int local_n = base + (rank < rem ? 1 : 0);

  int start_i = rank * base + std::min(rank, rem);
  int end_i   = start_i + local_n;  
  double h = (b - a) / static_cast<double>(n);

  double local_sum = 0.0;
  for (int i = start_i; i < end_i; ++i) {
    double x_left  = a + i * h;
    double x_right = a + (i + 1) * h;

    double f_left  = Function(x_left, in.func_id);
    double f_right = Function(x_right, in.func_id);

    local_sum += (f_left + f_right) * 0.5 * h;
  }

  double global_sum = 0.0;
  MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
  MPI_Bcast(&global_sum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);

  GetOutput() = global_sum;
  return true;
}
```

Ключевые моменты MPI-реализации:

* интервалы распределяются равномерно между процессами с учётом хвоста (`rem`);
* каждый процесс независимо вычисляет сумму своих трапеций;
* результат собирается в процессе 0 через `MPI_Reduce`;
* итоговая сумма рассылается всем процессам командой `MPI_Bcast`;
* обмен данными минимален — передаётся только одно число (`double`).

Таким образом, параллельная схема является простой и масштабируемой.

---

### 5.3.5. Постобработка

```cpp
bool GalkinDTrapezoidMethodMPI::PostProcessingImpl() {
  return true;
}
```

Используется для единообразия и соответствует структуре PPC.

---

### 5.4. Преимущества MPI-реализации

* **Равномерное распределение интервалов.** Каждый процесс получает примерно одинаковое количество трапеций.  
* **Масштабируемость.** При увеличении числа процессов время вычислений уменьшается.  
* **Минимальные коммуникации.** Используются только две коллективные операции (`Reduce` + `Bcast`).  
* **Независимые вычисления.** Каждый процесс интегрирует свой участок без гонок данных.  
* **Оптимальная память O(1).** Как SEQ, так и MPI-версия используют лишь несколько локальных переменных.  
* **Чистая схема "Map-Reduce".**  
  - Map: каждый процесс считает частичную сумму;  
  - Reduce: объединение в один результат.  

Эта структура делает метод трапеций хорошо подходящим для MPI-параллелизации.


---

## 6. Экспериментальная часть

### Аппаратная конфигурация

Запуски выполнялись в DevContainer (Docker).  
Хост-машина:

* MacBook Pro
* Apple M4 Pro (12 ядер)
* 24 GB RAM

### ПО внутри контейнера

* Ubuntu Linux
* GCC
* OpenMPI
* Сборка в режиме Release

### Команды запуска

```
mpirun -n 2 ./ppc_perf_tests
mpirun -n 4 ./ppc_perf_tests
mpirun -n 8 ./ppc_perf_tests
```

### Параметры эксперимента

* `a = 0`
* `b = π`
* `n = 2 000 000`
* `f(x) = sin(x)`

---

## 7. Результаты

### 7.1 Проверка корректности

Корректность подтверждена:

* функциональными тестами SEQ и MPI,
* тестами на некорректный ввод,
* сравнением с аналитическим значением (`GetExactIntegral`),
* сравнением SEQ vs MPI. 

Во всех случаях выполнялось:

```
|численный результат – точное значение| < 1e-4
```

---

### 7.2 Производительность

| Процессов | Время (с) | Ускорение | Эффективность |
| --------: | --------: | --------: | ------------: |
|   1 (SEQ) |   0.00592 |     1.00  |       —       |
|   2 (MPI) |   0.00629 |     0.94  |      47%      |
|   4 (MPI) |   0.00346 |     1.74  |      43%      |
|   8 (MPI) |   0.00190 |     3.31  |      41%      |

**Комментарий:**

* На 2 процессах ускорения нет — накладные расходы MPI перевешивают выгоду.  
* На 4 и 8 процессах производительность растёт.  
* Эффективность ~40–45% типична для задач с небольшим объёмом вычислений.  
* При увеличении `n` масштабируемость улучшится.

---

## 8. Заключение

В работе реализованы последовательная и параллельная версии метода трапеций.  
Полученные результаты показывают:

* метод трапеций хорошо параллелится;
* точность обеих реализаций — лучше `1e-4`;
* ускорение на 8 процессах достигает ×3.31;
* MPI-накладные расходы становятся заметны при небольших размерах задачи.

Направления развития:

* добавить больше функций для интегрирования;
* реализовать адаптивное разбиение;
* сравнить метод трапеций с методом Симпсона.

---

## 9. Литература

1. Gropp, Lusk, Skjellum — *Using MPI*  
2. MPI Forum — стандарт MPI  
3. Документация GoogleTest  
4. Сысоев А. В. Лекции по параллельному программированию.
5. Учебники по численным методам
