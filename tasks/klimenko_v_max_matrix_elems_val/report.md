# Максимальное значение элементов матрицы
### Студент: Клименко Владислав Сергеевич, группа 3823Б1ПР2

### Вариант: №13

## Введение
Поиск максимального значения элементов матрицы - одна из базовых задач как в математике, так и в прграммировании. В данной работе был реализован алгоритм вычисления максимального значения элементов квадратной матрицы. Реализация происходила при помощи двух технологий: SEQ (последовательная) и MPI (параллельная). Цель - ускорение вычислений за счёт распределения строк матрицы между процессами и параллельного вычисления локальных максимумов.

## Постановка задачи
### Определение: 
Дана квадратная матрица размера n * n, элементами которой являются целые числа. Требуется найти наибольший элемент среди всех элементов матрицы.

Формальное определение: пусть задана матрица A = {a_ij}, i,j = 0...n-1. Необходимо вычислить z = max(a_ij).
Пример
Для матрицы (11 22 34
             34 15 62
             72 81 94) 
наибольший элемент 94.

Входные данные: std::vector<std::vector<int>> - двумерная матрица целых чисел
Выходные данные: int - максимальный элемент матрицы.

### Ограничения:
Матрица может быть пустой.

Матрица может иметь строки разной длины (реализация предполагает прямоугольную матрицу).

Значения могут быть любыми целыми числами

## Описание последовательной версии
В последовательной реализации алгоритм последовательно проходит по всем элементам матрицы и запоминает наибольшее значение.

    int max_element = matrix[0][0];
    for (const auto &row : matrix) {
        for (int element : row) {
            max_element = std::max(element, max_element);
        }
    }

Этот вариант реализован в классе KlimenkoVMaxMatrixElemsValSEQ.

## Описание параллельной версии
Строки матрицы распределяются между процессами MPI с использованием блочного распределения. Если размер матрицы не делится на количество процессов, оставшиеся строки распределяются на процессы с меньшими рангами.

    const auto &matrix = GetInput();
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int n = matrix.size();
    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);
    if (n == 0) {
        if (rank == 0) {
        GetOutput() = 0;
        }
        return true;
    }
    int local_size = n / size;
    int remainder = n % size;
    int start_idx, end_idx;
    if (rank < remainder) {
        start_idx = rank * (local_size + 1);
        end_idx = start_idx + local_size + 1;
    } else {
        start_idx = remainder * (local_size + 1) + (rank - remainder) * local_size;
        end_idx = start_idx + local_size;
    }
    int local_max = INT_MIN;
    for (int i = start_idx; i < end_idx && i < n; i++) {
        for (size_t j = 0; j < matrix[i].size(); j++) {
        if (matrix[i][j] > local_max) {
            local_max = matrix[i][j];
        }
        }
    }
    if (local_size == 0 && rank >= n) {
        local_max = INT_MIN;
    }
    int global_max;
    MPI_Allreduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);
    GetOutput() = global_max;
    return true;

### Схема коммуникации

Широковещательная рассылка: корневой процесс рассылает размер матрицы всем процессам.

Распределение: неявное распределение через вычисление индексов.

Редукция: все процессы участвуют в MPI_Allreduce с операцией MPI_MAX.

### Роли процессов

Ранг 0: Выступает как корневой процесс, хранит исходную матрицу.

Все ранги: Вычисляют локальный максимум и участвуют в глобальной редукции.

Этот вариант реализован в классе KlimenkoVMaxMatrixElemsValMPI.

## Тестирование

Тестирования разделены на модули
functional/main.cpp — функциональные тесты
performance/main.cpp — производительные тесты

### Функциональное тестирование
В тестах проверяется корректность работы обеих реализаций на матрицах небольшого размера (3×3, 5×5, 7×7). Значения элементов генерируются последовательно, и это позволяет точно предсказать ожидаемый результат. Тест автоматически запускает как последовательную, так и MPI-реализацию и сверяет результаты с ожидаемым значением.
Дополнительно реализованы тесты для маленьких (50x50), средних (500x500) и больших (5000x5000) размеров матрицы.

### Тестирование производительности
Для каждой реализации (как MPI, так и последовательной) предусмотрено по два теста:

test_pipeline_run — тест, подразумевающий ручной запуск ValidationImpl, PreProcessingImpl, RunImpl, PostProcessingImpl.

test_task_run — тест с замером времени выполнения полной задачи.

Для оценки производительности задаём размер матрицы 10000x10000.

### Аппаратное обеспечение/ОС:
Процессор: Intel(R) Core(TM) i7-12650H

ОЗУ: 16 Гб

ОС: Windows 10


### Инструментарий:
Язык программирования: C++

Библиотека для параллельного программирования: MPI

Тип сборки: Release 

Фреймворк тестирования: Google Test

## Результаты тестирования
Все тесты успешно пройдены.

### Наблюдения
- Ускорение при использовании MPI-параллелизации есть, и оно с увеличением размеров матрицы возрастает.
- Паралеллизация при поиске максимального значения элементов матрицы на небольших размерах матрицы, не даёт значительного повышения производительности.

## Выводы
И последовательная, и MPI-реализации были успешно реализованы, они корректно решают задачу поиска максимального значения элементов матрицы. Для значительного повышения производительности при паралелизации необходимы очень большие размеры матрицы. 

## Источники
Лекции Сысоева А. В.
