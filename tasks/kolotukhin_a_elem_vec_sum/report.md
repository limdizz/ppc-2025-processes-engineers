# Сумма элементов вектора

- Студент: Колотухин Александр Дмитриевич, 3823Б1ПР2
- Технология: SEQ, MPI 
- Вариант: 1

## 1. Введение
Рассматривается задача суммирования элементов вектора. Работа предполагает разработку последовательной и MPI-параллельной версий алгоритма, сравнение их производительности на разных размерах входных данных и разном числе процессов.

## 2. Постановка задачи
На вход программе подается целое число — размер вектора N. Требуется найти сумму всех его элементов.

**Ограничения:**
- `N` является беззнаковым целым 64-биным числом
- Элементы вектора являются целыми 32-битными числами

## 3. Описание базового алгоритма (последовательная версия)
Последовательный алгоритм — последовательное вычисление элементов и накопление их суммы в переменную. Время выполнения `O(N)`.
```cpp
for (std::uint64_t i = 0; i < vec_size; i++) {
  total_sum += input_vec[i];
}
```

## 4. Схема распараллеливания
**Описание действий роцессов `0-N`:**
1. Каждый процесс получает свои индекс и количество процессов.
2. Нулевой процесс получает исходный вектор и рассылает его размер.
3. каждый процесс считает количество элемнетов суммирования и выделяет под них память. Нулевой процесс выделяет дополнительную память под остаток, если общее число элементов не делится на число процессов без остатка. Его передача происходит отдельно.
4. Нулевой процесс рассылает элементы векора равными частями.
5. Каждый процесс суммирует элементы локального вектора.
6. Все локальные суммы суммируются в одну глобальную через MPI_Reduce, результат доступен у корневого процесса.

## 5. Детали реализации
**Структура кода**
```text
kolotukhin_a_elem_vec_sum   
    │
    ├───common
    │   └───include
    │       └───common.hpp — Файл озадает типы используемых данных: воходные, выходные, тестовые
    │
    ├───mpi
    │   ├───include
    │   │   └───ops_mpi.hpp — Заголовочный файл параллелльной версии программы
    │   │
    │   └───src
    │       └───ops_mpi.cpp — Реализация параллельной версии
    │
    ├───seq
    │   ├───include
    │   │   └───ops_seq.hpp — Заголовочный файл последовательной версии программы
    │   │
    │   └───src
    │       └───ops_seq.cpp — реализация последовательной версии
    │
    ├───tests
    │   ├───functional
    │   │   └───main.cpp — Тесты оценки функционирования
    │   │
    │   └───performance
    │       └───main.cpp — Тесты оценки производительности
    │
    ├───info.json — Информация о выполнившем работу
    │
    ├───report.md — Отчет по работе (данный файл)
    │
    └───settings.json — Включение/выключение задачи
```

**Ключевые классы**
- `KolotukhinAElemVecSumMPI` — описание параллельного алгоритма
- `KolotukhinAElemVecSumSEQ` — описание последовательного алгоритма

**Важные предположения**
- Тип и размеры входных данных подходят для обработки.
- Программа должна корректно работать на одном и нескольких процессах

**Использование памяти**
- В корневом процессе хранятся все данные — использование памяти `O(N)`.
- Все процессы содержат часть исходных данных `O(N / proc_count)`. Итого на все процессы дополнительная память `O(N)`

- Суммарная память `O(N)`.

## 6. Экспериментальная установка
- Аппаратное обеспечение / ОС 
    - Модель процессора: Intel Core 7 240H (10 ядер, 16 потоков);
    - Оперативная память: 16 ГБ;
    - Версия ОС: Windows 11 Home 25H2 (26200.6899).
- Инструменты 
    - Компилятор: MSVC версии 19.44.35217.0 (Visual Studio 2022);
    - Тип сборки: Release;
    - MPI: Microsoft MPI 10.1.12498.52.
- Окружение
    - Количество процессов: 1, 2, 4, 6, 8.
- Данные
    - формирование данных происходит с помощью генератора — все генерации одинаковые при одной конфигурации генератора. 

## 7. Результаты

### 7.1 Корректность
Проверка осуществляется с помощью Google Test:

5 функциональных тестов покрывают:
- обработку пустого вектора
- единичный вектор
- обработка при неравномерном распределении элементов
- оработка маленьких векторов
- оработка больших векторов

Текущая реализация проходит функциональное тестирование

### 7.2 Эксплуатация
Результаты performance тестов при входной длине вектора `100000000` (100 млн.)

Описание результата:
- Выполняющая версия алгоритма
- Число участвующих процессов
- Время работы вычислительной части в секундах
- Ускорение = время seq версии / время mpi версии
- Эффективность = (Ускорение / Процессы) * 100 %

Итоги выполнения вычислительной части программы: ```task_run```
| Версия      | Процессы | Время, с | Ускорение | Эффективность |
|-------------|----------|----------|-----------|---------------|
|  seq        |    1     | 0.032    |   1.000   |               |
|  mpi        |    2     | 0.161    |   0.198   |     9.00%     |
|  mpi        |    4     | 0.156    |   0.205   |     5.13%     |
|  mpi        |    6     | 0.140    |   0.228   |     3.80%     |
|  mpi        |    8     | 0.162    |   0.197   |     2.46%     |

## 8. Заключение
- **Корректность:** алгоритм успешно обрабатывает векторы различных размеров — от минимальных до максимальных.
- **Производительность:** последовательная версия демонстрирует высокую эффективность, выполняясь за ~0.032 секунды на одном процессе.
- **Масштабируемость:** при увеличении числа процессов (2 - 8) наблюдается снижение скорости работы и эффективности, из-за низкой вычислительной сложности задачи по сравнению с накладными расходами MPI.

## 9. Источники
1. Документация по курсу «Параллельное программирование» / Parallel Programming Course [Электронный ресурс]. — Режим доступа: https://learning-process.github.io/parallel_programming_course/ru/index.html. — Дата обращения: 13.11.2025.
2. Kendall W. MPI Tutorial [Электронный ресурс]. — URL: https://mpitutorial.com. — Дата обращения: 15.11.2025.
3. Сысоев А. В. Курс лекций по параллельному программированию [Электронный ресурс]. — URL: https://source.unn.ru. — Требуется авторизация. — Дата обращения: 15.11.2025.

## Приложение
```cpp
bool KolotukhinAElemVecSumMPI::RunImpl() {
  int p_id = -1;
  int p_count = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &p_id);
  MPI_Comm_size(MPI_COMM_WORLD, &p_count);

  std::vector<int> input_data{};
  std::size_t input_size = 0;

  if (p_id == 0) {
    input_data = GetInput();
    input_size = input_data.size();
  }

  MPI_Bcast(&input_size, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);

  int base_size = input_size / p_count;
  int my_size = base_size;
  if (p_id == 0) {
    my_size += input_size % p_count;
  }

  std::vector<int> local_data(my_size);

  MPI_Scatterv(input_data.data(), base_size, MPI_INT, local_data.data(), base_size, MPI_INT, 0, MPI_COMM_WORLD);

  if (p_id == 0 && input_size % p_count != 0) {
    int remainder_start = base_size * p_count;
    int remainder_size = input_size % p_count;
    for (int i = 0; i < remainder_size; i++) {
      local_data[base_size + i] = input_data[remainder_start + i];
    }
  }

  std::int64_t local_sum = std::accumulate(local_data.begin(), local_data.end(), 0LL);
  std::int64_t global_sum = 0;
  MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT64_T, MPI_SUM, 0, MPI_COMM_WORLD);

  GetOutput() = global_sum;
  return true;
}
```