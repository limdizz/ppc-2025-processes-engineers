# Сумма значений по столбцам матрицы
  - Студент: Кондрашова Виктория Андреевна, group 3823Б1ПР1
  - Технологии: SEQ | MPI
  - Вариант: 12

## 1. Введение
Вычисление сумм элементов по столбцам матрицы является одной из базовых операций линейной алгебры и важным компонентом во множестве алгоритмов обработки данных, статистического анализа, машинного обучения и компьютерной графики. В рамках этого исследования проводится сравнительный анализ производительности последовательной и параллельной MPI-реализаций данного алгоритма.

## 2. Постановка задачи
**Цель работы:** Реализовать последовательную и MPI-параллельную версии алгоритма вычисления сумм элементов по столбцам матрицы, провести их сравнение и анализ эффективности.

**Определение задачи:** Для матрицы A размера m × n (где m — количество строк, n — количество столбцов) необходимо вычислить вектор сумм S размера n

**Ограничения:**
 - Корректность вычислений должна сохраняться при любых значениях элементов.
 - Результаты SEQ и MPI версий должны полностью совпадать.
 - Входные данные: вектор из (2 + m × n) элементов, где первые два элемента — размеры m и n, остальные — элементы матрицы.
 - Выходные данные: вектор из n элементов — суммы элементов каждого столбца.

## 3. Алгоритм(Последовательная версия)
**Шаги алгоритма:**
 - Разбор входных данных: Чтение размеров матрицы m (количество строк) и n (количество столбцов) из входного вектора

 - Валидация: Проверка, что размер входных данных равен 2 + m × n

 - Инициализация: Резервирование и обнуление вектора для n выходных значений (сумм по столбцам)

 - Обработка элементов матрицы:

  -- Для каждой строки i от 0 до m-1:
   -- Для каждого столбца j от 0 до n-1:
    -- Вычисление индекса элемента: index = 2 + i * n + j
    -- Накопление суммы: output_data[j] += input_data[index]
 - Возврат: Выходной вектор, содержащий n значений — суммы элементов каждого столбца

## 4. Схема распараллеливания
Этапы выполнения
### 1. Валидация (ValidationImpl)
Процесс 0:**

Проверяет корректность входных данных

**Все процессы:**

Получают результат валидации через MPI_Bcast
Возвращают единый результат

### 2. Предобработка (PreProcessingImpl)
**Процесс 0:**

Извлекает размеры матрицы из входных данных
Инициализирует выходной вектор размером cols_

**Все процессы:**

Получают rows_ и cols_ через MPI_Bcast
Теперь все знают размеры задачи

### 3. Основное вычисление (RunImpl)
### 3.1. Распределение столбцов между процессами
Используется балансировка с учётом остатка

**Правило распределения:**

Первые ost процессов: по (cols_on_proc + 1) столбцу
Остальные процессы: по cols_on_proc столбцов
Вычисление диапазона для процесса rank

### 3.2. Рассылка матрицы всем процессам
**Процесс 0:**

Копирует матрицу из входных данных (пропуская первые 2 элемента — размеры)

**Остальные процессы:**

Выделяют память под всю матрицу

**Все процессы:**

Получают полную копию матрицы

### 3.3. Локальное вычисление сумм
Каждый процесс независимо вычисляет суммы для своих столбцов

### 3.4. Сбор результатов на процессе 0
**Процесс 0:**

Копирует свои локальные результаты в выходной вектор

Принимает результаты от каждого процесса src = 1..size-1

**Остальные процессы:**

Отправляют свои результаты процессу 0

После этого только процесс 0 имеет полный вектор сумм по всем столбцам.

### 4. Постобработка (PostProcessingImpl)
**Процесс 0:**

Проверяет, что выходной вектор не пуст
Возвращает !GetOutput().empty()
**Остальные процессы:**

Сразу возвращают true

## 5.Детали реализации
### Структура кода

**Файлы:**
- `common/include/common.hpp` - определение типов данных
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` - последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` - параллельная реализация
- `tests/functional/main.cpp` - функциональные тесты
- `tests/performance/main.cpp` - тесты производительности

**Классы:**
- `KondrashovaVSumColMatSEQ` - последовательная версия (SEQ)
- `KondrashovaVSumColMatMPI` - параллельная версия (MPI)

**Методы (одинаковы для обеих реализаций):**
- `ValidationImpl()` - проверка корректности входа
- `PreProcessingImpl()` - подготовка данных
- `RunImpl()` - основная логика (последовательная или MPI).
- `PostProcessingImpl()` - проверка корректности выхода.


## 6. Экспериментальная установка
- Hardware/OS: CPU - AMD Ryzen 5 5600H, 6 ядер/12 потоков; RAM - 16 Gb; ОС - Windows 10 
- Toolchain: g++ 11.4.0 , build type: Release  
- Environment: PPC_NUM_PROC
- Data: тестовые данные задаются вручную.

## 7. Результаты и обсуждения

### 7.1 Корректность

**Корректность проверена через:**

* Модульные тесты для различных размеров и типов матриц

* Сравнение между последовательными и MPI результатами

* Проверка размера выходного вектора и положительных значений

* Все 10 функциональных тестов успешно пройдены

### 7.2 Производительность
**Характеристики задачи:**
- Размер: 20000 строк × 20000 столбцов = 400,000,000 элементов
- Выходной вектор: 20000 элементов

| Mode        | Count | Time, s  | Speedup | Efficiency |
|-------------|-------|----------|---------|------------|
| seq         | 1     | 57.621   | 1.00    | N/A        |
| omp         | 2     | 90.511   | 0.64    | 32.0%      |
| omp         | 8     | 60.508   | 0.95    | 11.9%      |

## 8. Вывод
В ходе выполнения работы была создана и протестирована программная реализация задачи нахождения сумм столбцов матрицы. Разработка включала два подхода: классический последовательный алгоритм и параллельную версию с применением технологии MPI. Разработанные алгоритмы корректно решают поставленную задачу, что подтверждается успешным прохождением всех функциональных тестов. MPI вариант решения строится на принципе разделения столбцов между вычислительными узлами. Каждый участвующий процесс обрабатывает закреплённую за ним группу столбцов, затем главный узел агрегирует частичные результаты в финальный вектор сумм. Из полученных результатов видно, что MPI-версия работает медленнее Seq-версии, это связано с высокими накладными расходами на межпроцессное взаимодействие.

## 9. Источники
1. Список лекций по курсу "Параллельное программирование". (Сысоев А.В. ННГУ 2025 г.)
2. Список практических занятий по курсу "Пареллельное программирование". (Оболенский А.А, ННГУ 2025 г.)
3. Документация по курсу: "Параллельное программирование": <https://learning-process.github.io/parallel_programming_course/ru/index.html> (Оболенский А.А, Нестеров А.Ю)

## 10. Приложение

### MPI-реализация(ключевой алгоритм)
```cpp
bool KondrashovaVSumColMatMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  const int cols_on_proc = cols_ / size;
  const int ost = cols_ % size;

  int first_col = 0;
  int end_col = 0;
  ComputeLocalCols(rank, ost, cols_on_proc, first_col, end_col);

  const int local_cols = end_col - first_col;

  std::vector<int> matrix;
  if (rank == 0) {
    matrix.assign(GetInput().begin() + 2, GetInput().end());
  } else {
    matrix.resize(static_cast<size_t>(rows_) * static_cast<size_t>(cols_));
  }

  MPI_Bcast(matrix.data(), rows_ * cols_, MPI_INT, 0, MPI_COMM_WORLD);

  std::vector<int> local_sums(static_cast<size_t>(local_cols), 0);
  ComputeLocalSums(matrix, local_sums, rows_, cols_, first_col);
  GatherSums(local_sums, first_col, local_cols, rank, size, GetOutput());

  return true;
}
```
