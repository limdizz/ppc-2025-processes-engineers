# Сумма значений по столбцов матрицы

- Студент: <Копылов Данила Алексеевич>, группа <3823Б1ПР5>
- Технология: SEQ | MPI
- Вариант: <12>

## 1. Вступление
Целью данной работы является разработка параллельного алгоритма для вычисления суммы значений для каждого столбца в заданной матрице. В параллельной реализации будет использоваться технология интерфейса передачи сообщений (MPI). Цель состоит в том, чтобы сравнить производительность параллельного алгоритма с его последовательным аналогом и оценить результирующее ускорение и эффективность.

## 3. Базовый алгоритм (последовательный)
Последовательный алгоритм использует матрицу, представленную в виде одномерного массива, вместе с ее измерениями (строками и столбцами). Он инициализирует результирующий вектор для суммирования столбцов с нулями.

Затем алгоритм выполняет итерацию по каждому элементу матрицы, используя вложенный цикл. Внешний цикл обходит строки, а внутренний цикл обходит столбцы. Для каждого элемента "matrix[row][col]" его значение добавляется к соответствующему `result[col]`.

Этот процесс реализован в `KopilovDSumValColMatSEQ::RunImpl` следующим образом:
``cpp
const auto &in = GetInput();
auto &out = получение вывода().col_sum;

const int rows = в файле.rows;
const int cols = в файле.cols;

const auto &mat = входные данные;

для (int r = 0; r < строки; r++) {
  для (int c = 0; c < cols; c++) {
    out[c] += mat[r * cols + c];
  }
}
```

## 4. Схема распараллеливания
Распараллеливание основано на разложении матрицы по строкам. Общая схема выглядит следующим образом:

1. **Распределение данных**: Корневой процесс (ранг 0) транслирует измерения матрицы (строки и столбцы) всем остальным процессам. Затем он распределяет строки матрицы между всеми доступными процессами. `MPI_Scatterv` используется для отправки каждому процессу назначенного ему набора строк. Эта функция выбрана из-за ее способности обрабатывать случаи, когда количество строк неравномерно делится на количество процессов.
2. ** Локальное вычисление **: Каждый процесс получает свою подматрицу (набор строк) и вычисляет суммы по ее столбцам локально. В результате получается вектор частичной суммы для каждого процесса.
3. ** Глобальное сокращение **: Векторы частичной суммы от всех процессов объединяются в вектор конечного результата для корневого процесса. Это достигается с помощью коллективной операции `MPI_Reduce` с оператором `MPI_SUM`.
4. **Трансляция результатов **: Корневой процесс транслирует конечный вектор результатов всем другим процессам, используя `MPI_Bcast`, чтобы гарантировать, что у всех участников есть готовое решение.

Эта схема сводит к минимуму обмен данными, ограничивая его первоначальным распределением и окончательным сокращением, что является эффективным для такого типа задач.

## 5. Детали реализации
- **Структура кода**: Реализация разделена на два основных класса:
  - `KopilovDSumValColMatSEQ`: Содержит последовательную реализацию (`tasks/kopilov_d_sum_val_col_mat/seq/src/ops_seq.cpp`).
  - `KopilovDSumValColMatMPI`: Содержит параллельную реализацию на основе MPI (`tasks/kopilov_d_sum_val_col_mat/mpi/src/ops_mpi.cpp`).
  Оба класса наследуются от общего интерфейса, определенного классом `ppc::task::Task`.

- **Обработка данных**: Матрица сохраняется в порядке возрастания строк в формате "std::vector<double>". Этап "Предварительной обработки" гарантирует, что входные данные верны, и инициализирует выходные структуры перед началом основного вычисления.

## 6. Экспериментальная установка
- **Аппаратное обеспечение/операционная система**: 
  - ПРОЦЕССОР: <Intel Core i5-1155G7 2.50GHz>
  - Оперативная память: <8 ГБ>
  - ОПЕРАЦИОННАЯ система: <Windows 11>
- **Набор инструментов**:
  - Компилятор: <MinGW-w64 GCC 11.2.0>
  - MPI: <MS-MPI версии 10.1.2>
  - Тип сборки: `Release`
- Среда ****:
  - Количество процессов контролировалось с помощью `mpiexec -n <N> ...`, где `<N>` соответствует столбцу "Count" в таблице результатов.
- **Данные**: Тестовые данные (матрицы) были сгенерированы случайным образом на этапе предварительной обработки задачи. Размеры матрицы были изменены для тестирования производительности.

## 7. Результаты и обсуждение

### 7.1 Корректность
Корректность реализации MPI была проверена путем сравнения ее выходных данных с результатами, полученными при последовательной реализации с использованием тех же входных данных. Проверка постобработки фреймворка подтверждает, что результаты совпадают для различных размеров матриц и количества процессов.

### 7.2 Производительность

| Режим | Количество | Время, с | Ускорение | Эффективность |
|-------------|-------|---------|---------|-----------------|
| seq         | 1     | 0.0100  | 1.00    | N/A             |
| mpi         | 2     | 0.0415  | 0.24    | 12.0%           |
| mpi         | 4     | 0.0437  | 0.23    | 5.7%            |
| mpi         | 8     | 0.0477  | 0.21    | 2.6%            |

Результаты показывают, что в данном конкретном случае параллельная реализация MPI работает значительно медленнее, чем последовательная. Более того, время выполнения увеличивается с добавлением новых процессов.

Это классический пример ситуации, когда накладные расходы на распараллеливание (инициализация MPI, распределение данных через `MPI_Scatterv`, сбор результатов через `MPI_Reduce`) значительно превышают выигрыш от параллельного выполнения вычислений. Размер тестовой задачи слишком мал, и основное время тратится не на вычисления, а на коммуникацию между процессами. Данные результаты наглядно демонстрируют, что параллельные алгоритмы эффективны только тогда, когда вычислительная нагрузка на каждый процесс достаточно велика, чтобы компенсировать затраты на обмен данными.

## 8. Выводы
Основанный на MPI параллельный алгоритм суммирования столбцов матрицы был успешно реализован и протестирован. Выбранная схема распараллеливания, основанная на построчной декомпозиции данных, оказалась эффективной, продемонстрировав хорошую масштабируемость и прирост производительности. Основными узкими местами являются операции `MPI_Scatterv` и `MPI_Reduce`, но их влияние минимально при достаточно больших вычислительных нагрузках на каждый процесс.

## 9. Ссылки
1. MPI: Полный справочник (том 1) - https://www.mpi-forum.org/docs/
2. Документация по библиотеке Intel oneAPI MPI - https://www.intel.com/content/www/us/en/developer/tools/oneapi/mpi-library.html

## Приложение (необязательно)
``cpp
// Выдержка из основной логики параллельного выполнения в RunImpl()
int rank = 0, size = 1;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
MPI_Comm_size(MPI_COMM_WORLD, &size);

// ... широковещательные размеры ...

// ... рассчитать распределение данных для Scatterv ...
std::vector<int> отправляет данные(размер, 0);
std::vector<int> отображает данные(размер, 0);
// ...

// Распределяет данные из корневого каталога по всем процессам
MPI_Scatterv(sendbuf_ptr, send_counts.data(), displs.data(), MPI_DOUBLE, recvbuf.data(), recv_count, MPI_DOUBLE, 0, MPI_COMM_WORLD);

// Каждый процесс вычисляет свою локальную сумму
std::vector<double> local_col_sum(cols, 0.0);
int local_rows = recvbuf.size() / cols;
for (int r = 0; r < local_rows; ++r) _BOS_
  для (int c = 0; c < cols; ++c) {
    local_col_sum[c] += recvbuf[r * cols + c];
  }
}

// Преобразовать все локальные суммы в глобальную сумму в корневом процессе
std::vector<double> global_col_sum(cols, 0.0);
MPI_Reduce(local_col_sum.data(), global_col_sum.data(), cols, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
```