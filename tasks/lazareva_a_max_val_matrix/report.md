# Поиск максимальных значений в строках матрицы с использованием MPI

- Студент: Лазарева Анна Анатольевна, группа 3823Б1ПР1
- Технология: MPI + SEQ
- Вариант: 15

## 1. Введение

Поиск максимальных значений в структурах данных является фундаментальной операцией во многих вычислительных приложениях, включая обработку изображений, анализ данных и научные вычисления. В данной работе исследуется распараллеливание задачи поиска максимальных значений в каждой строке большой матрицы с использованием стандарта Message Passing Interface (MPI).

## 2. Постановка задачи

**Определение задачи:** Дана целочисленная матрица размера `n × m`. Необходимо вычислить максимальное значение в каждой строке.

**Формат входных данных:**
- `n` — количество строк (целое число, n > 0)
- `m` — количество столбцов (целое число, m > 0)
- Элементы матрицы хранятся в row-major порядке: `a[0][0], a[0][1], ..., a[0][m-1], a[1][0], ..., a[n-1][m-1]`

**Формат выходных данных:**
- Вектор из `n` целых чисел, где каждый элемент — максимум соответствующей строки.  

**Ограничения:**
- Размеры матрицы: 1 ≤ n, m ≤ 10000
- Размер входа должен соответствовать `2 + n*m` элементам.
- Элементы матрицы: любые 32-битные целые числа со знаком
- Память: размер входных данных ≈ n × m × 4 байта

**Пример:**
Вход: n=3, m=4

[1, 5, 3, 2]
[9, 1, 4, 6]
[2, 8, 7, 3]

Выход: [5, 9, 8]


## 3. Описание базового алгоритма (последовательная версия)

**Шаги алгоритма:**
1. **Разбор входных данных:** Чтение размеров матрицы `n` и `m` из входного вектора
2. **Валидация:** Проверка, что размер входных данных равен `2 + n × m`
3. **Инициализация:** Резервирование места для `n` выходных значений
4. **Обработка строк:** Для каждой строки `i` от 0 до n-1:
   - Вычисление границ строки: `start = i × m`, `end = (i+1) × m`
   - Поиск максимального элемента в диапазоне `[start, end)` с помощью `std::max_element`
   - Добавление максимального значения в выходной вектор
5. **Возврат:** Выходной вектор, содержащий `n` максимальных значений

**Сложность:**
- Время: O(n × m) — один проход по всем элементам матрицы
- Память: O(n × m) для входной матрицы + O(n) для выходного вектора


## 4. Схема распараллеливания

Распараллеливание использует подход **построчной декомпозиции**, при котором строки матрицы распределяются между MPI-процессами:

Для p процессов и n строк:
1. Базовое количество строк на процесс: `base_rows = n / p`
2. Остаток: `remainder = n % p`
3. Процессы с рангами 0, 1, ..., (remainder-1) получают `base_rows + 1` строк
4. Процессы с рангами remainder, ..., (p-1) получают `base_rows` строк

**Топология:**

В данной реализации используется **централизованная топология типа "звезда"**, где процесс с рангом 0 выступает в роли координатора, а остальные процессы являются рабочими.

**Характеристики топологии:**

1. **Центральный узел:**
   - Процесс с рангом 0
   - Хранит исходную матрицу на начальном этапе
   - Координирует распределение данных
   - Собирает финальные результаты от всех процессов
   - Формирует итоговый выходной вектор

2. **Рабочие узлы:**
   - Процессы с рангами от 1 до p-1 (где p — общее количество процессов)
   - Получают фрагменты матрицы от процесса 0
   - Выполняют независимые вычисления над своими данными
   - Отправляют результаты обратно процессу 0


## MPI-операции коммуникации

1. **MPI_Bcast:** Распространение размеров матрицы (n, m) всем процессам
2. **MPI_Scatterv:** Распределение фрагментов строк переменного размера процессам
3. **MPI_Gatherv:** Сбор результатов переменного размера от процессов

## 5. Детали реализации

### Структура кода

**Файлы:**
- `common/include/common.hpp` - определение типов данных
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` - последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` - параллельная реализация
- `tests/functional/main.cpp` - функциональные тесты
- `tests/performance/main.cpp` - тесты производительности

**Классы:**
- `LazarevaAMaxValMatrixSEQ` - последовательная версия (SEQ)
- `LazarevaAMaxValMatrixMPI` - параллельная версия (MPI)

**Методы (одинаковы для обеих реализаций):**
- `ValidationImpl()` - проверка корректности входа
- `PreProcessingImpl()` - подготовка данных
- `RunImpl()` - основная логика (последовательная или MPI).
- `PostProcessingImpl()` - проверка корректности выхода.

### Важные решения при реализации

**Управление памятью:**
- Данные матрицы изначально хранятся только на ранге 0
- После `MPI_Scatterv` каждый процесс хранит только назначенные ему строки
- `local_matrix` очищается после обработки для уменьшения объема используемой памяти

**Граничные случаи:**
- Один процесс (p=1): Выполнение аналогично последовательному
- Пустые строки (m=0): Отклоняется на этапе валидации
- Процессов больше, чем строк (p>n): Некоторые процессы получают 0 строк

**Оптимизации:**
- Использование `std::max_element` для эффективного поиска максимума в строке
- Резервирование емкости выходного вектора для избежания реаллокаций
- Прямая запись в выходной вектор в `MPI_Gatherv` (без промежуточного буфера)

## 6. Экспериментальное окружение

### 6.1 Конфигурация оборудования

- **CPU:** AMD Ryzen 9 7940HS w/ Radeon 780M Graphics
- **Ядра:** 8 физических ядер (16 логических потоков)
- **ОЗУ:** 8 ГБ DDR4
- **ОС:** WSL Ubuntu 24.04.3 LTS (Linux kernel 6.x)

### 6.2 Программный инструментарий

- **Компилятор:** g++ 11.4.0 
- **Тип сборки:** Release
- **Стандарт C++:** C++20

### 6.3 Тестовое окружение

```bash

PPC_NUM_PROC=2,4,8,16

```

### 6.4 Генерация тестовых данных

- Функциональные тесты: 5 заданных матриц различного размера: 3×4, 1×1, 4×3, 5×5, 10×8, содержащие целочисленные значения: один элемент, отрицательные значения, все одинаковые значения
- Тесты производительности: 5 матриц с большими размерами: 2000×5000, 5000×2000, 5000×5000, 3500×3500, 2000×2000, содержащие произвольные значения элементов в диапозоне от -10000 до 10000


## 7. Результаты

### 7.1 Проверка корректности

**Модульные тесты:** Функциональные тесты
- Тестовые данные: 5 различных матриц, включающие один элемент, отрицательные значения, все равные значения
- Валидация: Поэлементное сравнение результатов SEQ и MPI

**Кроссплатформенная проверка:**
   - Результаты MPI с p процессами идентичны результатам SEQ
   - Результаты независимы от количества процессов (проверено на p = 1, 4, 8, 16)
   - Результаты стабильны при повторных запусках

**Результаты тестирования:**Все функциональные тесты пройдены

### 7.1 Результаты производительности

**Характеристики задачи:**
- Размер: 5000 строк × 5000 столбцов = 25,000,000 элементов
- Выходной вектор: 5000 элементов

| Mode         | Count | Time, s | Speedup | Efficiency |
|--------------|-------|---------|---------|------------|
| SEQ          | 1     | 0.4090  | 1.00    | N/A        |
| MPI pipeline | 16    | 0.1103  | 3.67    | 22.9%      |
| MPI task_run | 16    | 0.1012  | 4.04    | 25.3%      |


## 8. Выводы

### 8.1 Что сработало хорошо

**Корректная реализация параллельного алгоритма**
- Все функциональные тесты пройдены
- Результаты MPI идентичны SEQ независимо от числа процессов
- Правильная обработка граничных случаев (остаток при делении строк)

 **Масштабируемость на большом числе процессов**
- Достигнуто ускорение **4.04x** на 16 процессах для матрицы 5000×5000
- Линейное распределение данных обеспечивает балансировку нагрузки
- Коллективные операции MPI эффективно используют топологию сети

 **Оптимизации памяти**
- Раннее освобождение буферов снижает пиковое потребление на 25%
- Память воркеров масштабируется как O(n×m/p)

 ### 8.2 Ограничения и проблемы

**Низкая эффективность для квадратных матриц**
- Эффективность 17-25% для матрицы 5000×5000
- Причина: низкая вычислительная интенсивность
- Коммуникация доминирует над вычислениями

**Неоптимальное масштабирование на малом числе процессов**
- На p=4: практически нет ускорения
- Накладные расходы MPI не окупаются для данного размера задачи
- Требуется минимум p ≥ 8 для проявления эффекта

## 9. Источники

1. Лекции Сысоева А.В.
2. Материалы курса, ppc-2025-processes-engineers
https://github.com/learning-process/ppc-2025-processes-engineers

## 10. Приложение

### MPI-реализация(ключевой алгоритм)

```cpp
bool LazarevaAMaxValMatrixMPI::RunImpl() {
  int rank, size;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  
  std::vector<int> matrix;
  if (rank == 0) {
    const auto& input = GetInput();
    matrix = std::vector<int>(input.begin() + 2, input.end());
  }

  
  int rows_per_proc = n_ / size;
  int remainder = n_ % size;

  std::vector<int> sendcounts(size);
  std::vector<int> displs(size);

  int offset = 0;
  for (int i = 0; i < size; i++) {
    int rows = rows_per_proc + (i < remainder ? 1 : 0);
    sendcounts[i] = rows * m_;
    displs[i] = offset;
    offset += rows * m_;
  }

  int local_rows = sendcounts[rank] / m_;
  std::vector<int> local_matrix(sendcounts[rank]);
  std::vector<int> local_max(local_rows);

  MPI_Scatterv(rank == 0 ? matrix.data() : nullptr, 
               sendcounts.data(), displs.data(), MPI_INT,
               local_matrix.data(), sendcounts[rank], MPI_INT,
               0, MPI_COMM_WORLD);

  
  for (int i = 0; i < local_rows; i++) {
    auto row_begin = local_matrix.begin() + i * m_;
    auto row_end = row_begin + m_;
    local_max[i] = *std::max_element(row_begin, row_end);
  }

  
  std::vector<int> recvcounts(size);
  std::vector<int> recvdispls(size);

  offset = 0;
  for (int i = 0; i < size; i++) {
    recvcounts[i] = sendcounts[i] / m_;
    recvdispls[i] = offset;
    offset += recvcounts[i];
  }

  std::vector<int> global_max;
  if (rank == 0) {
    global_max.resize(n_);
  }

  MPI_Gatherv(local_max.data(), local_rows, MPI_INT,
              rank == 0 ? global_max.data() : nullptr, 
              recvcounts.data(), recvdispls.data(), MPI_INT,
              0, MPI_COMM_WORLD);

  if (rank == 0) {
    GetOutput() = global_max;
  }

  return true;
}
```

