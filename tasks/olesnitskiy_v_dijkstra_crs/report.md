# Поиск кратчайших путей из одной вершины (алгоритм Дейкстры). С CRS формой хранения графа.

-  Student: Олесницкий Владимир Тарасович, group 3823Б1ПР2
-  Technology: SEQ | MPI
-  Variant: 22

## 1. Introduction
Алгоритм Дейкстры — классический алгоритм поиска кратчайших путей от одной вершины до всех остальных во взвешенном графе с неотрицательными весами рёбер. Задача имеет широкое применение в маршрутизации сетей, транспортном планировании и анализе социальных сетей.

## 2. Problem Statement
**Задача**: Найти кратчайшие расстояния от заданной вершины до всех остальных в взвешенном графе.
**Формально**:
-   Вход:
    
    -   `source` — начальная вершина
        
    -   `offsets` — массив смещений в CRS-формате
        
    -   `edges` — массив смежных вершин
        
    -   `weights` — массив весов рёбер
        
-   Выход: `distances` — массив кратчайших расстояний от `source` до каждой вершины

**Ограничения**:

-   Веса рёбер неотрицательные
    
-   Граф может быть разреженным или плотным
    
-   Максимальное количество вершин ограничено доступной памятью

## 3. Baseline Algorithm (Sequential)
Базовый последовательный алгоритм Дейкстры с использованием CRS-формата:
```cpp
std::vector<int> distances(vertices, INF);
distances[source] = 0;

std::priority_queue<pair<int, int>, vector<pair<int, int>>, greater<>> pq;
pq.emplace(0, source);

while (!pq.empty()) {
    auto [current_dist, u] = pq.top();
    pq.pop();
    
    if (current_dist > distances[u]) continue;
    
    int start = offsets[u];
    int end = offsets[u + 1];
    
    for (int i = start; i < end; ++i) {
        int v = edges[i];
        int weight = weights[i];
        int new_dist = current_dist + weight;
        
        if (new_dist < distances[v]) {
            distances[v] = new_dist;
            pq.emplace(new_dist, v);
        }
    }
}
```

**Сложность**:

-   Временная: `O((V + E) log V)` с использованием двоичной кучи
    
-   Пространственная: `O(V + E)` для хранения графа и вспомогательных структур
    

**Преимущества CRS-формата**:

-   Эффективное использование памяти для разреженных графов
    
-   Быстрый доступ к соседям вершины
    
-   Минимальные накладные расходы на хранение структуры графа

**Особенности striped-версии**:

-   Автоматически определяет число полос `num_stripes` (общий делитель размеров, ≤8)
    
-   Улучшает локальность данных за счёт блочной обработки
    
-   При `num_stripes = 1` вырождается в стандартный алгоритм

## 4. Parallelization Scheme
**Схема MPI**: Распределение вершин по процессам с координацией через глобальную очередь

**Распределение данных**:

1.  Вершины равномерно распределяются между процессами
    
2.  Каждый процесс хранит локальные расстояния для своих вершин
    
3.  Глобальная координация через `MPI_Allreduce` с `MPI_MINLOC` для нахождения следующей вершины для обработки
    

**Коммуникационный паттерн**:

1.  Рассылка структуры графа всем процессам (`MPI_Bcast`)
    
2.  Итеративная обработка:
    
    -   Каждый процесс находит локально минимальное расстояние
        
    -   Глобальная редукция для выбора следующей вершины (`MPI_Allreduce`)
        
    -   Рассылка обновлений расстояний (`MPI_Alltoallv`)
        
    -   Синхронизация активности процессов (`MPI_Allreduce`)
        
3.  Сбор результатов на главном процессе (`MPI_Gatherv`)

**Псевдокод MPI**:

```cpp
MPI_Init()
rank = MPI_Comm_rank()
size = MPI_Comm_size()

if rank == 0:
    read_graph(source, offsets, edges, weights)
    distribute_graph()  // MPI_Bcast

divide_vertices_among_processes()

while active_processes > 0:
    local_min = find_local_min_distance()
    global_min = MPI_Allreduce(local_min, MPI_MINLOC)
    
    if global_min.vertex belongs to this process:
        relax_edges(global_min.vertex)
        send_updates_to_other_processes()
    
    receive_updates_from_others()
    update_local_distances()
    
    active = check_local_activity()
    MPI_Allreduce(active, MPI_SUM)

if rank == 0:
    gather_results()
MPI_Finalize()
```
## 5. Implementation Details
**Структура кода**:
```text
olesnitskiy_v_dijkstra_crs/
├── common/
│   └── include/common.hpp      # Общие типы данных
├── mpi/
│   ├── include/ops_mpi.hpp     # MPI реализация
│   └── src/ops_mpi.cpp
├── seq/
│   ├── include/ops_seq.hpp     # SEQ реализация
│   └── src/ops_seq.cpp
└── tests/
    ├── functional/main.cpp     # Функциональные тесты
    └── performance/main.cpp    # Тесты производительности
```

**Ключевые классы**:
-   `OlesnitskiyVDijkstraCrsSEQ` — последовательная реализация
    
-   `OlesnitskiyVDijkstraCrsMPI` — MPI реализация
    
**Особенности реализации**:

1.  **Динамическое распределение вершин**: Вершины равномерно распределяются по процессам с учетом остатка
    
2.  **Эффективная координация**: Использование `MPI_MINLOC` для нахождения глобально ближайшей вершины
    
3.  **Пакетная отправка обновлений**: Обновления расстояний накапливаются и отправляются пакетами через `MPI_Alltoallv`
    
4.  **Асинхронная обработка**: Процессы работают независимо, синхронизируясь только при выборе следующей вершины
    

**Использование памяти**:

-   Каждый процесс хранит: локальную часть графа, расстояния для своих вершин, очереди обновлений
    
-   Главный процесс дополнительно хранит полный массив расстояний


## 6. Experimental Setup
**Аппаратное обеспечение/ОС**:

-   **Модель ЦП**: AMD Ryzen 5 5600H with Radeon Graphics
    
-   **Архитектура**: x86_64, 6 физических ядер, 12 логических потоков (2 потока на ядро)
    
-   **Тактовая частота**: 3.30 GHz base, до 4.2 GHz turbo
    
-   **Кэш**: L1 384 KB, L2 3 MB, L3 16 MB
    
-   **ОЗУ**: 14 ГБ DDR4 (8.3 GiB доступно)
    
-   **ОС**: Ubuntu 25.10 (последняя версия)
    
-   **Ядро**: Linux 6.17.0-7-generic
    

**Инструментарий**:

-   **Компилятор C/C++**: GCC 15.2.0 (Ubuntu 15.2.0-4ubuntu4)
    
-   **Версия MPI**: OpenMPI 5.0.8
    
-   **CMake**: 3.31.6
    
-   **GNU Make**: 4.4.1
    
-   **Поддержка OpenMP**: 201511 (OpenMP 5.0)

**Тестовые данные**:

-   Графы размером до 1,000,000 вершин
    
-   Средняя степень вершин: ~100 рёбер
    
-   Веса рёбер: случайные значения от 1 до 20
    
-   Тип данных: целые числа (int)

## 7. Results and Discussion

### 7.1 Correctness

Корректность реализации подтверждена комплексным набором из 14 функциональных тестов:

1.  **Базовые случаи**:
    
    -   Одна вершина (0 рёбер)
        
    -   Две вершины (взаимное ребро)
        
2.  **Различные структуры графов**:
    
    -   Цепочка вершин (5 и 20 вершин)
        
    -   Звездообразный граф (6 вершин)
        
    -   Полный граф (4 вершины)
        
3.  **Специальные случаи**:
    
    -   Несвязный граф
        
    -   Граф с различными весами рёбер
        
    -   Граф с нулевыми весами
        
4.  **Случайные графы**:
    
    -   Разреженный граф (10 вершин, 15 рёбер)
        
    -   Плотный граф (8 вершин)
        
5.  **Структурированные графы**:
    
    -   Дерево (7 вершин)
        
    -   Сетка (3×3)
        

Все 14 тестов пройдены успешно для обеих реализаций (SEQ и MPI).

### 7.2 Performance
Результаты тестирования производительности (граф с 1,000,000 вершин и ~100,000,000 рёбер):

| Mode | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| sequential | 1 | 0.59027 | 1.00 | N/A |
| mpi | 2 | 2.82369 | 0.21 | 10.5% |
| mpi | 3 | 3.49070 | 0.17 | 5.7% |
| mpi | 4 | 3.61327 | 0.16 | 4.1% |

## 8. Conclusions

**Основные результаты**:

1. Корректность: Все функциональные тесты успешно пройдены

2. Производительность SEQ: Отличная - около 0.59 секунд для графа значительного размера

3. Производительность MPI: Неудовлетворительная - в 6.1 раза медленнее SEQ версии на 4 процессах

4. Эффективность MPI: Очень низкая - снижается с 10.5% на 2 процессах до 4.1% на 4 процессах
    

**Анализ результатов**:

1. Негативное ускорение: MPI реализация показывает значительное замедление

-   На 2 процессах: в 4.8 раза медленнее SEQ (speedup = 0.21)

-   На 4 процессах: в 6.1 раза медленнее SEQ (speedup = 0.16)

2. Ухудшение с ростом числа процессов:

-   С увеличением количества процессов время выполнения увеличивается

-   Эффективность падает с 10.5% до 4.1%

**Причины низкой производительности MPI:**

1. Последовательная природа алгоритма Дейкстры:

-   Алгоритм обрабатывает вершины последовательно по возрастанию расстояния

-  Каждая итерация зависит от результата предыдущей

2. Высокие коммуникационные затраты:

-   Частые операции MPI_Allreduce, MPI_Bcast, MPI_Alltoallv

-   Глобальная синхронизация на каждой итерации

-   Большой объем данных для обмена между процессами

3. Неэффективное распределение данных:

-   Большинство рёбер каждой вершины оказываются на других процессах

-   При высокой степени графа (~100 рёбер на вершину) почти каждое обновление требует коммуникации

4. Накладные расходы на координацию:

-   Все процессы должны синхронизироваться для выбора следующей вершины

-   Глобальные операции редукции создают узкое место

**Заключение**:  
Последовательная реализация алгоритма Дейкстры в CRS-формате демонстрирует отличную производительность. Параллельная MPI реализация в текущем виде непригодна для практического использования из-за значительного замедления и крайне низкой эффективности.

## 9. References
1. [Учебные материалы](https://disk.yandex.ru/d/NvHFyhOJCQU65w)

## Appendix
```cpp
// Ключевой фрагмент: распределение вершин по процессам
std::vector<int> CalculateVertexDistribution(int vertices, int size) {
    std::vector<int> counts(size);
    int base = vertices / size;
    int remainder = vertices % size;
    
    for (int i = 0; i < size; ++i) {
        counts[i] = base + (i < remainder ? 1 : 0);
    }
    return counts;
}
```