# Ленточная горизонтальная схема А, вертикальное В - умножение матрицы на матрицу

-  Student: Олесницкий Владимир Тарасович, group 3823Б1ПР2
-  Technology: SEQ | MPI
-  Variant: 14

## 1. Introduction
Умножение матриц - фундаментальная операция линейной алгебры, широко используемая в научных вычислениях, машинном обучении и компьютерной графике. Наивный алгоритм имеет сложность O(n³), что делает его вычислительно дорогим для больших матриц. Цель работы - реализовать и проанализировать параллельную версию алгоритма умножения матриц с использованием ленточной схемы распределения данных в MPI.

## 2. Problem Statement
**Задача**: Умножение двух матриц A и B: C = A × B

**Формально**:

-   Вход: A ∈ ℝ^(m×k), B ∈ ℝ^(k×n)
    
-   Выход: C ∈ ℝ^(m×n), где C[i][j] = Σ_{l=0}^{k-1} A[i][l] × B[l][j]
    

**Ограничения**:

-   cols_a == rows_b (матрицы совместимы для умножения)
    
-   Элементы матриц - числа с плавающей точкой двойной точности (double)
    
-   Максимальный размер ограничен доступной памятью

## 3. Baseline Algorithm (Sequential)
Базовый последовательный алгоритм:
```cpp
for (size_t i = 0; i < rows_a; ++i) {
    for (size_t j = 0; j < cols_b; ++j) {
        double sum = 0.0;
        for (size_t k = 0; k < cols_a; ++k) {
            sum += A[(i * cols_a) + k] * B[(k * cols_b) + j];
        }
        C[i * cols_b + j] = sum;
    }
}
```
**Оптимизированная версия с блочным разбиением** (striped):
```cpp
if (num_stripes_ > 1) {
    size_t rows_per_stripe = rows_a / num_stripes_;
    size_t cols_per_stripe = cols_b / num_stripes_;
    for (int stripe_a = 0; stripe_a < num_stripes_; ++stripe_a) {
        size_t start_row_a = stripe_a * rows_per_stripe;
        for (int stripe_b = 0; stripe_b < num_stripes_; ++stripe_b) {
            size_t start_col_b = stripe_b * cols_per_stripe;
            for (size_t i = 0; i < rows_per_stripe; ++i) {
                size_t row_idx = start_row_a + i;
                for (size_t j = 0; j < cols_per_stripe; ++j) {
                    size_t col_idx = start_col_b + j;
                    double sum = 0.0;
                    for (size_t k = 0; k < cols_a; ++k) {
                        sum += A[row_idx * cols_a + k] * 
                               B[k * cols_b + col_idx];
                    }
                    C[row_idx * cols_b + col_idx] = sum;
                }
            }
        }
    }
} else {
    // Стандартный алгоритм (num_stripes_ == 1)
    }
}
```

**Сложность**: O(m × n × k) операций умножения-сложения (одинаковая для обеих версий)

**Особенности striped-версии**:

-   Автоматически определяет число полос `num_stripes` (общий делитель размеров, ≤8)
    
-   Улучшает локальность данных за счёт блочной обработки
    
-   При `num_stripes = 1` вырождается в стандартный алгоритм

## 4. Parallelization Scheme
**Схема**: 
- MPI: Горизонтальные полосы матрицы A, полная матрица B рассылается всем процессам
- SEQ: Блочное разбиение обеих матриц на num_stripes полос (горизонтальные для A, вертикальные для B)

**Распределение данных**:

1.  Матрица A разбивается на горизонтальные полосы по строкам
    
2.  Матрица B полностью рассылается всем процессам
    
3.  Каждый процесс умножает свою полосу A на всю матрицу B
    
4.  Результаты собираются на главном процессе
    

**Коммуникационный паттерн**:
```text
Процесс 0:      A0 ----------> C0
                ↓ Broadcast B
Процесс 1:      A1 ----------> C1
                ↓ Broadcast B
Процесс 2:      A2 ----------> C2
                ↓ Broadcast B
...             ...
                ↓ Gatherv C
Процесс 0:      C = [C0, C1, C2]
```

**Псевдокод**:
```text
MPI_Init()
rank = MPI_Comm_rank()
size = MPI_Comm_size()

if rank == 0:
    read_matrices(A, B)
    calculate_row_distribution()

MPI_Scatterv(A, local_a)          # Распределение полос A
MPI_Bcast(B)                      # Рассылка всей матрицы B

# Локальное умножение
for i in local_rows:
    for j in cols_b:
        C_local[i][j] = Σ_k(local_a[i][k] * B[k][j])

MPI_Gatherv(C_local, C)          # Сбор результатов
MPI_Bcast(C)                     # Рассылка результата всем
MPI_Finalize()
```

## 5. Implementation Details
**Структура кода**:
```text
olesnitskiy_v_striped_matrix_multiplication/
├── common/
│   └── include/common.hpp      # Общие типы данных
├── mpi/
│   ├── include/ops_mpi.hpp     # MPI реализация
│   └── src/ops_mpi.cpp
├── seq/
│   ├── include/ops_seq.hpp     # SEQ реализация
│   └── src/ops_seq.cpp
└── tests/
    ├── functional/main.cpp     # Функциональные тесты
    └── performance/main.cpp    # Тесты производительности
```

**Ключевые классы**:
-   `OlesnitskiyVStripedMatrixMultiplicationSEQ` - последовательная реализация
-   `OlesnitskiyVStripedMatrixMultiplicationMPI` - MPI реализация
    
**Особенности реализации**:

1.  **Распределение строк с остатком**: Для равномерной нагрузки используется распределение строк по формуле: `base + (rank < remainder ? 1 : 0)`
    
2.  **Обработка малых матриц**: Если строк в A меньше, чем процессов, вычисление выполняется только на процессе 0
    
3.  **Валидация**: Проверка совместимости размеров и корректности данных
    
4.  **Сбор результатов**: Использование `MPI_Gatherv` для сбора полос результата
    

**Использование памяти**:

-   Каждый процесс хранит: полосу A, полную матрицу B, полосу результата C
    
-   Главный процесс дополнительно хранит полные матрицы A и C


## 6. Experimental Setup
**Аппаратное обеспечение/ОС**:

-   **Модель ЦП**: AMD Ryzen 5 5600H with Radeon Graphics
    
-   **Архитектура**: x86_64, 6 физических ядер, 12 логических потоков (2 потока на ядро)
    
-   **Тактовая частота**: 3.30 GHz base, до 4.2 GHz turbo
    
-   **Кэш**: L1 384 KB, L2 3 MB, L3 16 MB
    
-   **ОЗУ**: 14 ГБ DDR4 (8.3 GiB доступно)
    
-   **ОС**: Ubuntu 25.10 (последняя версия)
    
-   **Ядро**: Linux 6.17.0-7-generic
    

**Инструментарий**:

-   **Компилятор C/C++**: GCC 15.2.0 (Ubuntu 15.2.0-4ubuntu4)
    
-   **Версия MPI**: OpenMPI 5.0.8
    
-   **CMake**: 3.31.6
    
-   **GNU Make**: 4.4.1
    
-   **Поддержка OpenMP**: 201511 (OpenMP 5.0)

**Тестовые данные**:

-   Матрицы размером 1024×1024 элементов
    
-   Тип данных: double (8 байт на элемент)
    
-   Объем данных на процесс:
    
    -   SEQ: ~24 MB (A+B+C)
        
    -   MPI: ~16 MB (локальная_A + полная_B + локальная_C)
        
-   Генерация: псевдослучайные числа в диапазоне [0.0, 1.0)

## 7. Results and Discussion

### 7.1 Correctness
Корректность реализации подтверждена комплексным набором из 15 функциональных тестов:

1.  **Базовые случаи**:
    
    -   Матрица 1×1 (простое умножение 2×3=6)
        
    -   Матрица 2×2 (проверка базовых операций)
        
2.  **Специальные матрицы**:
    
    -   Единичные матрицы: A×I = A, I×B = B (тесты 3, 4)
        
    -   Нулевая матрица: 0×B = 0 (тест 11)
        
3.  **Различные размерности**:
    
    -   Кратные 4: 4×4, 8×8, 12×12 (тесты 4, 7, 13)
        
    -   Некратные: 3×3, 5×5, 6×6, 7×7, 9×9, 10×10 (тесты 3, 6, 9, 10, 14, 15)
        
    -   Простые числа: 7×7 (тест 14)
        
4.  **Прямоугольные матрицы**:
    
    -   2×3 × 3×2 = 2×2 (тест 5)
        
    -   3×4 × 4×2 = 3×2 (тест 8)
        
    -   4×6 × 6×3 = 4×3 (тест 12)
        
5.  **Особые значения**:
    -   Отрицательные числа (тест 13)
 
    -   Нулевые значения (тест 11)
        

Все 15 тестов пройдены успешно для обеих реализаций (SEQ и MPI) с допустимой погрешностью ε = 1e-6.

### 7.2 Performance
Present time, speedup and efficiency. Example table:

| Mode | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| sequential | 1 | 4.03364 | 1.00 | N/A |
| mpi | 2 | 1.96032 | 2.06 | 103.0% |
| mpi | 3 | 1.42989 | 2.82 | 94.0% |
| mpi | 4 | 1.20393 | 3.35 | 83.8% |


**Анализ результатов**:

1. SEQ vs MPI: MPI реализация показывает значительное ускорение

На 4 процессах: 3.35× ускорение относительно последовательной версии

На 3 процессах: 2.82× ускорение

На 2 процессах: 2.06× ускорение

2. Эффективность:

MPI на 2 процессах: 103.0% (незначительное суперлинейное ускорение)

MPI на 3 процессах: 94.0% (высокая эффективность)

MPI на 4 процессах: 83.8% (хорошая эффективность)

3. Сравнение режимов выполнения:

task_run и pipeline показывают схожую производительность (разница <5%)

В последовательной реализации разница также минимальна (~1%)

**Обсуждение**: 
 
Суперлинейное ускорение на 2 процессах (103.0%) объясняется:

Кэш-эффект: Каждый процесс работает с меньшим объёмом данных, что улучшает использование кэша процессора

Локализация данных: Полосочное распределение матрицы A улучшает пространственную локальность

Параллельное чтение из памяти: Несколько процессов могут одновременно читать данные из ОЗУ

Снижение эффективности с ростом числа процессов:

На 2 процессах: 103.0%

На 3 процессах: 94.0%

На 4 процессах: 83.8%

Объясняется увеличением коммуникационных затрат и накладных расходов на синхронизацию.
    

**Ограничения**:

- Дублирование данных: Каждый процесс хранит полную матрицу B (8.4 МБ для матрицы 1024×1024)

- Коммуникационные затраты: Рассылка матрицы B всем процессам создаёт дополнительную нагрузку

- Дисбаланс нагрузки: При некратном распределении строк матрицы A (1024 строк на N процессов)

- Ограниченная масштабируемость: Для большего числа процессов потребуется перепроектирование алгоритма

**Особенности SEQ реализации**: 
- Поддерживает блочное умножение через `num_stripes` (до 8 полос)

- Автоматически определяет оптимальное число полос как общий делитель размеров

- При `num_stripes = 1` работает как классический алгоритм

## 8. Conclusions
**Основные выводы**:

Основные выводы:

1. Положительное ускорение: В отличие от предыдущей задачи, MPI-реализация умножения матриц показывает значительное ускорение - до 3.35× на 4 процессах.

2. Высокая эффективность:

На 2 процессах наблюдается суперлинейное ускорение (103.0% эффективность), что объясняется лучшей локализацией данных и кэш-эффектами

На 4 процессах эффективность составляет 83.8%, что является очень хорошим результатом

3. Масштабируемость:

С увеличением количества процессов с 2 до 4 время выполнения уменьшается с 1.960 с до 1.204 с

Эффективность постепенно снижается с 103.0% до 83.8%, что типично для MPI-программ из-за роста коммуникационных затрат
    

**Ограничения**:

1.  Требуется хранение полной матрицы B на каждом процессе
    
2.  Коммуникационные затраты ограничивают масштабируемость
    
3.  Низкая эффективность для маленьких матриц (< 256×256)

## 9. References
1. [Учебные материалы](https://disk.yandex.ru/d/NvHFyhOJCQU65w)

## Appendix
```cpp
// Ключевой фрагмент: распределение данных
std::vector<int> CalculateCounts(int total, int num_parts) {
std::vector<int> counts(num_parts, 0);
    int base = total / num_parts;
    int remainder = total % num_parts;
    for (int i = 0; i < num_parts; ++i) {
        counts[i] = base + (i < remainder ? 1 : 0);
    }
    return counts;
}
```