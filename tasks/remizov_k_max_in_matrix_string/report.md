# Поиск максимальных элементов в строках матрицы

- Студент: Ремизов Кирилл Львович, группа 3823Б1ПР4  
- Технологии: SEQ, MPI  
- Вариант: 15  

## 1. Введение  

Задача заключается в нахождении максимальных элементов в каждой строке матрицы. Была реализована последовательная (SEQ) и параллельная версии с использованием MPI. Параллельная реализация предназначена для ускорения обработки больших матриц за счёт распределения вычислений между несколькими процессами.

## 2. Постановка задачи  

**Входные данные:** Двумерный вектор целых чисел (матрица) `std::vector<std::vector<int>>`  

**Выходные данные:** Вектор целых чисел `std::vector<int>`, содержащий максимальные элементы каждой строки матрицы  

**Ограничения:**  
- Матрица может быть произвольного размера  
- Строки могут быть пустыми  
- Элементы могут быть отрицательными  
- Для пустой матрицы возвращается пустой вектор  

## 3. Базовый алгоритм (Последовательный)  

Базовый алгоритм реализован в классе `RemizovKMaxInMatrixStringSEQ`:  

```cpp
bool RemizovKMaxInMatrixStringSEQ::RunImpl() {
  if (GetInput().empty()) {
    return true;
  }

  std::vector<int> result;
  for (const auto &row : GetInput()) {
    if (!row.empty()) {
      int max_val = *std::max_element(row.begin(), row.end());
      result.push_back(max_val);
    }
  }

  GetOutput() = result;
  return true;
}
```

Алгоритм последовательно обрабатывает каждую строку матрицы, используя стандартную функцию `std::max_element` для нахождения максимального элемента в строке.

## 4. Схема распараллеливания  

Для распараллеливания используется технология MPI с распределением строк матрицы между процессами.

### Распределение данных:  
- Процесс с рангом 0 (мастер-процесс) распределяет строки матрицы между всеми процессами  
- Каждый процесс вычисляет максимальные элементы для своего набора строк  
- Результаты собираются мастер-процессом  

### Алгоритм распределения:  
```cpp
std::vector<int> RemizovKMaxInMatrixStringMPI::CalculatingInterval(int size_prcs, int rank, int count_rows) {
  if (count_rows <= 0 || size_prcs <= 0 || rank < 0 || rank >= size_prcs) {
    return {-1, -1};
  }

  const int base_rows = count_rows / size_prcs;
  const int extra_rows = count_rows % size_prcs;

  int start = 0;
  int end = 0;

  if (rank < extra_rows) {
    start = rank * (base_rows + 1);
    end = start + base_rows;
  } else {
    start = (extra_rows * (base_rows + 1)) + ((rank - extra_rows) * base_rows);
    end = start + base_rows - 1;
  }

  if (start >= count_rows) {
    return {-1, -1};
  }

  if (end >= count_rows) {
    end = count_rows - 1;
  }

  return {start, end};
}
```

### Коммуникационная схема:  
1. Мастер-процесс вычисляет интервалы строк для каждого процесса  
2. Отправляет интервалы соответствующим процессам  
3. Каждый процесс вычисляет максимальные элементы для своих строк  
4. Рабочие процессы отправляют результаты мастер-процессу  
5. Мастер-процесс собирает все результаты  
6. Результат рассылается всем процессам через `MPI_Bcast`  

## 5. Детали реализации  

### Структура кода:  
- `common.hpp` – общие определения типов данных  
- `ops_seq.hpp/cpp` – последовательная реализация  
- `ops_mpi.hpp/cpp` – параллельная MPI реализация  
- `main.cpp` – функциональные тесты  
- `main.cpp` – производительностные тесты  

### Ключевые классы:  
- `BaseTask` – базовый класс для задач  
- `RemizovKMaxInMatrixStringSEQ` – последовательная реализация  
- `RemizovKMaxInMatrixStringMPI` – параллельная MPI реализация  

### Особенности реализации:  
- Обработка пустых строк матрицы  
- Корректная работа с отрицательными числами  
- Равномерное распределение строк между процессами  
- Проверка корректности на всех этапах выполнения  

## 6. Экспериментальная установка  

### Аппаратное обеспечение:  
- Процессор: Intel Core i7 (8 ядер)  
- Оперативная память: 16 GB DDR4  
- ОС: NixOS 24.05  

### Инструментарий:  
- Компилятор: GCC 9.4.0  
- Версия MPI: OpenMPI 4.0.3  
- Тип сборки: Release  
- Фреймворк тестирования: Google Test  

### Тестовые данные:  
Для производительностного тестирования использовалась матрица размером 50 000 000 элементов.

## 7. Результаты и обсуждение  

### 7.1 Корректность  

Корректность реализации проверяется с помощью набора функциональных тестов:  

```cpp
const std::array<TestType, 6> kTestParam = {
    std::make_tuple(std::vector<std::vector<int>>{{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}, std::vector<int>{3, 6, 9}),
    std::make_tuple(std::vector<std::vector<int>>{{-1, -5, -3}, {-9, -2, -7}}, std::vector<int>{-1, -2}),
    std::make_tuple(std::vector<std::vector<int>>{{5, 8, 2, 10, 1}}, std::vector<int>{10}),
    std::make_tuple(std::vector<std::vector<int>>{{7, 7, 7}, {7, 7, 7}}, std::vector<int>{7, 7}),
    std::make_tuple(std::vector<std::vector<int>>{{1, 5, 1}, {3, 3, 4}}, std::vector<int>{5, 4}),
    std::make_tuple(std::vector<std::vector<int>>{{42}}, std::vector<int>{42})};
```

Все тесты успешно проходят для обеих реализаций (SEQ и MPI).

### 7.2 Производительность  

Результаты производительностного тестирования для матрицы размером 50 000 000 элементов в системе NixOS 24.05:

| Режим     | Реализация | Время выполнения, сек |
|-----------|------------|------------------------|
| pipeline  | MPI        | 0.0155396 |
| task_run  | MPI        | 0.0155316 |
| pipeline  | SEQ        | 0.0157063 |
| task_run  | SEQ        | 0.0157225 |

**Анализ результатов:**  
- Обе реализации (SEQ и MPI) демонстрируют очень близкое время выполнения (разница менее 0.002 секунды)  
- MPI реализация показывает незначительное преимущество (примерно на 0.0002 секунды быстрее)  
- Разница между режимами `pipeline` и `task_run` в рамках одной реализации минимальна  
- Общее время обработки матрицы из 50 млн элементов составляет около 0.0155 секунд  

**Объяснение результатов:**  
- Схожесть производительности объясняется тем, что задача является **вычислительно простой** (поиск максимума в строке)  
- Накладные расходы на межпроцессное взаимодействие в MPI компенсируют потенциальный выигрыш от параллелизма  
- Для матрицы такого размера и простоты операции преимущество MPI проявляется минимально  
- Эффективная оптимизация компилятором последовательного кода также снижает потенциальный выигрыш от параллелизации  

## 8. Выводы  

1. Реализованы корректные последовательная и параллельная версии алгоритма поиска максимальных элементов в строках матрицы  
2. Для матрицы размером 50 000 000 элементов в системе NixOS 24.05 **MPI реализация показывает незначительное преимущество** над последовательной (примерно 1% ускорения)  
3. Основные причины минимального ускорения:  
   - Низкая вычислительная сложность базовой операции (поиск максимума)  
   - Существенные накладные расходы на коммуникацию в MPI  
   - Эффективная оптимизация последовательного кода компилятором  
4. Алгоритм эффективно работает с матрицами различных размеров и корректно обрабатывает граничные случаи  
5. Для достижения более заметного ускорения при использовании MPI рекомендуется:  
   - Использовать матрицы значительно большего размера  
   - Увеличивать сложность вычислений в каждой строке  
   - Оптимизировать схему распределения данных и коммуникации  

## 9. Источники  

1. Документация MPI: https://www.open-mpi.org/doc/  
2. Стандартная библиотека C++: https://en.cppreference.com/  
3. Фреймворк тестирования Google Test: https://github.com/google/googletest  

## Приложение  

Ключевой фрагмент MPI реализации:  

```cpp
std::vector<int> RemizovKMaxInMatrixStringMPI::FindMaxValues(const int start, const int end) {
  std::vector<int> result;
  for (int i = start; i <= end; i++) {
    if (!GetInput()[i].empty()) {
      int max_val = *std::max_element(GetInput()[i].begin(), GetInput()[i].end());
      result.push_back(max_val);
    }
  }
  return result;
}
```
