# Сумма значений по столбцам матрицы

- Student: Сафронов Максим Александрович, group 3823Б1ПР4
- Technology: SEQ | MPI
- Variant: 21

## 1. Introduction
Задача заключается в сортировке входного массива целых чисел по возрастанию с использованием алгоритма чётно-нечётной сортировки.
Реализованы две версии алгоритма: последовательная (SEQ) и параллельная с использованием MPI.
Также выполнено сравнение их производительности.

## 2. Problem Statement
На вход подаётся вектор размера N.
Требуется отсортировать массив по возрастанию, с помощью пузырьковой сортирвки (алгоритм чет-нечетной перестановки).
В результате должен получиться отсортированный по возрастанию вектор длиной N.

## 3. Baseline Algorithm (Sequential)
На каждой итерации выполняются два прохода по массиву:
1. Сначала сравниваются и при необходимости меняются местами элементы с чётными индексами (0–1, 2–3, …)
2. Затем обрабатываются пары элементов с нечётными индексами (1–2, 3–4, …)
Итерации повторяются до тех пор, пока за полный цикл не произойдёт ни одного обмена, что означает полную отсортированность массива.
Результат сохраняется в выходной вектор.

## 4. Parallelization Scheme
Пусть имеется массив из N элементов и P процессов MPI.
Главный процесс (ранг 0) отвечает за распределение столбцов между процессами (включая себя) и сбор итогового результата.

Распределение данных:
1. Сначала главный процесс (ранг 0) с помощью MPI_Bcast всем процессам рассылает размер массива, а затем — весь исходный массив целиком.
2. Главный процесс рассчитывает диапазоны индексов ([start, end]) для каждого процесса: 
* Вычисляется базовое количество элементов на процесс: whole_part = N / P.
* Вычисляется остаток: real_part = N % P.
* Первые real_part процессов получают whole_part + 1 элементов, остальные — whole_part.
3. Раздача интервалов. 
* Ранг 0 вычисляет интервалы индексов.
* Для себя (ранг 0) он просто сохраняет интервал.
* Для остальных процессов (1..P-1) он отправляет пару чисел [start, end] с помощью MPI_Send.
* Остальные процессы принимают свои интервалы через MPI_Recv.
4. Каждый процесс, зная свой интервал [start, end] и имея копию общего массива, "вырезает" свой локальный кусок данных (own_data) для сортировки.

Алгоритм сортировки:
1. Цикл сортировки: Алгоритм выполняется в цикле N + P - 1 итераций. N итераций необходимо, чтобы элемент мог пройти путь из конца массива в начало. Добавка P - 1 нужна, чтобы гарантированно протолкнуть элементы через все границы между процессами.
2. Внутри цикла происходит чередование фаз (в зависимости от чётности счётчика цикла i):
* А. Чётная фаза (Even Phase, i % 2 == 0)
Локальная сортировка: Вызывается OddEvenBubble с параметром 0. Происходит проход по локальному массиву и обмен элементов на чётных глобальных позициях.
Выбор соседа для обмена:
Обмениваются пары процессов: (0, 1), (2, 3), (4, 5) ... 
Если ранг процесса чётный (0, 2)... и у него есть сосед справа, он инициирует обмен с соседом справа (передает neighbor = 1).
Если ранг процесса нечётный (1, 3, ...), он инициирует обмен с соседом слева (передает neighbor = -1).

* Б. Нечётная фаза (Odd Phase, i % 2 == 1)
Локальная сортировка: Вызывается OddEvenBubble с параметром 1. Происходит проход по локальному массиву и обмен элементов на нечётных глобальных позициях.
Выбор соседа для обмена:
Обмениваются пары процессов: (1, 2), (3, 4), (5, 6)...
Если ранг процесса нечётный (1, 3, ...) и у него есть сосед справа, он инициирует обмен с соседом справа (передает neighbor = 1).
Если ранг процесса чётный (2, 4, ...) и он не 0-й, он инициирует обмен с соседом слева (передает neighbor = -1).

Обмен границами (Data Exchange):
1. Используется MPI_Sendrecv для одновременной отправки и приема данных
2. Если сосед справа (neighbor == 1): Процесс отправляет свой последний элемент (back()).
3. Если сосед слева (neighbor == -1): Процесс отправляет свой первый элемент (front()).
4. Процесс, который находится слева в паре (тот, у кого сосед справа), оставляет себе минимум (std::min) из своего и полученного элемента.
5. Процесс, который находится справа в паре (тот, у кого сосед слева), оставляет себе максимум (std::max).

Сбор результатов: после завершения цикла сортировки каждый процесс имеет отсортированный фрагмент общего массива.
1. Главный процесс помещает свой локальный own_data в начало результирующего вектора GetOutput.
2. Затем он проходит циклом по всем остальным процессам (от 1 до P-1).
3. Он принимает отсортированные части от других процессов с помощью MPI_Recv и последовательно добавляет их в конец вектора GetOutput
4. Остальные процессы отправляют свои отсортированные массивы главному процессу с помощью MPI_Send.

Рассылка итога:
1. После того как Rank 0 собрал полный отсортированный массив, вызывается SendingResult.
2. Сначала с помощью MPI_Bcast рассылается итоговый размер вектора.
3. Затем с помощью MPI_Bcast рассылается сам отсортированный вектор всем остальным процессам, чтобы синхронизировать выходные данные задачи.

## 5. Implementation Details
Основные файлы:
* ops_seq.cpp — последовательная реализация;
* ops_mpi.cpp — параллельная реализация с использованием MPI;
* common.hpp — определения типов данных;

Основные классы: SafronovMBubbleSortOddEvenSEQ и SafronovMBubbleSortOddEvenMPI.

## 6. Experimental Setup
- Hardware/OS: CPU - Intel Core i5-11400F, 6 ядер/12 потоков; RAM - 16 Gb; ОС - Windows 10 
- Toolchain: MinGW-w64 (g++ 7.3.0, x86_64-posix-seh), build type: Release  
- Environment: PPC_NUM_PROC
- Data: тестовые данные задаются вручную.

## 7. Results and Discussion

### 7.1 Correctness
В функциональных тестах (func) проверялась корректность алгоритма в различных ситуациях:

* Когда количество элементов в массиве меньше количества процессов;
* Когда количество элементов в массиве равно количеству процессов;
* Когда количество элементов в массиве больше количества процессов;
* Когда массив элементов пустой;
* Проверка была выполнена на разном количестве процессов.

В производительных тестах (perf) используется массив из 50 000 элементов, инициализированный в обратном порядке — от большего к меньшему.

### 7.2 Performance
Present time, speedup and efficiency. Example table:

Тесты на 2 процессах:
| Count             |Time seq версия (с)| Time mpi версия (с) | Ускорение | Эффективность |
|-------------------|----------------|----------------|-----------|-----------|
| 100               | 0.0000       | 0.0003       | 0.00     | N/A          |
| 1000             | 0.0000       | 0.0035     | 0.00    |  N/A          |
| 10000             | 0.3743       | 0.2387        | 1.57    |  78.3%      |
| 50000          | 13.668     |   7.12    | 1.92    |    96.0%     |
| 100000           | 36.9967        | 22.5798         | 1.64     |   82.0%   |

Тесты на 4 процессах:
| Count             |Time seq версия (с)| Time mpi версия (с) | Ускорение | Эффективность |
|-------------------|----------------|----------------|-----------|-----------|
| 100               | 0.0000       | 0.0007       | 0.00     | N/A          |
| 1000             | 0.0000       | 0.0025      | 0.00    |  N/A          |
| 10000             | 0.3743       | 0.1823        | 2.05    |  51.3%      |
| 50000          | 13.668     |   6.64     | 2.06    |    51.5%     |
| 100000           | 36.9967        | 13.2887         | 2.78     |   69.6%   |

Тесты MPI выполнялись на 4 процессах.
Массив с Count элементами инициализирован в обратном порядке.

Как видно из результатов, на малых размерах массивов (≤ 1000 элементов) последовательная версия выполняется практически мгновенно, в то время как MPI-версия показывает худший результат из-за накладных расходов на инициализацию среды и коммуникацию. Однако, с ростом объема данных (от 10 000 элементов и выше) параллельная версия начинает значительно опережать последовательную. Неполное достижение идеального линейного ускорения (4.0x) объясняется расходами на межпроцессорное взаимодействие: на каждой итерации процессам необходимо обмениваться граничными значениями (MPI_Sendrecv) и синхронизировать фазы сортировки

## 8. Conclusions
В результате были реализованы последовательная (Seq) и параллельная (MPI) версии алгоритма сортировки пузырьком (алгоритм чет-нечетной перестановки). Из полученных результатов видно, что MPI-версия работает быстрее Seq-версии на больших размерах матрицы, тогда как на малых размерах эффективнее последовательный алгоритм.

## 9. References
1. Лекции по параллельному программированию
2. Практики по параллельному программированию