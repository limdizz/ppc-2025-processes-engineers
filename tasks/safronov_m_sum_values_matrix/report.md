# Сумма значений по столбцам матрицы

- Student: Сафронов Максим Александрович, group 3823Б1ПР4
- Technology: SEQ | MPI
- Variant: 12

## 1. Introduction
Задача заключается в вычислении суммы элементов по столбцам матрицы.
Необходимо реализовать две версии решения: последовательную (SEQ) и параллельную (MPI), а также провести сравнение их производительности.

## 2. Problem Statement
На вход подаётся матрица размера N × M, представленная в виде двумерного массива (std::vector<std::vector<double>>), заполненного вещественными числами.
Требуется вычислить сумму элементов каждого столбца.
В результате должен получиться вектор длиной M, где каждый элемент соответствует сумме элементов соответствующего столбца входной матрицы.

## 3. Baseline Algorithm (Sequential)
Алгоритм проходит по каждому столбцу матрицы, суммирует все элементы столбца и добавляет полученную сумму в результирующий вектор.

## 4. Parallelization Scheme
Пусть имеется M столбцов матрицы и k процессов MPI.
Главный процесс (ранг 0) отвечает за распределение столбцов между процессами (включая себя) и сбор итогового результата.

Алгоритм распределения столбцов:
1. Вычисляется целая часть при деления "a = M/k", это количество столбцов приходящихся на каждый процесс.
2. Вычисляется остаток от деления "b = M%k"
3. Каждый процесс получает "a" столбцов, а процессы, у которых rank < b получают дополнительно ещё по одному столбцу
4. Для каждого процесса вычисляется начальный и конечный индексы столбцов, которые он должен обработать.
5. Возвращается вектор из 2 чисел: интервал [start, end], обозначающий диапазон столбцов для данного процесса.

Этапы выполнения программы:
1. Главный процесс с помощью MPI_Bcast рассылает всем процессам параметры входной матрицы (число строк и столбцов), а затем — сами элементы матрицы.
1. Главный процесс отправляет каждому процессу его интервал с помощью MPI_Send. Остальные процессы принимают свой интервал через MPI_Recv.
2. Каждый процесс, включая главный, вычисляет суммы элементов по своим столбцам.
3. Главный процесс добавляет результаты вычисления своих столбцов в результирующий вектор.
4. Все процессы кроме 0-го, отправляют на главный процесс сначала размер вектора с результатами, а затем и сами данные с помощью MPI_Send.
5. Главный процесс принимает результаты при помощью MPI_Recv и добавляет их в итоговый вектор.
6. После объединения всех частичных сумм главный процесс рассылает итоговый результат всем остальным процессам с помощью MPI_Bcast: сначала длину вектора, затем сам вектор.

## 5. Implementation Details
Основные файлы:
* ops_seq.cpp — последовательная реализация;
* ops_mpi.cpp — параллельная реализация с использованием MPI;
* common.hpp — определения типов данных;

Основные классы: SafronovMSumValuesMatrixSEQ и SafronovMSumValuesMatrixMPI.

Особое внимание уделено корректному распределению данных между процессами.

## 6. Experimental Setup
- Hardware/OS: CPU - Intel Core i5-11400F, 6 ядер/12 потоков; RAM - 16 Gb; ОС - Windows 10 
- Toolchain: MinGW-w64 (g++ 7.3.0, x86_64-posix-seh), build type: Release  
- Environment: PPC_NUM_PROC
- Data: тестовые данные задаются вручную.

## 7. Results and Discussion

### 7.1 Correctness
В функциональных тестах (func) проверялась корректность алгоритма в различных ситуациях:

* Когда количество столбцов меньше количества процессов;
* Когда количество процессов равно количеству столбцов;
* Когда количество столбцов больше количества процессов;
* Когда в матрице присутствуют как положительные, так и отрицательные элементы;
* Когда элементы матрицы вещественные или целые;

В производительных тестах (perf) использовалась матрица размером 2000×2000, где каждый элемент равен 2.

### 7.2 Performance
Present time, speedup and efficiency. Example table:

| Count             |Time seq версия (с)| Time mpi версия (с) | Ускорение | Эффективность |
|-------------------|----------------|----------------|-----------|-----------|
| 100               | 0.0000       | 0.0004       | 0.00     | N/A          |
| 10000             | 0.0000       | 0.0005        | 0.00    |  N/A          |
| 1000000             | 0.0005       | 0.0026        | 0.192    |  4.5%      |
| 5000000          | 0.0399      | 0.0142        | 2.81    |    70%     |
| 10000000           | 0.0647        | 0.0157         | 3.42     |   85%   |

Тесты MPI выполнялись на 4 процессах.

Как видно из результатов, для небольших размеров матриц (≤ 1000000 элементов) последовательная версия работает быстрее, а MPI проигрывает. Это связано с тем, что в MPI значительное время тратится на синхронизацию процессов и обмен данными между ними с помощью MPI_Send и MPI_Recv.
При больших размерах матриц MPI версия почти в 4 раза быстрее последовательной, но немного не дотягивает до максимального ускорения (4x). Это также происходит из-за дополнительного времени, которое тратится на передачу данных и синхронизацию между процессами.

## 8. Conclusions
В результате были реализованы последовательная (Seq) и параллельная (MPI) версии алгоритма вычисления суммы элементов по столбцам матрицы. Из полученных результатов видно, что MPI-версия работает быстрее Seq-версии на больших размерах матрицы, тогда как на малых размерах эффективнее последовательный алгоритм.

## 9. References
1. Лекции по параллельному программированию
2. Практики по параллельному программированию