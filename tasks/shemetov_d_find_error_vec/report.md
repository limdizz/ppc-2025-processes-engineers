# Поиск ошибок в порядке элементов вектора с использованием MPI

- Студент: Шеметов Даниил Олегович, группа 3823Б1ПР3  
- Технология: MPI + SEQ  
- Вариант: 6

## 1. Введение

Определение нарушений порядка в числовых последовательностях является важной задачей в анализе данных, проверке корректности вычислений и обработке сигналов. В данной работе рассматривается распараллеливание задачи подсчета «ошибок» (нарушений возрастания) в векторе с использованием стандарта Message Passing Interface (MPI).

## 2. Постановка задачи

**Определение задачи:**  
Дан вектор вещественных чисел `data`. Необходимо вычислить количество пар элементов `(data[i], data[i+1])`, для которых `data[i] > data[i+1]`.

**Формат входных данных:**
- Вектор `data` длины `data_size` (n ≥ 0)
- Элементы типа double

**Формат выходных данных:**
- Целое число — количество нарушений возрастания

**Ограничения:**
- 0 ≤ n ≤ 10^8
- Элементы типа double
- Память: O(n) для хранения вектора

**Пример:**  
Вход: `[1.0, 3.0, 2.0, 5.0, 4.0]`  
Выход: `2` (нарушения между 3.0→2.0 и 5.0→4.0)

## 3. Последовательная реализация (SEQ)

**Алгоритм:**
1. Проверка длины вектора
2. Инициализация счетчика `violations = 0`
3. Проход по вектору:
   ```cpp
    const size_t size = data.size();

    for (size_t i = 0; i + 1 < size; i += 1) {
        if (data[i] > data[i + 1] + kEpsilon) {
            violations += 1;
        }
    }

## 4. Схема распараллеливания

Распараллеливание использует подход **декомпозиции по диапазонам вектора**, при котором каждый MPI-процесс обрабатывает подмассив последовательных элементов:

1. Определяем количество процессов `world_size` и длину вектора `data_size`.
2. Базовое количество элементов на процесс: `base_chunk = data_size / world_size`.
3. Остаток: `remainder = data_size % world_size`.
4. Процессы с рангами `0..remainder-1` получают `base_chunk + 1` элементов.
5. Остальные процессы получают `base_chunk` элементов.

**Обработка граничных элементов:**
Каждый процесс, кроме ранга 0, дополнительно проверяет первый элемент своего диапазона с последним элементом предыдущего процесса, чтобы учесть нарушение на границе.

**Топология:**  
Централизованная («звезда»), процесс с рангом 0 выступает координатором.

### MPI-операции коммуникации

1. **MPI_Comm_size / MPI_Comm_rank** — определение числа процессов и ранга текущего процесса.
2. **MPI_Allreduce** — суммирование локальных счетчиков нарушений по всем процессам, результат сохраняется на каждом процессе.

## 5. Детали реализации

### 5.1 Структура кода

**Файлы:**
- `common/include/common.hpp` — определение типов данных `InType`, `OutType`
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` — последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` — параллельная реализация
- `tests/functional/main.cpp` — функциональные тесты
- `tests/performance/main.cpp` — тесты производительности

**Классы:**
- `ShemetovDFindErrorVecSEQ` — последовательная реализация
- `ShemetovDFindErrorVecMPI` — параллельная реализация

**Методы (для обеих реализаций):**
- `ValidationImpl()` — проверка корректности входных данных
- `PreProcessingImpl()` — подготовка данных
- `RunImpl()` — основной алгоритм подсчета нарушений
- `PostProcessingImpl()` — проверка корректности выхода

### 5.2 Важные решения при реализации

**Управление памятью:**
- Вектор хранится полностью только на процессе 0.
- Каждый процесс обрабатывает только назначенный диапазон элементов.
- Локальные переменные освобождаются после использования для снижения пикового потребления памяти.

**Граничные случаи:**
- Один процесс (`world_size = 1`) → выполняется как последовательный алгоритм.
- Пустой вектор → возвращается 0.
- Число процессов больше длины вектора → некоторые процессы получают 0 элементов.

**Оптимизации:**
- Использование локального счетчика нарушений для каждого процесса.
- Добавление проверки границы между процессами.
- Раннее освобождение локальных буферов.
## 6. Экспериментальное окружение

### 6.1 Конфигурация оборудования

- **CPU:** AMD Ryzen 9 7940HS w/ Radeon 780M Graphics  
- **Ядра:** 8 физических ядер (16 логических потоков)  
- **ОЗУ:** 8 ГБ DDR4  
- **ОС:** WSL Ubuntu 24.04.3 LTS (Linux kernel 6.x)  

### 6.2 Программный инструментарий

- **Компилятор:** g++ 11.4.0  
- **Тип сборки:** Release  
- **Стандарт C++:** C++20  
- **Библиотеки:** MPI (OpenMPI 4.x)

### 6.3 Тестовое окружение

Тестирование параллельной версии проводится с переменным числом процессов:

PPC_NUM_PROC=1,2,4,8,16

### 6.4 Генерация тестовых данных

- **Функциональные тесты:** набор векторов разного размера:
  - пустой вектор
  - один элемент
  - возрастающая последовательность
  - убывающая последовательность
  - смешанный порядок
  - вектор с повторяющимися значениями
  - векторы с проверкой точности чисел с плавающей запятой
- **Тесты производительности:** большие векторы:
  - 90 000 000 элементов  
  - элементы с небольшим локальным изменением для создания нарушений

### 6.5 Настройка параллелизма

- **MPI-процессы:** переменное число процессов задается через переменную окружения `PPC_NUM_PROC`  
- **Распределение данных:**  
  - Вектор делится на равные части между процессами  
  - Процессы с меньшими рангами получают на 1 элемент больше, если размер не делится ровно  
- **Коммуникации MPI:**  
  - `MPI_Bcast` — рассылка размеров вектора всем процессам  
  - `MPI_Allreduce` — суммирование локальных нарушений между процессами  

## 7. Результаты

### 7.1 Проверка корректности

**Функциональные тесты:**  
- Векторы разного размера и структуры (пустые, один элемент, возрастающие, убывающие, смешанные, с дублирующимися значениями, проверка точности)  
- Результаты MPI идентичны SEQ  
- Пропусков и ошибок не обнаружено

### 7.2 Результаты производительности

**Пример задачи:**  
- Вектор 90 000 000 элементов

| Mode       | Processes | Time, s | Speedup | Efficiency |
|------------|-----------|---------|---------|------------|
| SEQ        | 1         | 3.85    | 1.00    | N/A        |
| MPI task   | 4         | 1.22    | 3.16    | 79%        |
| MPI task   | 8         | 0.68    | 5.66    | 71%        |
| MPI task   | 16        | 0.40    | 9.63    | 60%        |

## 8. Выводы

### 8.1 Что сработало хорошо

**Корректная реализация параллельного алгоритма**
- Все функциональные тесты пройдены успешно
- Результаты MPI идентичны последовательной версии (SEQ) независимо от числа процессов
- Правильная обработка граничных случаев (остаток при делении элементов вектора между процессами)

**Масштабируемость**
- Достигнуто ускорение ~9.6x на 16 процессах для вектора 90 000 000 элементов
- Линейное распределение данных обеспечивает балансировку нагрузки
- Коллективные операции MPI эффективно используют топологию сети

**Оптимизация памяти**
- Локальные буферы освобождаются сразу после обработки
- Память на процессах масштабируется как O(n/p), где n — размер вектора, p — число процессов

### 8.2 Ограничения и проблемы

**Эффективность для малых размеров**
- На небольших векторах MPI накладные расходы превышают выигрыш
- При малом числе процессов ускорение минимально

**Низкая эффективность при большом числе процессов**
- Для 16 процессов эффективность ~60% из-за коммуникационных накладных расходов
- Коммуникация начинает доминировать над вычислениями при сильном распараллеливании

**Численная точность**
- Используется порог kEpsilon = 1e-10 для сравнения double
- При очень маленьких различиях могут возникать погрешности

## 9. Источники

1. Лекции Сысоева А.В.
2. Материалы курса `ppc-2025-processes-engineers`:  
   [GitHub Repository](https://github.com/learning-process/ppc-2025-processes-engineers)

## 10. Приложение

### 10.1 MPI-реализация алгоритма подсчёта нарушений

```cpp
bool ShemetovDFindErrorVecMPI::RunImpl() {
  const auto &data = GetInput();
  const int data_size = static_cast<int>(data.size());

  if (data_size < 2) {
    GetOutput() = 0;
    return true;
  }

  int world_rank = 0;
  int world_size = 1;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

  std::vector<int> sendcounts(world_size);
  std::vector<int> displs(world_size);

  int base = data_size / world_size;
  int extra = data_size % world_size;

  for (int rank_idx = 0; rank_idx < world_size; rank_idx++) {
    sendcounts[rank_idx] = base + (rank_idx < extra ? 1 : 0);
    displs[rank_idx] = (rank_idx * base) + std::min(rank_idx, extra);
  }

  int local_size = sendcounts[world_rank];
  std::vector<double> local_data(local_size);

  MPI_Scatterv(data.data(), sendcounts.data(), displs.data(), MPI_DOUBLE, local_data.data(), local_size, MPI_DOUBLE, 0,
               MPI_COMM_WORLD);

  int local_viol = 0;

  for (int i = 0; i + 1 < local_size; i++) {
    local_viol += DetectDrop(local_data[i], local_data[i + 1]);
  }

  if (world_rank > 0 && local_size > 0 && displs[world_rank] > 0) {
    double left = data[displs[world_rank] - 1];
    double right = local_data[0];
    local_viol += DetectDrop(left, right);
  }

  int global_viol = 0;
  MPI_Allreduce(&local_viol, &global_viol, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

  GetOutput() = global_viol;
  return true;
}

bool ShemetovDFindErrorVecMPI::PostProcessingImpl() {
    return true;
}