# Подсчёт числа несовпадающих символов двух строк
- Студент: Соснина Александра Антоновна, группа 3823Б1ПР1
- Технология: SEQ | MPI
- Вариант: 27


## 1. Введение

   Задача подсчёта различий между строками имеет практическое применение в исправлении опечаток пользователей, машинном переводе и системах контроля версий. При обработке очень длинных строк последовательные алгоритмы становятся узким местом. 
   
   Ожидаемым результатом моей работы является реализация и сравнение последовательного и MPI-параллельного алгоритмов для подсчёта несовпадающих символов с учётом разницы в длине строк.

---

## 2. Постановка задачи

**Цель работы:**  
Реализовать последовательную и параллельную версии алгоритма подсчёта количества несовпадающих символов в двух строках, а также провести сравнение их эффективности.

**Определение задачи:**  
Для двух заданных строк `s1` и `s2` необходимо вычислить количество позиций, в которых символы различаются. Формально:  
`diff(s1, s2) = count(i from 0 to min(len(s1), len(s2))-1 where s1[i] != s2[i]) + abs(len(s1) - len(s2))`

**Ограничения:**
- Входные данные - это две строки произвольной длины (включая пустые строки)
- Корректность должна сохраняться при строках разной длины
- Должен учитываться регистр символов ('A' ≠ 'a')
- Для параллельной реализации используется MPI
- Результат обеих реализаций (последовательной и параллельной) должен совпадать

---

## 3. Базовый алгоритм

### Алгоритм последовательной реализации

1. Инициализация
- Получить на вход две строки: `str1` и `str2`
- Инициализировать счетчик различий: `diff_counter = 0`

2. Определение длин строк
- Вычислить длину первой строки: `len1 = str1.length()`
- Вычислить длину второй строки: `len2 = str2.length()`
- Найти минимальную длину: `min_len = min(len1, len2)`
- Найти максимальную длину: `max_len = max(len1, len2)`

3. Сравнение символов в общей части  

   Для каждого индекса `i` от `0` до `min_len - 1`:
- Получить символ из первой строки: `char1 = str1[i]`
- Получить символ из второй строки: `char2 = str2[i]`
- Если `char1 ≠ char2`, то увеличить счетчик: `diff_counter = diff_counter + 1`

4. Учет разницы в длинах
- Вычислить количество дополнительных различий: `extra_diff = max_len - min_len`
- Добавить к общему счетчику: `diff_counter = diff_counter + extra_diff`

5. Возврат результата
- Вернуть значение `diff_counter`


### Код последовательной реализации:

```cpp
bool SosninaADiffCountSEQ::RunImpl() {
  const std::string &str1 = input_.first;
  const std::string &str2 = input_.second;

  std::size_t min_len = std::min(str1.size(), str2.size());
  std::size_t max_len = std::max(str1.size(), str2.size());
  diff_counter_ = 0;

  // Сравниваем символы в общей части строк
  for (std::size_t i = 0; i < min_len; i++) {
    if (str1[i] != str2[i]) {
      diff_counter_++;
    }
  }

  // Добавляем разницу в длинах
  diff_counter_ += static_cast<int>(max_len - min_len);
  
  return true;
}
```

---

## 4. Схема распараллеливания

### Распределение данных

Для параллельной обработки используется блочное распределение данных с балансировкой нагрузки. Область обработки охватывает весь диапазон `[0, max(len(str1), len(str2))]`.

**Инициализация данных:**
- Только процесс 0 изначально получает входные строки в конструкторе
- В фазе PreProcessing процесс 0 рассылает данные всем процессам:
  1. Рассылка длин строк через `MPI_Bcast`
  2. Рассылка содержимого строк через `MPI_Bcast`
- После PreProcessing все процессы имеют полные копии строк для обработки

**Распределение работы:**
- Общий диапазон: `total_len = max(str1_len, str2_len)`
- Базовая длина блока: `chunk_size = total_len / num_processes`
- Остаток: `remainder = total_len % num_processes`

Распределение выполняется с учетом остатка для равномерной нагрузки:
- Процессы с рангом `i < remainder` получают на 1 элемент больше
- Каждый процесс обрабатывает диапазон `[start, end)`

### Схема связи и топология

Используется звездообразная топология с процессом 0 в качестве центрального координатора:

**Нисходящие связи:**
- От процесса 0 к worker-процессам (рассылка данных через `MPI_Bcast`)
- От процесса 0 к worker-процессам (рассылка финального результата через `MPI_Bcast`)

**Восходящие связи:**
- От worker-процессов к процессу 0 (передача частичных результатов через `MPI_Reduce`)

Схема коммуникации представляет собой четырехэтапный процесс:
1. **Фаза распространения данных**: Процесс 0 рассылает строки всем процессам через `MPI_Bcast`
2. **Фаза сбора и редукции**: Worker-процессы отправляют свои локальные счетчики процессу 0 через `MPI_Reduce`
3. **Фаза синхронизации результата**: Процесс 0 рассылает финальный результат всем процессам через `MPI_Bcast`
4. **Фаза сохранения**: Все процессы сохраняют результат в `GetOutput()`

### Ранжирование ролей

**Процесс 0 (Master-координатор):**
- Получает исходные данные в конструкторе
- Рассылает данные всем процессам в PreProcessing через `MPI_Bcast`
- Выполняет локальную обработку своей части данных
- Принимает и суммирует все частичные результаты через `MPI_Reduce`
- Рассылает финальный результат всем процессам через `MPI_Bcast`
- Сохраняет конечный результат в `GetOutput()`

**Процессы 1..N-1 (Worker-процессы):**
- Получают данные от процесса 0 в PreProcessing через `MPI_Bcast`
- Обрабатывают назначенные блоки данных
- Отправляют свои локальные счетчики процессу 0 через `MPI_Reduce`
- Получают финальный результат от процесса 0 через `MPI_Bcast`
- Сохраняют результат в `GetOutput()`

### Декомпозиция

**По данным:** 
- Диапазон `[0, total_len)` делится на непрерывные блоки
- Каждый процесс работает только со своим сегментом данных

**По функциям:**
1. Распространение данных (процесс 0 → все процессы через `MPI_Bcast`)
2. Локальный подсчет (все процессы)
3. Сбор и суммирование результатов (все процессы → процесс 0 через `MPI_Reduce`)
4. Синхронизация результата (процесс 0 → все процессы через `MPI_Bcast`)
5. Сохранение результата (все процессы)

### Планирование

1. **Инициализация данных**: только процесс 0 получает исходные строки
2. **Распространение данных**: процесс 0 рассылает строки всем процессам через `MPI_Bcast`
3. **Распределение работы**: расчет границ блоков для каждого процесса
4. **Локальная обработка**: подсчет различий в назначенном сегменте
5. **Сбор результатов**: передача и суммирование счетчиков через `MPI_Reduce`
6. **Синхронизация результата**: процесс 0 рассылает финальный счетчик всем процессам через `MPI_Bcast`
7. **Сохранение результата**: все процессы записывают итог в `GetOutput()`


### Псевдокод

```
function PreProcessingImpl():
    rank, size = MPI_comm_info()
    
    if rank == 0:
        lengths = [str1_len, str2_len]
    
    MPI_Bcast(lengths)           // Рассылка длин строк
    
    if rank != 0:
        allocate_strings(lengths) // Выделение памяти под строки
    
    MPI_Bcast(str1_data)         // Рассылка содержимого str1
    MPI_Bcast(str2_data)         // Рассылка содержимого str2

function RunImpl():
    rank, size = MPI_comm_info()
    total_len = max(str1_len, str2_len)
    
    if total_len == 0:
        diff_counter_ = 0
        return
    
    // Распределение работы с балансировкой
    chunk_size = total_len / size
    remainder = total_len % size
    
    start = (rank * chunk_size) + min(rank, remainder)
    end = start + chunk_size
    if rank < remainder:
        end += 1
    
    // Локальные вычисления
    local_count = 0
    for i from start to end-1:
        if i >= str1_len or i >= str2_len or str1[i] != str2[i]:
            local_count += 1
    
    // Сбор результатов через Reduce и синхронизация
    MPI_Reduce(local_count, diff_counter_, MPI_SUM, 0)
    MPI_Bcast(diff_counter_, 1, MPI_INT, 0)
    GetOutput() = diff_counter_

function PostProcessingImpl():
    return true
```

---
## 5. Детали реализации

### Структура кода

**Файловая структура:**

sosnina_a_diff_count/  
├── common/include/common.hpp  
├── seq/include/ops_seq.hpp  
├── seq/src/ops_seq.cpp  
├── mpi/include/ops_mpi.hpp  
├── mpi/src/ops_mpi.cpp  
├── tests/functional/main.cpp  
├── tests/performance/main.cpp  
└── data/  


**Ключевые классы и файлы:**

1. Последовательная реализация (`seq`):
   - `ops_seq.hpp` - объявление класса `SosninaADiffCountSEQ`
   - `ops_seq.cpp` - реализация методов:
     - `RunImpl()` - основной алгоритм сравнения строк
     - `PreProcessingImpl()` - инициализация счетчика
     - `PostProcessingImpl()` - сохранение результата

2. MPI реализация (`mpi`):
   - `ops_mpi.hpp` - объявление класса `SosninaADiffCountMPI`
   - `ops_mpi.cpp` - реализация методов:
     - `SosninaADiffCountMPI()` - конструктор с получением данных
     - `ValidationImpl()` - проверка инициализации MPI
     - `PreProcessingImpl()` - распространение данных между процессами
     - `RunImpl()` - параллельные вычисления и сбор результатов
     - `PostProcessingImpl()` - финализация

3. Общие компоненты (`common`):
   - `common.hpp` - общие типы данных и константы

**Архитектурные особенности:**
- Разделение интерфейса (.hpp) и реализации (.cpp)
- Единый стиль именования для обеих реализаций
- Изолированные тестовые модули для функциональности и производительности
- Использование MPI для межпроцессного взаимодействия
- Блочное распределение данных с балансировкой нагрузки
- Централизованная модель с процессом-координатором (rank 0)
- Эффективное использование MPI_Bcast для распространения данных
- Применение MPI_Reduce для сбора и суммирования результатов
- Обработка граничных случаев (пустые строки, разная длина)
- Минимизация коммуникационных операций

### Важные допущения и ограничения

**Обработка данных:**
- Поддержка строк произвольной длины (ограничено `size_t`)
- Учет регистра символов: 'A' ≠ 'a'
- Корректная обработка специальных символов и UTF-8

**Граничные случаи:**
```cpp
diff("", "") = 0 // Пустые строки
diff("hello", "help") = 2 // Разная длина
diff("cat", "cat") = 0 // Полное совпадение
diff("cat", "dog") = 3 // Полное различие
```

### Рекомендации по использованию памяти
- Для строк длиной до 1 млн символов использовать последовательную версию
- Для больших объемов данных (>10 млн символов) применять MPI версию
- Контролировать общий объем памяти
---
## 6. Экспериментальная установка

### Аппаратное обеспечение и ОС

Системные характеристики:
- Модель процессора: Apple M2 Chip (8-core CPU)
- Архитектура: ARM64
- Ядра/потоки: 4 производительных + 4 энергоэффективных ядра
- Оперативная память: 16 GB 
- Операционная система: macOS Sonoma 14.x
- Тип системы: Ноутбук (MacBook Air)

### Набор инструментов

Компиляция и сборка:
- Компилятор: GCC 11.4.0 (через Homebrew)
- Стандарт языка: C++17
- Среда разработки: Visual Studio Code
- Тип сборки: Release
- Система сборки: CMake

### Управление процессами

PPC_NUM_PROC: устанавливается через параметр -n в mpirun

```cpp
//Запуск с различным количеством процессов MPI
mpirun -n 1 ./ppc_perf(func)_tests --gtest_filter="*SosninaADiffCount*"
mpirun -n 2 ./ppc_perf(func)_tests --gtest_filter="*SosninaADiffCount*"
mpirun -n 4 ./ppc_perf(func)_tests --gtest_filter="*SosninaADiffCount*"
mpirun -n 8 ./ppc_perf(func)_tests --gtest_filter="*SosninaADiffCount*"
```

## 7. Результаты и обсуждение

### 7.1 Корректность

**Методы проверки корректности:**

1. Комплексное модульное тестирование:
   - 22 функциональных теста - проверка базовых сценариев
   - 10 тестов покрытия - обработка граничных случаев

2. Тестирование производительности:
   - 2 теста производительности с измерением времени выполнения
   - Тестирование на данных объемом 300 миллионов символов
   - Сравнение времени выполнения SEQ и MPI версий

**Ключевые тестовые сценарии:**
```cpp
"baby_baby" → 0 различий //Полное совпадение
"happy_heppy" → 1 различие //Частичное совпадение  
"abc_defgh" → 5 различий //Разная длина
"__" → 0 различий //Пустые строки
"prizet_PRIZET" → 6 различий //Регистр символов
"54321_09876" → 5 различий //Числовые строки
```
**Методология проверки:**
- Каждый тест выполняется для обеих реализаций (SEQ и MPI)
- Результаты сравниваются с эталонным значением
- Проверяется идентичность результатов между SEQ и MPI версиями
- Используется фреймворк (Google Test) для автоматизированной проверки

**Результаты проверки корректности:**
- Все 22 функциональных теста пройдены успешно
- 2 теста производительности подтвердили работоспособность на больших данных
- Результаты SEQ и MPI реализаций полностью совпадают
- Эталонная функция подтверждает правильность вычислений
- Обработка всех граничных случаев корректна

### 7.2 Производительность

Результаты измерения производительности для строк длиной 200 миллионов символов:

### Время выполнения (task_run) - чистые вычисления

| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 0.4136   | 1.00      | 100%          |
| mpi   | 2        | 0.1287   | 3.21      | 160.5%        |
| mpi   | 3        | 0.1124   | 3.68      | 122.7%        |
| mpi   | 4        | 0.0709   | 5.83      | 145.8%        |

### Время выполнения (pipeline) - полный цикл

| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 0.4134   | 1.00      | 100%          |
| mpi   | 2        | 0.1636   | 2.53      | 126.5%        |
| mpi   | 3        | 0.1829   | 2.26      | 75.3%         |
| mpi   | 4        | 0.1317   | 3.14      | 78.5%         |


**Анализ результатов:**

1. **Результаты Task_Run (чистые вычисления):**
   - Ускорение растет с увеличением процессов: 3.21×, 3.68×, 5.83×
   - Эффективность: 160.5%, 122.7%, 145.8%
   - На 4 процессах достигнуто 5.83-кратное ускорение

2. **Результаты Pipeline (полный цикл):**
   - Ускорение: 1.59×, 2.53×, 2.26×, 3.14×
   - Эффективность: 159%, 126.5%, 75.3%, 78.5%
   - Наилучший результат на 4 процессах (3.14× ускорение)

**Ключевые выводы:**
- MPI версия демонстрирует сверхлинейное ускорение на всех конфигурациях для task_run
- Наивысшая эффективность достигается на 2 процессах для task_run (160.5%)
- Оптимальная конфигурация: 4 процесса для максимального ускорения, 2 процесса для наилучшей эффективности
- Pipeline режим показывает снижение эффективности при 3+ процессах из-за коммуникационных накладных расходов

---
## 8. Выводы

### Достижения

1. Корректность реализации:
   - Обе версии (SEQ и MPI) прошли все 22 функциональных теста 
   - Результаты полностью совпадают с эталонными значениями
   - Обеспечена корректная обработка граничных случаев

2. Высокая производительность:
   - Достигнуто сверхлинейное ускорение до 5.83× на 4 процессах
   - Эффективность вычислений превышает 100% 
   - MPI версия значительно превосходит последовательную реализацию

3. Эффективное распараллеливание:
   - Алгоритм хорошо масштабируется с ростом числа процессов
   - Оптимальная производительность достигается на 4 процессах
   - Схема распределения данных обеспечивает балансировку нагрузки

### Ограничения и проблемы

1. Коммуникационные накладные расходы:
   - Полный цикл (pipeline) показывает снижение эффективности при 4 процессах

2. Ограничения масштабируемости:
   - Эффективность может снижаться при очень большом числе процессов

3. Требования к памяти:
   - Для очень больших данных требуется значительный объем RAM

---

## 9. Список литературы

1. Антонов А. С. Параллельное программирование с использованием технологии MPI. — М.: Изд-во МГУ, 2010.  
2. Корнеев В. Д. Параллельное программирование в MPI. — М.: Изд-во МГУ, 2002.  
3. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.
4. MPI Forum. MPI: A Message-Passing Interface Standard, Version 4.0. 2021. https://www.mpi-forum.org/docs/
5. Microsoft. Справочник по MPI. — 2024. https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-reference
