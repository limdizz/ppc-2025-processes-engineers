# Ленточная горизонтальная схема - разбиение только матрицы А - умножение матрицы на матрицу
- Студент: Соснина Александра Антоновна, группа 3823Б1ПР1
- Технология: SEQ | MPI
- Вариант: 13


## 1. Введение

   Умножение матриц — ключевая операция в научных вычислениях, компьютерной графике и машинном обучении. Для больших матриц классический алгоритм с кубической сложностью требует значительных вычислительных ресурсов.

    Ожидаемый результат: создание корректного параллельного решения, демонстрирующего ускорение за счёт распределения вычислительной нагрузки и оптимизации коммуникаций.

---

## 2. Постановка задачи

**Цель работы:**  
Реализовать и сравнить производительность последовательного (SEQ) и параллельного (MPI) алгоритмов умножения матриц, основанного на ленточной горизонтальной схеме, где только матрица A распределяется по строкам между процессами, а полная матрица B рассылается всем процессам.

**Определение задачи:**  
Для двух заданных матриц A (размером M×K) и B (размером K×N) необходимо вычислить матрицу C = A × B (размером M×N), где каждый элемент вычисляется как:
C[i][j] = Σ (A[i][k] * B[k][j]) для k = 0..K-1

**Ограничения:**
- Входные данные - две совместимые матрицы произвольных размеров (столбцы A = строки B)
- Для параллельной реализации используется MPI с ленточной горизонтальной схемой
- Только матрица A распределяется по строкам между процессами
- Матрица B полностью рассылается всем процессам
- Результат обеих реализаций (последовательной и параллельной) должен совпадать

---

## 3. Базовый алгоритм

### Алгоритм последовательной реализации

1. Инициализация
- Получить на вход две матрицы: matrix_A (M×K) и matrix_B (K×N)
- Инициализировать результирующую матрицу result_matrix размером M×N нулями

2. Умножение матриц
- Для каждой строки i от 0 до M-1:
- Для каждого столбца j от 0 до N-1:
    - Вычислить скалярное произведение: sum = 0
    - Для каждого k от 0 до K-1:
        - sum = sum + matrix_A[i][k] * matrix_B[k][j]
    - result_matrix[i][j] = sum

3. Возврат результата
- Вернуть матрицу result_matrix


### Код последовательной реализации:

```cpp
bool SosninaAMatrixMultHorizontalSEQ::RunImpl() {
  const auto &matrix_a = input_.first;
  const auto &matrix_b = input_.second;

  size_t rows_a = matrix_a.size();
  size_t cols_a = matrix_a[0].size();
  size_t cols_b = matrix_b[0].size();

  auto &output = GetOutput();
  output = std::vector<std::vector<double>>(rows_a, std::vector<double>(cols_b, 0.0));

  // Умножение матриц 
  for (size_t i = 0; i < rows_a; i++) {
    for (size_t k = 0; k < cols_a; k++) {
      double aik = matrix_a[i][k];
      for (size_t j = 0; j < cols_b; j++) {
        output[i][j] += aik * matrix_b[k][j];
      }
    }
  }

  return true;
}

bool SosninaAMatrixMultHorizontalSEQ::PostProcessingImpl() {
  return true;
}
```
---

## 4. Схема распараллеливания

### 4.1. Распределение данных

Для параллельной обработки используется ленточная горизонтальная схема распределения данных:
- Матрица A распределяется по строкам между всеми процессами
- Матрица B полностью дублируется на всех процессах
- Каждый процесс работает только со своей частью матрицы A, но имеет полную копию матрицы B

**Инициализация данных:**
- Только процесс с рангом 0 изначально получает входные матрицы в конструкторе
- В фазе PreProcessing процесс 0 рассылает данные всем процессам

**Распределение строк матрицы A:**
- Общее количество строк: `rowsA`
- Количество процессов: `world_size_`
- Распределение выполняется по принципу: строка с индексом `i` обрабатывается процессом с рангом `i % world_size_`
- Каждый процесс получает примерно равное количество строк

### 4.2. Схема связи и топология

Используется звездообразная топология с процессом 0 в качестве центрального координатора:

**Нисходящие связи (от процесса 0 к worker-процессам):**
- Рассылка размеров матриц (`MPI_Bcast`)
- Рассылка матрицы B (`MPI_Bcast`)
- Распределение строк матрицы A (`MPI_Send`)

**Восходящие связи (от worker-процессов к процессу 0):**
- Передача частичных результатов умножения (`MPI_Send`)

### 4.3. Распределение данных и коммуникации

#### Исходное состояние
Только процесс с рангом 0 владеет исходными матрицами `A` и `B`, хранящимися в виде двумерных векторов `matrix_A_` и `matrix_B_`.

**Фаза 1: Рассылка метаданных**  
Процесс 0 передает размеры матриц всем процессам через широковещательную рассылку `MPI_Bcast`.

**Фаза 2: Распространение матрицы B**  
Процесс 0 преобразует матрицу B в линейный массив и рассылает его всем процессам за одну операцию `MPI_Bcast`.

**Фаза 3: Распределение строк матрицы A**  
Каждый процесс определяет свои строки для обработки по принципу `i % world_size`. Процесс 0 отправляет данные строк процессам-получателям.

**Фаза 4: Локальное умножение**  
Каждый процесс вычисляет произведение своей части матрицы A на полную матрицу B.

**Фаза 5: Сбор результатов**  
Worker-процессы отправляют свои части результата процессу 0, который собирает полную матрицу.

**Фаза 6: Синхронизация результата**  
Процесс 0 рассылает итоговую матрицу всем процессам через `MPI_Bcast`.

**Фаза 7: Сохранение результата**  
Все процессы сохраняют полученную матрицу в выходную структуру.

### Планирование

### 4.4. Ранжирование ролей и планирование

### Ранжирование ролей

### Процесс 0 (Master-координатор):

1. **Инициализация данных**:
   - Получает исходные матрицы A и B в конструкторе

2. **Распространение метаданных**:
   - Рассылает размеры матриц всем процессам через `MPI_Bcast` в `PrepareAndValidateSizes()`

3. **Распространение матрицы B**:
   - Подготавливает и рассылает матрицу B всем процессам через `MPI_Bcast` в `PrepareAndBroadcastMatrixB()`

4. **Распределение данных**:
   - Определяет распределение строк матрицы A между процессами (блочно-циклическое распределение по строкам)
   - Рассылает соответствующие строки матрицы A каждому процессу через `MPI_Send` в `DistributeMatrixAData()`

5. **Локальная обработка**:
   - Выполняет локальное умножение своих строк матрицы A на матрицу B в `ComputeLocalMultiplication()`

6. **Сбор результатов**:
   - Собирает частичные результаты от всех процессов через `MPI_Recv` в `GatherResults()`

7. **Финализация**:
   - Рассылает финальную результирующую матрицу всем процессам через `MPI_Bcast`
   - Сохраняет конечный результат в `GetOutput()`

---

### Процессы 1..N-1 (Worker-процессы):

1. **Получение метаданных**:
   - Получают размеры матриц от процесса 0 через `MPI_Bcast` в `PrepareAndValidateSizes()`

2. **Получение матрицы B**:
   - Получают матрицу B от процесса 0 через `MPI_Bcast` в `PrepareAndBroadcastMatrixB()`

3. **Получение данных**:
   - Получают назначенные строки матрицы A от процесса 0 через `MPI_Recv` в `DistributeMatrixAData()`

4. **Локальная обработка**:
   - Выполняют локальное умножение полученных строк матрицы A на матрицу B в `ComputeLocalMultiplication()`

5. **Отправка результатов**:
   - Отправляют свои частичные результаты процессу 0 через `MPI_Send` в `GatherResults()`

6. **Получение финального результата**:
   - Получают финальную результирующую матрицу от процесса 0 через `MPI_Bcast`
   - Сохраняют результат в `GetOutput()`


### 4.5. Декомпозиция задачи

**Декомпозиция по данным:**
- Матрица A делится на горизонтальные блоки (строки)
- Каждый процесс обрабатывает свой блок строк матрицы A
- Матрица B полностью реплицируется на всех процессах

**Декомпозиция по функциям:**
1. **Распространение метаданных** — процесс 0 → все процессы (`MPI_Bcast`)
2. **Распространение матрицы B** — процесс 0 → все процессы (`MPI_Bcast`)
3. **Распределение матрицы A** — процесс 0 → worker-процессы (`MPI_Send/MPI_Recv`)
4. **Локальное умножение** — все процессы независимо
5. **Сбор результатов** — worker-процессы → процесс 0 (`MPI_Send/MPI_Recv`)
6. **Синхронизация результата** — процесс 0 → все процессы (`MPI_Bcast`)
7. **Сохранение результата** — все процессы в `GetOutput()`

### 4.6. Планирование выполнения

1. **Инициализация данных**: только процесс 0 получает исходные матрицы
2. **Распространение метаданных**: процесс 0 рассылает размеры матриц всем процессам
3. **Распространение матрицы B**: процесс 0 рассылает полную матрицу B
4. **Распределение матрицы A**: процесс 0 отправляет блоки строк worker-процессам
5. **Локальная обработка**: каждый процесс умножает свою часть A на B
6. **Сбор результатов**: worker-процессы отправляют результаты процессу 0
7. **Сбор и синхронизация**: процесс 0 собирает все части и рассылает полную матрицу
8. **Сохранение результата**: все процессы сохраняют итог в `GetOutput()`

### Псевдокод

```
function PreProcessingImpl():
    rank, size = MPI_comm_info()
    
    if rank == 0:
        rows_a, cols_a, rows_b, cols_b = get_matrix_sizes()
    
    MPI_Bcast([rows_a, cols_a, rows_b, cols_b]) // Рассылка размеров матриц
    
    if rank != 0:
        allocate_matrix_b_flat(rows_b * cols_b)  // Выделение памяти под матрицу B
    
    MPI_Bcast(matrix_b_flat)                     // Рассылка содержимого матрицы B

function RunImpl():
    rank, size = MPI_comm_info()
    
    if size == 1:
        return RunSequential()
    
    // Получение локальных строк матрицы A
    local_rows, my_row_indices = распределение_работы(rows_a, rank, size)
    
    if rank == 0:
        // Процесс 0 собирает данные для всех процессов
        for dest от 1 до size-1:
            dest_rows = get_rows_for_process(dest, rows_a)
            send_rows_data(dest, dest_rows, cols_a)
        
        // Заполняем свои локальные данные
        local_a_flat = get_my_rows_data(my_row_indices, cols_a)
    else:
        // Получаем данные от процесса 0
        receive_rows_from_root(local_rows, my_row_indices, local_a_flat, cols_a)
    
    // Локальное умножение
    local_result = new double[local_rows * cols_b]
    для i от 0 до local_rows-1:
        для k от 0 до cols_a-1:
            aik = local_a_flat[i*cols_a + k]
            для j от 0 до cols_b-1:
                local_result[i*cols_b + j] += aik * matrix_b_flat[k*cols_b + j]
    
    // Сбор и распределение результатов
    if rank == 0:
        final_result = new double[rows_a * cols_b]
        собрать_свои_результаты(my_row_indices, local_result, final_result)
        
        для src от 1 до size-1:
            получить_результаты_от_процесса(src, final_result)
        
        MPI_Bcast(final_result) // Рассылаем полный результат всем
    else:
        отправить_результаты(local_result, local_rows, cols_b)
        получить_финальный_результат(final_result)
    
    // Конвертация в матричный формат (только для rank 0)
    if rank == 0:
        result_matrix = конвертировать_в_матрицу(final_result, rows_a, cols_b)
        GetOutput() = result_matrix

function RunSequential():
    если rank != 0:
        return true
    
    result_matrix = new double[rows_a][cols_b]
    для i от 0 до rows_a-1:
        для k от 0 до cols_a-1:
            aik = matrix_A[i][k]
            для j от 0 до cols_b-1:
                result_matrix[i][j] += aik * matrix_B[k][j]
    
    GetOutput() = result_matrix

function PostProcessingImpl():
    return true

// Вспомогательные функции распределения
function распределение_работы(total_rows, rank, size):
    base = total_rows / size
    remainder = total_rows % size
    
    local_rows = base
    если rank < remainder:
        local_rows = local_rows + 1
    
    my_row_indices = []
    для i от 0 до total_rows-1:
        если i % size == rank:
            добавить i в my_row_indices
    
    return local_rows, my_row_indices

function get_rows_for_process(process_rank, total_rows):
    rows = []
    для i от 0 до total_rows-1:
        если i % size == process_rank:
            добавить i в rows
    return rows
```
---
## 5. Детали реализации

### Структура кода

**Файловая структура:**

sosnina_a_matrix_mult_horizontal/  
├── common/include/common.hpp  
├── seq/include/ops_seq.hpp  
├── seq/src/ops_seq.cpp  
├── mpi/include/ops_mpi.hpp  
├── mpi/src/ops_mpi.cpp  
├── tests/functional/main.cpp  
├── tests/performance/main.cpp  
└── data/  


**Ключевые классы и файлы:**

1. Последовательная реализация (`seq`):
   - `ops_seq.hpp` - объявление класса `SosninaAMatrixMultHorizontalSEQ`
   - `ops_seq.cpp` - реализация методов:
     - `RunImpl()` - основной алгоритм сравнения строк
     - `PreProcessingImpl()` - инициализация счетчика
     - `PostProcessingImpl()` - финализация

2. MPI реализация (`mpi`):

**Основные методы:**
- `SosninaAMatrixMultHorizontalMPI()` - конструктор, получающий матрицы A и B только в процессе 0
- `ValidationImpl()` - проверка инициализации MPI и количества процессов
- `PreProcessingImpl()` - получение ранга и размера MPI-коммуникатора
- `RunImpl()` - основной алгоритм параллельного умножения:
  - Подготовка данных через `PrepareAndValidateSizes()` (рассылка размеров)
  - Рассылка матрицы B через `PrepareAndBroadcastMatrixB()`
  - Распределение матрицы A через `DistributeMatrixAData()`
  - Локальные вычисления через `ComputeLocalMultiplication()`
  - Сбор результатов через `GatherResults()`
  - Формирование результата через `ConvertToMatrix()`
- `PostProcessingImpl()` - финализация

**Вспомогательные методы распределения данных:**
- `PrepareAndValidateSizes()` - широковещательная рассылка размеров матриц
- `PrepareAndBroadcastMatrixB()` - преобразование и рассылка матрицы B
- `DistributeMatrixAData()` - основное распределение строк матрицы A
- `GetRowsForProcess()` - определение строк для каждого процесса (циклическое распределение)
- `FillLocalAFlat()` - заполнение локального буфера данными матрицы A
- `SendRowsToProcess()` / `ReceiveRowsFromRoot()` - отправка и прием строк матрицы A

**Вспомогательные методы вычислений и сбора результатов:**
- `ComputeLocalMultiplication()` - локальное умножение части матрицы A на матрицу B
- `GatherResults()` - основной сбор результатов
- `CollectLocalResults()` - сбор локальных результатов в корневом процессе
- `SendLocalResults()` / `ReceiveResultsFromProcess()` - отправка и прием результатов
- `ConvertToMatrix()` - преобразование плоского массива в двумерную матрицу


3. Общие компоненты (`common`):
   - `common.hpp` - общие типы данных и константы

**Архитектурные особенности:**

- Использование MPI для межпроцессного взаимодействия
- Ленточная горизонтальная схема распределения данных
- Централизованная модель с процессом-координатором (rank 0)
- Эффективное использование MPI_Bcast для рассылки матрицы B 
- Применение MPI_Send/MPI_Recv для распределения строк матрицы A
- Обработка граничных случаев (пустые матрицы, несовместимые размеры)
- Минимизация коммуникационных операций за счет использования одного Bcast для матрицы B
- Балансировка нагрузки при распределении строк матрицы A

### Важные допущения и ограничения

**Обработка данных:**
- Поддержка матриц произвольных совместимых размеров
- Использование типа double для элементов матриц
- Проверка корректности размеров в ValidationImpl()

**Граничные случаи:**
```cpp
// Базовые тесты умножения
A = [[1,2],[3,4]], B = [[5,6],[7,8]] → C = [[19,22],[43,50]]
A = [[1,0],[0,1]], B = [[1,2],[3,4]] → C = [[1,2],[3,4]]  // Единичная матрица
A = [[0,0],[0,0]], B = [[1,2],[3,4]] → C = [[0,0],[0,0]]  // Нулевая матрица
A = [[1,2,3]], B = [[4],[5],[6]] → C = [[32]]             // Векторное умножение
```

### Рекомендации по использованию памяти
- Для матриц размером до 500×500 использовать последовательную версию
- Для больших матриц применять MPI версию
- Контролировать общий объем памяти, особенно для матрицы B которая дублируется на всех процессах
---
## 6. Экспериментальная установка

### Аппаратное обеспечение и ОС

Системные характеристики:
- Модель процессора: Apple M2 Chip (8-core CPU)
- Архитектура: ARM64
- Ядра/потоки: 4 производительных + 4 энергоэффективных ядра
- Оперативная память: 16 GB 
- Операционная система: macOS Sonoma 14.x
- Тип системы: Ноутбук (MacBook Air)

### Набор инструментов

Компиляция и сборка:
- Компилятор: GCC 11.4.0 (через Homebrew)
- Стандарт языка: C++17
- Среда разработки: Visual Studio Code
- Тип сборки: Release
- Система сборки: CMake

### Управление процессами

PPC_NUM_PROC: устанавливается через параметр -n в mpirun

```cpp
//Запуск с различным количеством процессов MPI
mpirun -n 1 ./bin/ppc_(perf)func_tests --gtest_filter="*sosnina_a_matrix_mult_horizontal*"
mpirun -n 2 ./bin/ppc_(perf)func_tests --gtest_filter="*sosnina_a_matrix_mult_horizontal*"
mpirun -n 3 ./bin/ppc_(perf)func_tests --gtest_filter="*sosnina_a_matrix_mult_horizontal*"
mpirun -n 4 ./bin/ppc_(perf)func_tests --gtest_filter="*sosnina_a_matrix_mult_horizontal*"
```

## 7. Результаты и обсуждение

### 7.1 Корректность

**Методы проверки корректности:**

1. Комплексное модульное тестирование:
   - 22 функциональных теста - проверка базовых сценариев
   - 10 тестов покрытия - обработка граничных случаев

2. Тестирование производительности:
   - 1 тест производительности с измерением времени выполнения для матриц 800×800
   - Проверка работоспособности на больших данных

**Ключевые тестовые сценарии:**
```cpp
// Базовые тесты умножения
[[1,2],[3,4]] × [[5,6],[7,8]] = [[19,22],[43,50]]
[[1,0],[0,1]] × [[1,2],[3,4]] = [[1,2],[3,4]]  // Единичная матрица
[[1,1],[1,1]] × [[1,1],[1,1]] = [[2,2],[2,2]]  // Матрица из единиц

// Граничные случаи
[[1]] × [[2]] = [[2]]                          // 1×1 матрицы
[[1,2,3]] × [[4],[5],[6]] = [[32]]             // Вектор-строка × вектор-столбец
[[1],[2],[3]] × [[4,5,6]] = [[4,5,6],[8,10,12],[12,15,18]] // Вектор-столбец × вектор-строку
```
**Методология проверки:**
- Каждый тест выполняется для обеих реализаций (SEQ и MPI)
- Результаты сравниваются с эталонным значением
- Проверяется идентичность результатов между SEQ и MPI версиями
- Используется фреймворк (Google Test) для автоматизированной проверки

**Результаты проверки корректности:**
- Все 22 функциональных теста пройдены успешно
- 2 теста производительности подтвердили работоспособность на больших данных
- Результаты SEQ и MPI реализаций полностью совпадают
- Тест производительности подтвердил работоспособность на больших данных 

### 7.2 Производительность

Результаты измерения производительности для матриц:

### Время выполнения (task_run) - чистые вычисления

| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 0.0748   | 1.00      | 100%          |
| mpi   | 2        | 0.0420   | 1.78      | 89%           |
| mpi   | 3        | 0.0336   | 2.23      | 74%           |
| mpi   | 4        | 0.0360   | 2.08      | 52%           |

### Время выполнения (pipeline) - полный цикл

| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 0.0753   | 1.00      | 100%          |
| mpi   | 2        | 0.0465   | 1.62      | 81%           |
| mpi   | 3        | 0.0377   | 2.00      | 67%           |
| mpi   | 4        | 0.0362   | 2.08      | 52%           |

**Анализ результатов:**

1. **Результаты Task_Run (чистые вычисления):**
   - Максимальное ускорение достигается на 3 процессах: 2.23×
   - Эффективность последовательно снижается: 89%, 74%, 52%
   - На 4 процессах наблюдается снижение производительности относительно 3 процессов

2. **Результаты Pipeline (полный цикл):**
   - Ускорение: 1.62×, 2.00×, 2.08×
   - Эффективность: 81%, 67%, 52%
   - Схожая динамика с task_run, но с дополнительными накладными расходами

**Ключевые выводы:**
- MPI версия демонстрирует положительное ускорение на 2-4 процессах
- Наивысшая эффективность достигается на 2 процессах (89% для task_run)
- Оптимальная конфигурация: 3 процесса для баланса ускорения и эффективности
- Pipeline режим показывает аналогичные тенденции с task_run

**Основные наблюдения:**
1. Алгоритм демонстрирует предсказуемое поведение с пиком производительности на 3 процессах
2. Для данных размеров матриц оптимально использовать 2-3 процесса

---
## 8. Выводы

### Достижения

1. Корректность реализации:
   - Обе версии (SEQ и MPI) прошли все 22 функциональных теста 
   - Результаты полностью совпадают с эталонными значениями
   - Обеспечена корректная обработка граничных случаев

2. Эффективное распараллеливание:
   - Алгоритм хорошо масштабируется с ростом числа процессов
   - Оптимальная производительность достигается на 2 процессах (89%)
   - Схема распределения данных обеспечивает балансировку нагрузки

### Ограничения и проблемы

1. Коммуникационные накладные расходы:
   - Полный цикл (pipeline) показывает снижение эффективности при 4 процессах

2. Ограничения масштабируемости:
   - Эффективность может снижаться при очень большом числе процессов

3. Требования к памяти:
   - Для очень больших данных требуется значительный объем RAM

---

## 9. Список литературы

1. Антонов А. С. Параллельное программирование с использованием технологии MPI. — М.: Изд-во МГУ, 2010.  
2. Корнеев В. Д. Параллельное программирование в MPI. — М.: Изд-во МГУ, 2002.  
3. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.
4. MPI Forum. MPI: A Message-Passing Interface Standard, Version 4.0. 2021. https://www.mpi-forum.org/docs/
5. Microsoft. Справочник по MPI. — 2024. https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-reference
