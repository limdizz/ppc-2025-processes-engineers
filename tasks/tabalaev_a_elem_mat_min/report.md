# Минимальное значение элементов матрицы

- Студент: Табалаев Антон Максимович
- Технология: SEQ | MPI
- Вариант: 14

## 1. Введение 

Поиск минимального элемента матрицы является одной из базовых операций линейной алгебры и важным компонентом во множестве алгоритмов обработки данных, компьютерной графики и др.
В рамках этого исследования проводится сравнительный анализ производительности последовательной и параллельной MPI-реализаций данного алгоритма.

## 2. Постановка задачи

**Цель работы:**
Реализовать последовательную и MPI-параллельную версии алгоритма поиска минимального элемента матрицы, провести их сравнение и анализ эффективности.

**Определение задачи:**
Для матрицы `A` размера `m × n` необходимо определить значение: `min(A) = min(A[i][j])`, где `0 <= i < n`, `0<= j < m`.

**Ограничения:**
- Матрица должна быть представлена как вектор целых чисел.
- Размеры матрицы могут быть большими.
- Корректность должна сохраняться при любых значениях элементов.
- Результаты SEQ и MPI версий должны совпадать.

## 3. Алгоритм(Последовательная версия)

**Входные данные:** количество строк `rows`, количество колонок `columns`, вектор размера `rows * columns`.

**Выходные данные:** целое число - минимальный элемент матрицы.

**Алгоритм**:
1. Получить входной вектор `matrix`.
2. Присвоить переменной `minik` значение первого элемента вектора.
3. В цикле пройтись по всем оставшимся элементам, начиная со второго:
 - Записать в `minik` минимальное значение из текущего элемента и `minik`.
4. Вернуть значение `minik`.

**Сложность**: O(n), где `n` - размер вектора `matrix`.

### Код последовательной версии алгоритма
```
bool TabalaevAElemMatMinSEQ::RunImpl() {
  auto &matrix = std::get<2>(GetInput());

  int minik = matrix[0];
  for (size_t i = 1; i < matrix.size(); i++) {
    minik = std::min(minik, matrix[i]);
  }

  GetOutput() = minik;
  return true;
}
```

## 4. Схема распараллеливания

Алгоритм параллельного вычисления минимального элемента матрицы реализует распределение данных между процессами с последующей редукцией результатов. Процессы работают с определёнными блоками элементов матрицы, что обеспечивает равномерную нагрузку.

- **Инициализация:** Все процессы запускаются в коммуникаторе MPI_COMM_WORLD, определяется общее количество процессов и ранг текущего процесса.
- **Распределение данных:** Нулевой процесс загружает входную матрицу и вычисляет параметры для распределения данных:
  - Определяет размер каждого блока: `part_size = matrix_size / world_size`.
  - Распределяет остаточные элементы: первые процессы получают по одному дополнительному элементу, если размер матрицы нацело не делится на количество процессов.
  - Формирует вектора `sendcounts` (размеры блоков) и `displs` (смещения в матрице).
  Далее вектор `sendcounts` рассылается всем процессам через `MPI_Bcast`, позволяя каждому процессу определить размер своего `local_matrix`. После этого матрица распределяется между процессами при помощи функции `MPI_Scatterv`.
- **Локальные вычисления:** Каждый процесс находит минимальный элемент в своем `local_matrix`.
- **Формирование результатов:** Локальные минимумы со всех процессов объединяются с помощью операции `MPI_Allreduce` с параметром `MPI_MIN`.
- **Формирование итога:** Глобальный минимум становится доступным на всех процессах одновременно.

### Код параллельной версии алгоритма
```
bool TabalaevAElemMatMinMPI::RunImpl() {
  int world_size = 1;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  int world_rank = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

  std::vector<int> local_vec;
  std::vector<int> matrix;
  std::vector<int> sendcounts(world_size);
  std::vector<int> displs(world_size);

  size_t matrix_size = 0;

  if (world_rank == 0) {
    auto &input = GetInput();
    matrix = std::get<2>(input);

    matrix_size = matrix.size();
    size_t part_size = matrix_size / world_size;
    size_t remainder = matrix_size % world_size;

    size_t offset = 0;
    for (int i = 0; i < world_size; ++i) {
      sendcounts[i] = part_size + (i < remainder ? 1 : 0);
      displs[i] = offset;
      offset += sendcounts[i];
    }
  }

  MPI_Bcast(sendcounts.data(), world_size, MPI_INT, 0, MPI_COMM_WORLD);
  int local_size = sendcounts[world_rank];
  local_vec.resize(local_size);

  MPI_Scatterv(matrix.data(), sendcounts.data(), displs.data(), MPI_INT, local_vec.data(), local_size, MPI_INT, 0,
               MPI_COMM_WORLD);

  int local_minik = INT_MAX;
  for (int elem : local_vec) {
    local_minik = std::min(local_minik, elem);
  }

  int global_minik = 0;
  MPI_Allreduce(&local_minik, &global_minik, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);

  GetOutput() = global_minik;

  return true;
}
```

## 5. Экспериментальные исследования

### Окружение

| Параметр           | Значение                      |
|--------------------|-------------------------------|
| **OS** | Windows 11 Pro 25H2  |
| **CPU** | AMD Ryzen 5 5600X (6 cores, 12 threads, 3.70 GHz) |
| **RAM** | 16 GB DDR4 3400 МГц      |
| **Компилятор**      | MSVC 14.43.34808 |
| **Версия MPI**      | Microsoft MPI 10.1.12498.52 |

### Тестовые данные

1. Функциональные тесты:
- Используют заранее подготовленные матрицы размеров от 3×3 до 10×10 с известным минимальным элементом, размещенным в центре матрицы.
- Значения элементов матрицы генерируются случайным образом в диапазоне от минимального элемента до 250.
2. Тесты производительности:
- Используют матрицу `5000×5000` с элементами, вычисляемыми по формуле `i * i + j`.
- Минимальный элемент имеет фиксированное значение `-1_000_000` и размещается в центре матрицы.

### Сравнение производительности

Для сравнения производительности использовалась матрица размером `5000×5000`.

Вычисления производились по следующим формулам:

- `Ускорение = Время_последовательное / Время_параллельное`
- `Эффективность = (Ускорение / Количество_процессов) × 100%`

| Режим выполнения | Количество процессов | Время выполнения (сек) | Ускорение | Эффективность |
|------------------|---------------------|------------------------|-----------|---------------|
| **Последовательный** | | | | |
| pipeline | 1 | 0.0113795200 | 1.00x | - |
| task_run | 1 | 0.0113832200 | 1.00x | - |
| **MPI (2 процесса)** | | | | |
| pipeline | 2 | 0.0368656000 | 0.31x | 15% |
| task_run | 2 | 0.0370944800 | 0.31x | 15% |
| **MPI (4 процесса)** | | | | |
| pipeline | 4 | 0.0382311400 | 0.30x | 7% |
| task_run | 4 | 0.0355026800 | 0.32x | 8% |
| **MPI (6 процессов)** | | | | |
| pipeline | 6 | 0.0361363000 | 0.31x | 5% |
| task_run | 6 | 0.0372967800 | 0.31x | 5% |

## 6. Результаты

1. Результаты функционального тестирования
- Все функциональные тесты успешно пройдены.
- Все реализации (SEQ и MPI) работают правильно и выдают идентичные результаты.

2. Результаты сравнения производительности
- MPI версии работают в 3.2-3.4 раза медленнее последовательных.
- Лучший результат: task_run с 4 процессами (0.32x ускорения).
- Худший результат: pipeline с 4  процессами (0.30x ускорения).

## 7. Выводы

1. Разработанные алгоритмы корректно решают поставленную задачу, что подтверждается успешным прохождением всех функциональных тестов.
2. Параллельная реализация с использованием MPI показала снижение производительности по сравнению с последовательной версией. MPI-версии работают в 3.2-3.4 раза медленнее, что связано с высокими накладными расходами на межпроцессное взаимодействие.
3. При увеличении числа процессов эффективность снижается до 5-7%, что объясняется ростом коммуникационных издержек между процессами при относительно небольшой вычислительной нагрузке задачи поиска минимума.

## 8. Источники

1. Сысоев А. В. Лекции по курсу «Параллельное программирование для кластерных систем».
2. Официальная документация Microsoft MPI — https://learn.microsoft.com/ru-ru/message-passing-interface
3. Документация Open MPI — https://www.open-mpi.org/doc/
4. C++ Reference — https://en.cppreference.com
