# Отчёт по лабораторной работе №1: Нахождение наиболее близких соседей вектора

---

Студент: Цыплаков Кирилл Александрович, группа: 3823Б1ПР2 \
Технологии: SEQ, MPI\
Вариант: 7
## 1. Введение  
---
В данной работе изучается задача определения пары соседних элементов вектора с наименьшей разницей между их значениями. Основная цель — реализовать как последовательный, так и MPI-параллельный алгоритм, провести сравнение их производительности на векторах различной длины и оценить, как обмен данными между процессами влияет на масштабируемость решения.

## 2. Постановка задачи
---
На вход подается вектор целых чисел. Необходимо определить такой индекс i, для которого разница между элементами с индексами i и i+1 минимальна. В качестве результата возвращается кортеж индексов (i, i+1).

### Дополнительные требования и ограничения:
1. Если встречаются несколько пар с одинаковой минимальной разницей, выбирается пара с индексом, который ближе к 0 (к началу вектора);
2. Алгоритмы должны корректно обрабатывать любые значения элементов вектора;
3. Для одинаковых входных данных последовательная и параллельная реализации должны возвращать одинаковый результат;
4. Параллельный алгоритм реализуется с использованием MPI и должен правильно работать.

## 3. Базовый алгоритм
---
Концепция такова, чтобы за один проход по вектору вычислить наименьший модуль разности соседних элементов. 

### Ход работы:
1. В конструкторе принимаем на вход вектор, а на выход ставим по умолчанию (-1, -1), что означает, что нету такой пары, которая удовлетворяла бы поставленную цель.
```
TsyplakovKVecNeighboursSEQ::TsyplakovKVecNeighboursSEQ(const InType& in) {
  SetTypeOfTask(GetStaticTypeOfTask());
  GetInput() = in;
  GetOutput() = std::make_tuple(-1, -1);
}
```

2. Реализуем поиск пары с минимальной разницей по значению. Создаём переменные, которым присваиваем по умолчанию значения - индексу -1, а минимальной разнице MAX_INT (самое большоре число типа int).
```
bool TsyplakovKVecNeighboursSEQ::RunImpl() {
  const std::size_t n = vector_data_.size();

  if (n < 2) {
    GetOutput() = std::make_tuple(-1, -1);
    return true;
  }

  int min_diff = std::numeric_limits<int>::max();
  int min_index = -1;

  for (std::size_t i = 0; i + 1 < n; ++i) {
    int64_t diff = std::llabs(static_cast<int64_t>(vector_data_[i + 1]) - static_cast<int64_t>(vector_data_[i]));

    if (diff < min_diff || (diff == min_diff && std::cmp_less(i, static_cast<std::size_t>(min_index)))) {
      min_diff = static_cast<int>(diff);
      min_index = static_cast<int>(i);
    }
  }

  if (min_index >= 0) {
    GetOutput() = std::make_tuple(min_index, min_index + 1);
  } else {
    GetOutput() = std::make_tuple(-1, -1);
  }

  return true;
}
```
Конечно же, учитываем ситуацию, когда размер вектора меньше двух.

## 4. Схема распараллеливания
Алгоритм параллельного поиска минимальной разницы соседних элементов

### 1. Инициализация MPI:
Все процессы вызывают стандартные функции:

- ```MPI_Comm_rank``` - для определения номера процесса (rank).
- ```MPI_comm_size``` - для определения общего количества процессов (size).

Работа выполняется внутри коммуникатора ```MPI_COMM_WORLD```.

### 2. Получение входных данных:
Входной массив ```full_arr``` считывается и хранится только в процессе с рангом 0.
Если размер массива меньше 2, возвращается результат ```(-1, -1)```.

### 3. Подготовка распределения данных между процессами:

Массив разбивается на блоки по количеству процессов. Для равномерной загрузки:
- каждый процесс получает ```base = n / size``` элементов

- первые ```extra = n % size``` процессов получают на один элемент больше

Дополнительно, чтобы корректно обрабатывать пары на стыке блоков, каждому процессу кроме последнего добавляется перекрытие в один элемент.
Перекрытие гарантирует, что не будет потеряна пара, лежащая на границе двух блоков.

Формируются два массива:

<b>sendcounts</b> — количество элементов, отправляемых каждому процессу

<b>displs</b> — смещения в исходном массиве для каждого процесса

### 4. Распределение данных: MPI_Scatterv:
Используется вызов:

```
MPI_Scatterv(
    full_arr.data(),
    sendcounts.data(),
    displs.data(),
    MPI_INT,
    local_arr.data(),
    recv_count,
    MPI_INT,
    0,
    MPI_COMM_WORLD
);
```
Теперь каждый процесс получает только свой участок данных, а не весь массив.
### 5. Локальный поиск минимальной разницы:

Каждый процесс анализирует только свой локальный буфер local_arr:
```
for (int i = 0; i < recv_count - 1; i++) {
    int diff = abs(local_arr[i] - local_arr[i + 1]);
    ...
}
```

Локально вычисляется:

- минимальная разница ```best_local_gap```

- индекс этой пары в глобальных координатах (путём добавления смещения ```global_offset = displs[rank]```)

### 6. Глобальная агрегация результатов:
Все процессы участвуют в ```MPI_Allreduce(&best_local_gap, &best_global_gap, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD)```. В результате каждый процесс знает глобальный минимум разницы соседних элементов.

Процесс-кандидат получает свой локальный индекс, остальные — INT_MAX:
```
candidate_index = (best_local_gap == best_global_gap)
                  ? best_local_pos
                  : INT_MAX;
```
Далее выполняется:

```
MPI_Allreduce(&candidate_index, &final_index, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD)
```

```final_index``` — окончательный глобальный индекс первой пары элементов с минимальной разницей.

### 7. Формирование результата:
Поскольку соседняя пара состоит из элементов с индексами ```i``` и ```i + 1```, итоговое значение:

```GetOutput() = std::make_tuple(final_index, final_index + 1);```

Все процессы имеют одинаковый результат.

### 8. Особенности параллельной топологии:

- В алгоритме распределение данных выполняется только процессом с рангом 0, который использует MPI_Scatterv.

- Каждый процесс получает собственный участок входного массива, что соответствует классической модели «master → workers».

- Для корректной обработки пар используется перекрытие блоков на один элемент.

- Все процессы равноправно участвуют в глобальных редукциях, но входные данные получает только rank 0.


## 5. Детали реализации:
---
### Структура кода:
```
tsyplakov_k_vec_neighbours
    ├───common
    │   └───include
    │           common.hpp - определение типов входных/выходных/тестовых данных
    │
    ├───mpi
    │   ├───include
    │   │       ops_mpi.hpp - заголовочный файл MPI-реализации
    │   │
    │   └───src
    │           ops_mpi.cpp - MPI-реализация
    │
    ├───seq
    │   ├───include
    │   │       ops_seq.hpp - заголовочный файл SEQ-реализации
    │   │
    │   └───src
    │           ops_seq.cpp - SEQ-реализация
    │
    └───tests
        ├───functional
        │       main.cpp - функциональные тесты
        │
        └───performance
                main.cpp - тесты производительности
```

### Основные классы и функции:

```TsyplakovKVecNeighboursSEQ``` - последовательная реализация.\
```TsyplakovKVecNeighboursMPI``` - параллельная реализация.\

```ValidationImpl()``` - проверка корректности входных данных перед выполнением алгоритма.\
```PreProcessingImpl()``` - подготовка данных перед основными вычислениями.\
```RunImpl()``` - основной алгоритм поиска минимальной разницы соседних элементов.
```PostProcessingImpl()``` - действия после завершения основного вычисления.

### Особые случаи

Особые ситуации в параллельной обработке возникают при работе с элементами, которые потенциально могут образовать минимальную разницу на границах диапазонов, обрабатываемых разными процессами.

### 1. Распределение данных между процессами

В новой реализации вводится явное распределение элементов массива от процесса 0 к остальным процессам с использованием MPI_Scatterv.
Каждый процесс получает непрерывный поддиапазон исходного массива, длина которого определяется заранее рассчитанными массивами:

- sendcounts — количество элементов для каждого процесса

- displs — смещение каждого блока в глобальном массиве

Чтобы не потерять пару соседних элементов, находящихся на границе двух блоков, процессы (кроме последнего) получают дополнительный перекрывающий элемент справа.

В итоге каждый процесс получает:
```
local_arr[0 ... recv_count - 1]
```

где последний элемент (кроме последнего процесса) также входит в блок следующего процесса.

### 2. Локальный поиск минимальной разницы

Каждый процесс вычисляет разницы только внутри своего локального блока:
```
for (int i = 0; i < recv_count - 1; i++) {
    int diff = abs(local_arr[i] - local_arr[i + 1]);
    ...
}
```

Если локальная разница меньше текущего минимума best_local_gap, процесс обновляет:
```
best_local_gap

best_local_pos — глобальный индекс пары
```
Глобальный индекс получается через смещение:
```
global_index = displs[rank] + i
```

Из-за заранее добавленного перекрытия дополнительных обменов между процессами не требуется — каждая граница уже покрыта локальными данными.

### 3. Глобальная агрегация результатов

После завершения локальных вычислений алгоритм выполняет две глобальные редукции.

Шаг 1: поиск минимальной разницы
```
MPI_Allreduce(&best_local_gap, &best_global_gap,
              1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);
```

Все процессы получают минимальную разницу среди всех локальных результатов.

Шаг 2: выбор глобального индекса

Процессы, у которых локальная разница равна глобальному минимуму, передают свой индекс, остальные отправляют INT_MAX:
```
candidate_index = (best_local_gap == best_global_gap) 
                  ? best_local_pos 
                  : INT_MAX;
```

Глобальный минимальный индекс находится командой:
```
MPI_Allreduce(&candidate_index, &final_index,
              1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);
```

Этот индекс одинаков для всех процессов к концу выполнения.

4. Формирование результата

Пара с минимальной разницей записывается как:
```
(final_index, final_index + 1)
```

и сохраняется в ```GetOutput()```.

Если массив слишком мал или отсутствуют корректные пары, результат остаётся ```(-1, -1)```.

### Пространственная и временная сложности алгоритмов

<b>Последовательная версия:</b>

Временная сложность составляет ```O(N)```, где  ```N``` - размер вектора.\
Пространственная сложность - ```O(N)```, где ```N``` - размер вектора индексов, равный размеру входного вектора.

<b>Параллельная версия:</b>

Временная сложность - ```O(N/P + log(P))```, где ```N``` - размер вектора, ```P``` - количество процессов, ```log(P)``` - сложность операции ```MPI_Reduce```.\
Пространственная сложность - ```O(N/P)``` на процессах, связанных с вычислениями и ```O(N)``` на Root-процессах.

## 6. Тестовая инфраструктура
---

### Аппартаное обеспечение

<table border="1">
  <tr>
    <td><b>Параметр</b></td>
    <td><b>Значение</b></td>
  </tr>
  <tr>
    <td>CPU</td>
    <td>Intel Core i5-12500H (8 cores, 12 threads, 2.50 GHz, L3 Cache 18 MB)</td>
  </tr>
  <tr>
    <td>RAM</td>
    <td>16 GB, DDR4</td>
  </tr>
</table>

### Программное обеспечение 

<table border="1">
  <tr>
    <td><b>Параметр</b></td>
    <td><b>Значение</b></td>
  </tr>
  <tr>
    <td>OC</td>
    <td>Windows 10, WSL (Ubuntu 24.04)</td>
  </tr>
  <tr>
    <td>MPI</td>
    <td>OpenMPI (4.1.6)</td>
  </tr>
  <tr>
    <td>Компилятор</td>
    <td>g++ (13.3.0)</td>
  </tr>
  <tr>
    <td>Сборка</td>
    <td>Release</td>
  </tr>
</table>

### Тестовые данные 

<b>Functional tests:</b> берутся параметры теста (в ```functional/main.cpp``` например есть вектора на 1е6, 1е7 и 1е8 элементов), генерируется вектор ```input_data__``` и заполняется значениями по определённой формуле, после чего вычисляется ожидаемый результат.

<b>Perfomance tests:</b> есть переменная ```kCount_```, которая отвечает за размер вектора, после чего вектор заполняется значениями: ```0, 1, 2, 3, 4, ..., kCount_```.

## 7. Результаты и обсуждения.
---

### Корректность:
Проверка корректности реализована при помочи библиотеки ```gtest.h```.

<b>Functional тесты</b> покрывают обычные малые вектора, большие вектора, нулевые, отрицательные.\
<b>Perfomance тесты:</b> проверяют работу алгоритма на больших данных.

### Производительность:

<b>Метрики:</b>

1. Время выполнения вычислительной части алгоритма в миллисекундах.
2. Ускорение параллельной версии относительно последовательной.
3. Эффективность распараллеливания: (ускорение / число процессов) * 100%.


<b>Размер вектора: 150.000.000</b>
<table border="1"> <thead> <tr> <th>Режим</th> <th>Процессы</th> <th>Время (мс)</th> <th>Ускорение</th> <th>Эффективность</th> </tr> </thead> <tbody> <tr> <td>SEQ</td> <td>1</td> <td>148.30</td> <td>1.00</td> <td>N/A</td> </tr> <tr> <td>MPI</td> <td>2</td> <td>321.41</td> <td>0.46</td> <td>23.0%</td> </tr> <tr> <td>MPI</td> <td>4</td> <td>230.62</td> <td>0.643</td> <td>16.1%</td> </tr> </tbody> </table>

### Анализ полученных результатов

При размере вектора в 150 млн элементов параллельная версия оказывается медленнее последовательной из-за огромных накладных расходов на MPI_Scatterv. Данные объёмом ~600 МБ (150 млн * 4 байта) пересылаются от ранга 0, что полностью перекрывает выгоду от параллельного вычисления простой линейной операции.

## 8. Выводы
---

В ходе работы были разработаны и исследованы последовательная и параллельная версии алгоритма поиска пары соседних элементов вектора с минимальной разницей значений. Параллельная версия была реализована с использованием MPI и протестирована на различных количествах процессов.

Полученные экспериментальные данные показали, что при запуске на двух процессах параллельная версия не даёт ускорения относительно последовательной. Более того, при увеличении числа процессов до четырёх производительность улучшается относительно варианта с двумя процессами, но всё равно остаётся хуже последовательной реализации.

Такое поведение объясняется тем, что рассматриваемый алгоритм обладает крайне низкой вычислительной сложностью на одну операцию и высокой плотностью обращений к памяти. В результате основное время выполнения определяется доступом к данным, а не вычислениями, и добавление параллельности не сокращает общее время, поскольку все процессы обращаются к одному и тому же большому вектору.

Дополнительные накладные расходы MPI — синхронизация, коллективные операции MPI_Allreduce и необходимость поддерживать согласованность данных — приводят к тому, что рост количества процессов не только не ускоряет вычисления, но и снижает общую эффективность. Фактически задача становится ограниченной памятью, и пропускная способность памяти становится узким местом.

Таким образом, проведённые эксперименты показывают, что для данной задачи параллельная реализация с использованием MPI плохо масштабируется и не даёт выигрыша при увеличении числа процессов. Последовательная версия остаётся наиболее эффективной. Параллельный подход может быть оправдан лишь при значительно больших объёмах данных, где память не является разделяемым узким ресурсом.

## 9. Список литературы
---

1. Документация по курсу: "Параллельное программирование": <https://learning-process.github.io/parallel_programming_course/ru/index.html> (Оболенский А.А, Нестеров А.Ю)
2. Список лекций по курсу "Параллельное программирование". (Сысоев А.В. ННГУ 2025 г.)
3. Список практических занятий по курсу "Пареллельное программирование". (Оболенский А.А, ННГУ 2025 г.)