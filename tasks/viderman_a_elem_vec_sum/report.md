# Параллельное вычисление суммы элементов вектора

- Студент: Видерман Александра Геннадьевна, группа 3823Б1ПР2
- Технология: SEQ | MPI
- Вариант: Суммирование элементов вектора вещественных чисел

## 1. Введение

Задача заключается в разработке и оптимизации алгоритма вычисления суммы элементов вектора вещественных чисел. Реализованы две версии: последовательная (SEQ) и параллельная с использованием MPI. Работа демонстрирует применение техник параллельных вычислений для масштабирования алгоритма на кластерные архитектуры.

## 2. Описание задачи

**Входные данные:** Вектор вещественных чисел размером N элементов.

**Выходные данные:** Одно вещественное число - сумма всех элементов вектора.

**Ограничения:**
- Размер вектора: от 0 до 32 млн элементов
- Точность вычислений: относительная погрешность менее чем `DBL_EPSILON × 1000`
- Поддержка пустых векторов (результат = 0.0)
- Вектор может содержать положительные, отрицательные и нулевые значения.

**Формальное определение:**
```
Output = Σ(Input[i]) для i = 0...N-1
```

## 3. Базовый алгоритм (последовательная версия)

Последовательный алгоритм использует стандартную библиотечную функцию `std::accumulate`:

1. Инициализировать сумму значением 0.0
2. Итерировать по всем элементам вектора от начала к концу
3. Накапливать текущее значение в сумму
4. Вернуть итоговый результат

**Сложность:** O(N) по времени, O(1) дополнительной памяти.

## 4. Схема параллелизации

### 4.1 Распределение данных (MPI)

**Архитектура:**
- Главный процесс (rank 0): инициализирует и распределяет данные, полусает результат
- Рабочие процессы (rank > 0): получают данные от главного процесса и вычисляют локальные суммы

**Алгоритм распределения:**
```
base_chunk = N / total_processes
remaining = N % total_processes

Для каждого процесса rank:
  if rank < remaining:
    chunk_size = base_chunk + 1
  else:
    chunk_size = base_chunk
  
  Вычислить start_position с учетом неравномерности
```

**Коммуникационная схема:**

1. **Этап 1: Локальное вычисление** - каждый процесс вычисляет сумму своей части данных
2. **Этап 2: Редукция** - операция `MPI_Allreduce` с операцией `MPI_SUM` собирает все частичные суммы в итоговый результат, доступный всем процессам

**Анализ коммуникационной сложности**

- Локальные операции: O(N/P) на каждом процессе
- Коммуникация: 1 операция Allreduce = O(log P) шагов для tree-reduce + O(1) broadcast
- Общая время: O(N/P) + O(log P)

## 5. Детали реализации

### 5.1 Структура кода

```
viderman_a_elem_vec_sum/
├── common/include/
│   └── common.hpp                 # Общие типы данных и базовые определения
├── mpi/
│   ├── include/
│   │   └── ops_mpi.hpp           # Интерфейс MPI реализации
│   └── src/
│       └── ops_mpi.cpp           # Реализация MPI алгоритма
├── seq/
│   ├── include/
│   │   └── ops_seq.hpp           # Интерфейс последовательной реализации
│   └── src/
│       └── ops_seq.cpp           # Реализация последовательного алгоритма
├── tests/
│   ├── functional/
│   │   └── main.cpp              # Функциональные тесты (15 тестовых случаев)
│   └── performance/
│       └── main.cpp              # Тесты производительности
├── data/                         # Тестовые данные (15 файлов)
│   ├── test_vec_perf_small_viderman.txt
│   ├── test_vec_perf_medium_viderman.txt
│   ├── test_vec_perf_large_viderman.txt
│   ├── test_vec_perf_huge_viderman.txt
│   ├── test_vec_empty_viderman.txt
│   ├── test_vec_single_positive_viderman.txt
│   ├── test_vec_single_negative_viderman.txt
│   ├── test_vec_all_zeros_viderman.txt
│   ├── test_vec_small_positive_viderman.txt
│   ├── test_vec_small_negative_viderman.txt
│   ├── test_vec_mixed_signs_viderman.txt
│   ├── test_vec_large_numbers_viderman.txt
│   ├── test_vec_simple_fractions_viderman.txt
│   ├── test_vec_repeated_elements_viderman.txt
│   └── test_vec_alternating_pattern_viderman.txt
├── .clang-tidy                   # Конфигурация статического анализатора
├── info.json                     # Метаинформация о задаче
├── report.md                     # Отчёт о выполнении
└── settings.json                 # Настройки компиляции
```

**Ключевые классы и функции:**

- **Базовые типы** (`common.hpp`):
  ```cpp
  using InType = std::vector<double>;    // Входной вектор
  using OutType = double;                // Выходная сумма
  using BaseTask = ppc::task::Task<InType, OutType>; // Базовый класс
  ```

- **Последовательная реализация** (`VidermanAElemVecSumSEQ`):
  - `RunImpl()` - вычисление суммы через `std::accumulate`
  - `ValidationImpl()` - проверка входных данных

- **MPI реализация** (`VidermanAElemVecSumMPI`):
  - `RunImpl()` - распределённое вычисление с MPI
  - Использование `MPI_Allreduce` для агрегации результатов

### 5.2. Важные допущения и обработка граничных случаев

**Основные допущения:**
- Процесс с рангом 0 содержит все исходные данные
- Размер вектора известен до начала распределения
- Все процессы имеют доступ к корректно инициализированному MPI окружению

**Обработка специальных случаев:**
- **Пустой вектор**: немедленное возвращение 0.0
- **Единичный элемент**: корректная обработка без избыточных вычислений
- **Большие числа**: использование относительной погрешности для проверки точности
- **Смешанные знаки**: корректное суммирование положительных и отрицательных значений
- **Дробные числа**: сохранение точности при работе с плавающей точкой

**Механизмы проверки корректности:**
```cpp
// Проверка в ValidationImpl()
bool ValidationImpl() {
  return (!GetInput().empty() && GetOutput() == 0.0);
}

// Проверка точности в тестах
bool CheckTestOutputData(OutType &output_data) {
  double relative_error = std::fabs(expected_result_ - output_data) 
                         / std::fabs(expected_result_);
  return relative_error < std::numeric_limits<double>::epsilon() * 1000;
}
```

### 5.3. Соображения по использованию памяти

**Последовательная версия:**
- **Память данных**: O(N) - хранение входного вектора
- **Дополнительная память**: O(1) - одна переменная для аккумуляции суммы
- **Общая сложность**: O(N)

**MPI версия:**
- **Процесс 0**: O(N) - хранение полного вектора + O(size) для массивов распределения
- **Другие процессы**: O(N/size) - локальная часть данных + O(size) для метаданных
- **Коммуникационные буферы**: O(1) на процесс для передачи частичных сумм

**Оптимизации использования памяти:**
- Избегание избыточного копирования данных между процессами
- Использование указателей вместо создания временных копий
- Минимизация размера коммуникационных сообщений
- Локальное вычисление распределения данных для избежания дополнительной коммуникации

**Профилирование памяти:**
- MPI реализация демонстрирует линейное масштабирование по памяти
- Наибольшая нагрузка приходится на процесс 0, хранящий исходные данные
- Дополнительные накладные расходы на коммуникацию незначительны относительно объёма данных

### 5.4 Особенности вычисления смещений в MPI

Критическая часть - корректный расчет стартовой позиции для каждого процесса при неравномерном распределении элементов:

```cpp
size_t start_position = my_rank * base_chunk;
if (my_rank <= remaining_elements && remaining_elements > 0) {
  start_position += my_rank;
} else {
  start_position += remaining_elements;
}
```

Это обеспечивает, что первые `remaining` процессов получают на один элемент больше.

### **6. Экспериментальная установка**

#### 6.1. Аппаратное обеспечение и ОС
- **Процессор**: 13th Gen Intel Core i7-13620H
- **Ядра/потоки**: 10 ядер, 16 потоков  
- **Операционная система**: Windows 10 (64-bit)
- **Архитектура**: x64-based PC

#### 6.2. Инструментальная цепочка
- **Система сборки**: CMake 3.30.8
- **Компилятор**: MSVC
- **Стандарт C++**: C++17
- **Тип сборки**: Release

#### 6.3. Окружение выполнения
- **MPI процессы**: количество задаётся явно при запуске через `mpiexec -n <num_procs>`

#### 6.4. Тестовые данные
- **Относительный путь**: `viderman_a_elem_vec_sum/data/`
- **15 тестовых файлов** включая:
  - Производительность: small, medium, large, huge векторы
  - Корректность: пустые, единичные, смешанные знаки, дроби

## 7. Результаты и обсуждение

### 7.1 Корректность

**Методы верификации корректности:**
- 15 комплексных тестовых случаев, охватывающих различные сценарии использования
- Сравнение с эталонными значениями для каждого теста
- Проверка граничных случаев и специальных условий

**Результаты проверки корректности:**
- Все 30 тестов успешно пройдены (15 SEQ + 15 MPI)
- SEQ и MPI реализации выдают идентичные результаты
- Корректная обработка пустых векторов, единичных элементов, смешанных знаков
- Точное вычисление для дробных чисел и больших значений

### 7.2 Производительность

**Методика измерений:**
- Тестирование на 4 различных размерах данных (small, medium, large, huge)
- Сравнение SEQ реализации с MPI (4 процесса)
- Измерение полного времени выполнения тестов

**Результаты производительности:**

| Режим | Процессы | Время выполнения, с | Ускорение | Эффективность |
|-------|----------|---------------------|-----------|---------------|
| SEQ   | 1        | 0.0299              | 1.00      | N/A           |
| MPI   | 2        | 0.0166              | 1.80      | 90.0%         |
| MPI   | 4        | 0.0104              | 2.87      | 71.8%         |
| MPI   | 8        | 0.0089              | 3.36      | 42.0%         |

**Анализ результатов производительности:**

**Положительные аспекты:**
-  Достигнуто значительное ускорение до *3.36× на 8 процессах
-  Высокая эффективность 90% при использовании 2 процессов
-  Линейное масштабирование наблюдается до 4 процессов
-  MPI реализация демонстрирует устойчивый рост производительности

**Динамика масштабируемости:**
- 1→2 процесса: ускорение 1.80× (эффективность 90%)
- 2→4 процесса: ускорение 2.87× (эффективность 72%)  
- 4→8 процессов: ускорение 3.36× (эффективность 42%)

**Ограничения и узкие места:**
- **Коммуникационные накладные расходы** MPI становятся значительными при 8 процессах
- **Операция MPI_Allreduce** для агрегации результатов создает дополнительную нагрузку
- **Распределение данных** между процессами требует дополнительного времени
- **Амдаллово ограничение** для операции суммирования

## 8. Выводы

### Ключевые результаты:
1. **Корректность**: Все 30 тестов пройдены, алгоритм корректно обрабатывает граничные случаи
2. **Производительность**: MPI версия показывает ускорение 1.80× (2 процесса), 2.87× (4 процесса), 3.36× (8 процессов)
3. **Эффективность**: Наивысшая эффективность 90% достигается при использовании 2 процессов

### Ограничения:
- Эффективность значительно падает при использовании 8 процессов (42%)
- Накладные расходы MPI коммуникации ограничивают дальнейшее масштабирование
- Алгоритм наиболее эффективен для больших объемов данных

### Практическая значимость:
Реализация демонстрирует, что для задачи суммирования векторов MPI является эффективным инструментом, обеспечивающим значительное ускорение при сохранении точности вычислений. Оптимальным является использование 4 процессов, что обеспечивает баланс между производительностью и эффективностью использования ресурсов.

## 9. Источники

1. **MPI: The Complete Reference** - https://www.mpi-forum.org/docs/
2. **Google Test Framework Documentation** - https://github.com/google/googletest
3. **C++ Standard Template Library Reference** - https://en.cppreference.com/w/
4. **Introduction to Parallel Computing** - https://computing.llnl.gov/tutorials/parallel_comp/

## Приложение

### Ключевой фрагмент MPI реализации:
```cpp
bool VidermanAElemVecSumMPI::RunImpl() {
  int rank = 0, size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  
  const auto &input_vector = GetInput();
  const size_t element_count = input_vector.size();
  
  // распределение данных между процессами
  const size_t base_chunk = element_count / size;
  const size_t remaining_elements = element_count % size;
  
  size_t my_chunk_size = base_chunk + (rank < remaining_elements ? 1 : 0);
  size_t start_position = rank * base_chunk + std::min(rank, remaining_elements);
  
  // локальная сумма для каждого процесса
  double process_sum = 0.0;
  auto segment_start = input_vector.begin() + start_position;
  auto segment_end = segment_start + my_chunk_size;
  
  for (auto it = segment_start; it != segment_end; ++it) {
    process_sum += *it;
  }
  
  // редукция
  double final_result = 0.0;
  MPI_Allreduce(&process_sum, &final_result, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
  
  GetOutput() = final_result;
  return true;
}
```

### Ключевой фрагмент SEQ реализации:
```cpp
bool VidermanAElemVecSumSEQ::RunImpl() {
  const auto &input = GetInput();
  GetOutput() = std::accumulate(input.begin(), input.end(), 0.0);
  return true;
}
```