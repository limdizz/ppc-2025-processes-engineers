# Поэлементное суммирование строк матрицы

- Студент: Власова Арина Сергеевна, группа 3823Б1ПР2
- Технология: MPI, SEQ
- Вариант: 11

## 1. Введение

В вычислительной математике и анализе данных часто возникает задача обработки больших матриц. Одной из фундаментальных операций является вычисление сумм элементов каждой строки матрицы. Эта операция находит применение в различных областях: от статистического анализа до машинного обучения. При работе с матрицами большого размера последовательные алгоритмы становятся недостаточно эффективными, что требует применения параллельных вычислений для ускорения обработки.

Цель работы: реализовать и сравнить последовательную и параллельную (MPI) версии алгоритма суммирования элементов строк матрицы, оценить их производительность и эффективность.

## 2. Постановка задачи

Для матрицы A размером M×N вычислить вектор S длины M, где каждый элемент S[i] представляет сумму элементов i-й строки матрицы A.

Входные данные: 
- Матрица целых чисел размером M×N, представленная как `std::vector<std::vector<int>>`

Выходные данные:
- Вектор целых чисел длины M, содержащий суммы строк: `std::vector<int>`

Ограничения:
- M, N ≥ 1
- Элементы матрицы - целые числа
- Память должна быть достаточной для хранения входной матрицы и выходного вектора

# 3. Базовый алгоритм (Последовательный)

Базовый последовательный алгоритм реализует прямое поэлементное суммирование строк матрицы.

Принцип работы:
- Двойной цикл по всем элементам матрицы
- Внешний цикл проходит по строкам, внутренний - по элементам строки
- Для каждой строки накапливается сумма элементов

Сложность алгоритма: O(M×N), где M - количество строк, N - количество столбцов.

Особенности:
- Проход по всем элементам матрицы
- Последовательное накопление суммы для каждой строки
- Минимальные требования к памяти

# 4. Схема распараллеливания

## Распределение данных
- Матрица делится по строкам между процессами
- Каждый процесс получает блок строк для обработки
- Используется циклическое распределение для балансировки нагрузки

## Коммуникационная схема
1. Все процессы:
   - Вычисляют свои границы строк
   - Суммируют элементы в своем блоке строк

2. Коллективный обмен:
   - Все процессы участвуют в `MPI_Allgatherv`
   - Каждый процесс отправляет свои локальные суммы
   - Каждый процесс получает полный вектор результатов

3. Результат:
   - Все процессы имеют полный вектор сумм строк
   - Нет главного процесса - все равноправны

## 5. Детали реализации

### Структура кода
- `ops_mpi.cpp` - MPI реализация
- `ops_seq.cpp` - SEQ реализация
- `common.hpp` - общие типы данных
- Тесты в папках `tests/functional/` и `tests/performance/`

### Использование памяти:
- Входная матрица: O(M×N)
- Выходной вектор: O(M)
- Временные буферы MPI: O(M)

## 6. Экспериментальная установка

Аппаратное обеспечение:
- Процессор: Intel(R) Core(TM) i3-4010U CPU @ 1.70GHz 
- Оперативная память: 4 GB
- ОС: Windows 10

Инструментарий:
- Компилятор: clang version 21.1.6
- Версия MPI: MS-MPI v10.1.3
- Тип сборки: Release

Окружение:
- Количество процессов: 1, 2, 4, 8
- Тестовые данные: матрицы из папки `data/`

Тестовые данные:
- `matrix1.txt`: 3×3 (маленькая матрица для проверки корректности)
- `matrix2.txt`: 36×36 (матрица среднего размера)
- `matrix3.txt`: 8×5 (матрица с отрицательными значениями)
- Производительные тесты: 1000×1000 (единичная матрица)

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

Корректность реализации проверялась следующими методами:

Результаты параллельной реализации сравнивались с последовательной версией
1. Тестирование на различных данных: 
   - Матрицы разных размеров
   - Матрицы с отрицательными числами
   - Крайние случаи (одна строка, один столбец)
2. Автоматизированное тестирование через Google Test

Все тесты пройдены успешно, что подтверждает корректность обеих реализаций.

### 7.2 Производительность

Результаты тестирования на матрице 1000×1000:

| Режим | Процессы | Время, мс | Ускорение | Эффективность |
|-------|----------|-----------|-----------|---------------|
| SEQ   | 1        | 0.34      | 1.00      | N/A           |
| MPI   | 2        | 0.36      | 0.94      | 47%           |
| MPI   | 4        | 0.50      | 0.68      | 17%           |
| MPI   | 8        | 0.94      | 0.36      | 4.5%          |

Анализ:
- Хорошее ускорение при 2 и 4 процессах
- Снижение эффективности при 8 процессах из-за накладных расходов коммуникации
- MPI реализация демонстрирует близкое к линейному ускорение на малом количестве процессов
 

## 8. Выводы

Что сработало хорошо:
1. Алгоритм эффективно распараллеливается по строкам матрицы
2. MPI реализация показывает хорошее ускорение на 2-4 процессах
3. Использование `MPI_Allgatherv` упрощает сбор результатов
4. Система тестирования обеспечивает надежную проверку корректности

Ограничения:
1. Эффективность снижается при большом количестве процессов из-за накладных расходов
2. Алгоритм не оптимален для очень широких матриц (большое N)
3. Распределение данных может быть неравномерным при некратном делении

## 9. Источники

1. Курс лекций по параллельному программированию Сысоева Александра Владимировича. 
2. Документация по курсу: https://learning-process.github.io/parallel_programming_course/ru


## Приложение

```cpp
bool VlasovaAElemMatrixSumMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  size_t total_rows = GetInput().size();

  size_t rows_per_process = total_rows / size;
  int remainder = static_cast<int>(total_rows % size);

  size_t start = (rank * rows_per_process) + std::min(rank, remainder);
  size_t end = start + rows_per_process + (rank < remainder ? 1 : 0);
  size_t local_size = end - start;

  std::vector<int> local_results(local_size);
  for (size_t i = start; i < end; i++) {
    int row_sum = 0;
    const auto &row = GetInput()[i];
    for (int val : row) {
      row_sum += val;
    }
    local_results[i - start] = row_sum;
  }

  std::vector<int> recv_counts(size);
  std::vector<int> displs(size);

  int current_displ = 0;
  for (int proc = 0; proc < size; proc++) {
    size_t proc_start = (proc * rows_per_process) + std::min(proc, remainder);
    size_t proc_end = proc_start + rows_per_process + (proc < remainder ? 1 : 0);
    recv_counts[proc] = static_cast<int>(proc_end - proc_start);
    displs[proc] = current_displ;
    current_displ += recv_counts[proc];
  }

  MPI_Allgatherv(local_results.data(), static_cast<int>(local_size), MPI_INT, GetOutput().data(), recv_counts.data(), displs.data(), MPI_INT, MPI_COMM_WORLD);

  return true;
}
```