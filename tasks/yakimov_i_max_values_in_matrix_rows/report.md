# Нахождение максимальных значений по строкам матрицы Якимов Илья

- Student: Якимов Илья Владимирович, group 3823Б1ПР2
- Technology: SEQ | MPI 
- Variant: 15

## 1. Introduction
Разработка однопоточного алгоритма (SEQ) вычисления максимальных значений по строкам матрицы и его параллельной реализации (MPI) с использованием MPI. Цель - ускорение обработки больших матриц за счет распределения вычислений между несколькими процессами.

## 2. Problem Statement
Для заданной матрицы размером M×N найти максимальное значение в каждой строке, затем вычислить сумму этих максимумов.

Вход: целочисленная матрица - заранее сгенерированная data - 30 txt файлов в папке parallel_programming\ppc-2025-processes-engineers\tasks\yakimov_i_max_values_in_matrix_rows\data

Выход: сумма максимальных элементов строк

Ограничения: время работы <1 секунды в functional тестах

## 3. Baseline Algorithm (Sequential)

'''for (size_t i = 0; i < rows_; i++) {
    if (matrix_[i].empty()) {
      return false;
    }

    max_Values_[i] = matrix_[i][0];

    for (size_t j = 1; j < cols_; j++) {
      if (matrix_[i][j] > max_Values_[i]) {
        max_Values_[i] = matrix_[i][j];
      }
    }
}'''

matrix_[i] - i-ая строка матрицы 
max_Values_ - вектор, хранящий значения максимальных элементов в строке

## 4. Parallelization Scheme

Распределение данных: блочное распределение строк матрицы между процессами

Коммуникация:
- Процесс 0 читает матрицу и распределяет данные
- Точечные сообщения для отправки данных рабочим процессам
- Сбор результатов методом "gather" (каждый рабочий процесс вычисляет максимумы для своего блока строк -Ю результаты отправляются на мастер-процесс (rank 0) -> rank 0 собирает все части в итоговый массив)

Роли рангов:
- Rank 0: мастер-процесс (чтение, распределение, сбор)
- Rank 1..N-1: рабочие процессы (обработка своих блоков)

## 5. Implementation Details
Структура кода:
Используем команду "tree tasks/yakimov_i_max_values_in_matrix_rows/" для того чтобы узнать структуру проекта:

tasks/yakimov_i_max_values_in_matrix_rows/
├── common
│   └── include
│       └── common.hpp
├── data
│   ├── 1.txt
│   ├── 10.txt
│   ├── 11.txt
│   ├── 12.txt
│   ├── 13.txt
│   ├── 14.txt
│   ├── 15.txt
│   ├── 16.txt
│   ├── 17.txt
│   ├── 18.txt
│   ├── 19.txt
│   ├── 2.txt
│   ├── 20.txt
│   ├── 21.txt
│   ├── 22.txt
│   ├── 23.txt
│   ├── 24.txt
│   ├── 25.txt
│   ├── 26.txt
│   ├── 27.txt
│   ├── 28.txt
│   ├── 29.txt
│   ├── 3.txt
│   ├── 30.txt
│   ├── 4.txt
│   ├── 5.txt
│   ├── 6.txt
│   ├── 7.txt
│   ├── 8.txt
│   ├── 9.txt
│   ├── generate_matrices
│   └── generate_matrices.cpp
├── info.json
├── mpi
│   ├── include
│   │   └── ops_mpi.hpp
│   └── src
│       └── ops_mpi.cpp
├── report.md
├── seq
│   ├── include
│   │   └── ops_seq.hpp
│   └── src
│       └── ops_seq.cpp
├── settings.json
└── tests
    ├── functional
    │   └── functional.cpp
    └── performance
        └── performance.cpp

13 directories, 42 files

Ключевые методы (вспомогательные для RunImpl(), с целью декомпозиции функции и избежания ее перегрузки):
- BroadcastMatrixDimensions() - передача размеров матрицы
- CalculateLocalRows() - вычисление локальных строк для процесса
- FindLocalMaxValues() - поиск максимумов в локальном блоке

Особенности:
- Обработка неравномерного распределения при делении строк
- Проверка валидности на процессе 0
- Барьерная синхронизация перед началом вычислений

## 6. Experimental Setup
- Hardware/OS: CPU model: Intel Core i7 12700H, cores/threads: 14/20, RAM: 16GB, OS version: Ubuntu 24.04.3 LTS
- Toolchain: compiler: gcc, version: 14, MPI: OpenMPI 4.1.2, build type (Release/RelWithDebInfo): Release
- Environment: PPC_NUM_THREADS / PPC_NUM_PROC: 1 / 8, other relevant vars: -
- Data: размеры матриц: от 100×100 до 5000×5000 элементов 

## 7. Results and Discussion

### 7.1 Correctness
- Проверка против последовательной реализации
- Unit-тесты с эталонными значениями
- Тестирование на матрицах разного размера
- Валидация граничных случаев (1 строка, 1 столбец)

### 7.2 Performance
Результаты в столбце "Time" представлены как среднее между 3 запусками performance тестов

Speedup = T_seq / T_parallel
Efficiency = Speedup / Count * 100%

#### Измерения "чистого" времени вычислений максимальных элементов по строкам матрицы - task_run

| Mode        | Count |    Time, ms    | Speedup | Efficiency |
|-------------|-------|----------------|---------|------------|
| seq         | 1     | 0.0081394196   | 1.00    | N/A        |
| mpi         | 4     | 0.0183642742   | 0.44    | 11.0%      |
| mpi         | 8     | 0.0257455308   | 0.32    | 4.0%       |
| mpi         | 12    | 0.0304794270   | 0.27    | 2.2%       |
| mpi         | 20    | 0.0298154522   | 0.27    | 1.4%       |

#### Измерения полного времени вычислений ("чистое" + затраты на открытие файла, считывание и коммуникацию процессов) - pipeline

| Mode        | Count |    Time, ms    | Speedup | Efficiency |
|-------------|-------|----------------|---------|------------|
| seq         | 1     | 3.2155227184   | 1.00    | N/A        |
| mpi         | 4     | 1.5889995484   | 2.02    | 50.5%      |
| mpi         | 8     | 2.9256506878   | 1.10    | 13.8%      |
| mpi         | 12    | 2.8845829832   | 1.11    | 9.3%       |
| mpi         | 20    | 4.8718488208   | 0.66    | 3.3%       |

Ограничения масштабируемости:
- Коммуникационные затраты: При увеличении числа процессов время передачи данных начинает доминировать над временем вычислений
- Неравномерность нагрузки: При делении M строк на N процессов может возникать дисбаланс, если M не кратно N
- Накладные расходы MPI: Синхронизация процессов и управление сообщениями

"Бутылочные горлышки":
- Распределение данных - процесс 0 становится узким местом при отправке данных всем рабочим процессам
- Сбор результатов - последовательный прием данных на мастере ограничивает параллелизм
- Размер сообщений - для маленьких матриц накладные расходы MPI превышают выгоду от распараллеливания

Порог эффективности:
- Алгоритм эффективен для матриц размером от 1000×1000 элементов. Для меньших матриц последовательная версия может оказаться быстрее из-за накладных расходов параллелизации.

## 8. Conclusions
- MPI реализация обеспечивает значительное ускорение
- Наилучшая эффективность достигается при умеренном числе процессов (от 4 до 12 процессов, так как дальше идут неоправданные затраты времени на коммуникацию рабочих процессов с мастер процессом)
- Алгоритм хорошо масштабируется для больших матриц
- Основное ограничение - коммуникационные затраты

## 9. References
1. OpenMPI документация: <https://www.open-mpi.org/>
2. MPI стандарт: <https://www.mpi-forum.org/>
3. Курс параллельного программирования: материалы курса <https://learning-process.github.io/parallel_programming_course/ru/common_information/report.html>
4. Мастер проекта: <https://github.com/learning-process/ppc-2025-processes-engineers>